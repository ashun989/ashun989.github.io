<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"ashun989.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.14.2","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null,"activeClass":"gitalk"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="本文是Attention is not all you need: Pure attention loses rank doubly exponentially with depth的阅读笔记，主要贡献为：  提出了一种路径分解(path decomposition)的方式来理解自注意力网络(SA or SANs)； 从理论和实验上证明了如果没有skip connections和ML">
<meta property="og:type" content="article">
<meta property="og:title" content="《Attention is not all you need》阅读笔记">
<meta property="og:url" content="https://ashun989.github.io/2022/08/29/SA-inductive-bias/index.html">
<meta property="og:site_name" content="Ashun&#39;s Blog">
<meta property="og:description" content="本文是Attention is not all you need: Pure attention loses rank doubly exponentially with depth的阅读笔记，主要贡献为：  提出了一种路径分解(path decomposition)的方式来理解自注意力网络(SA or SANs)； 从理论和实验上证明了如果没有skip connections和ML">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ashun989.github.io/2022/08/29/SA-inductive-bias/image-20220829182555652.png">
<meta property="og:image" content="https://ashun989.github.io/2022/08/29/SA-inductive-bias/image-20220829182917579.png">
<meta property="og:image" content="https://ashun989.github.io/2022/08/29/SA-inductive-bias/image-20220829184907635.png">
<meta property="og:image" content="https://ashun989.github.io/2022/08/29/SA-inductive-bias/image-20220829191051128.png">
<meta property="og:image" content="https://ashun989.github.io/2022/08/29/SA-inductive-bias/image-20220829191440509.png">
<meta property="og:image" content="https://ashun989.github.io/2022/08/29/SA-inductive-bias/image-20220829192520446.png">
<meta property="og:image" content="https://ashun989.github.io/2022/08/29/SA-inductive-bias/image-20220829192613228.png">
<meta property="og:image" content="https://ashun989.github.io/2022/08/29/SA-inductive-bias/image-20220829193554480.png">
<meta property="og:image" content="https://ashun989.github.io/2022/08/29/SA-inductive-bias/image-20220829193800445.png">
<meta property="og:image" content="https://ashun989.github.io/2022/08/29/SA-inductive-bias/image-20220829193830091.png">
<meta property="og:image" content="https://ashun989.github.io/2022/08/29/SA-inductive-bias/image-20220829194442073.png">
<meta property="og:image" content="https://ashun989.github.io/2022/08/29/SA-inductive-bias/image-20220829200030809.png">
<meta property="og:image" content="https://ashun989.github.io/2022/08/29/SA-inductive-bias/image-20220829201840986.png">
<meta property="og:image" content="https://ashun989.github.io/2022/08/29/SA-inductive-bias/image-20220829202101795.png">
<meta property="og:image" content="https://ashun989.github.io/2022/08/29/SA-inductive-bias/image-20220829202137282.png">
<meta property="og:image" content="https://ashun989.github.io/2022/08/29/SA-inductive-bias/image-20220829202527114.png">
<meta property="og:image" content="https://ashun989.github.io/2022/08/29/SA-inductive-bias/image-20220829203753205.png">
<meta property="og:image" content="https://ashun989.github.io/2022/08/29/SA-inductive-bias/image-20220829210557076.png">
<meta property="article:published_time" content="2022-08-29T09:04:04.000Z">
<meta property="article:modified_time" content="2023-02-12T12:24:32.281Z">
<meta property="article:author" content="ashun">
<meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ashun989.github.io/2022/08/29/SA-inductive-bias/image-20220829182555652.png">


<link rel="canonical" href="https://ashun989.github.io/2022/08/29/SA-inductive-bias/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://ashun989.github.io/2022/08/29/SA-inductive-bias/","path":"2022/08/29/SA-inductive-bias/","title":"《Attention is not all you need》阅读笔记"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>《Attention is not all you need》阅读笔记 | Ashun's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Ashun's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">我的学习记录</p>
      <img class="custom-logo-image" src="/uploads/a.png" alt="Ashun's Blog">
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/me" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">17</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">4</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">31</span></a></li><li class="menu-item menu-item-whisper"><a href="/whisper/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>whisper</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%90%86%E8%AE%BA%E9%83%A8%E5%88%86"><span class="nav-number">1.</span> <span class="nav-text">理论部分</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B7%AF%E5%BE%84%E5%88%86%E8%A7%A3"><span class="nav-number">1.1.</span> <span class="nav-text">路径分解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#san%E6%94%B6%E6%95%9B%E5%88%B0rank-1%E7%9F%A9%E9%98%B5"><span class="nav-number">1.2.</span> <span class="nav-text">SAN收敛到rank-1矩阵</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%95%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84%E6%83%85%E5%86%B5"><span class="nav-number">1.2.1.</span> <span class="nav-text">单头注意力的情况</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84%E6%83%85%E5%86%B5"><span class="nav-number">1.2.2.</span> <span class="nav-text">多头注意力的情况</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8A%B5%E6%B6%88%E7%A7%A9%E5%B4%A9%E5%A1%8C%E7%9A%84%E6%96%B9%E5%BC%8F"><span class="nav-number">1.3.</span> <span class="nav-text">抵消秩崩塌的方式</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#skip-connections%E5%BE%88%E6%9C%89%E7%94%A8"><span class="nav-number">1.3.1.</span> <span class="nav-text">Skip connections很有用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#mlp%E6%9C%89%E7%94%A8"><span class="nav-number">1.3.2.</span> <span class="nav-text">MLP有用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#layernorm%E6%B2%A1%E7%94%A8"><span class="nav-number">1.3.3.</span> <span class="nav-text">LayerNorm没用</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E9%83%A8%E5%88%86"><span class="nav-number">2.</span> <span class="nav-text">实验部分</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%BC%E8%87%B4%E7%A7%A9%E5%9D%8D%E7%BC%A9%E7%9A%84%E9%AA%8C%E8%AF%81"><span class="nav-number">2.1.</span> <span class="nav-text">导致秩坍缩的验证</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%81%BF%E5%85%8D%E7%A7%A9%E5%9D%8D%E7%BC%A9%E7%9A%84%E9%AA%8C%E8%AF%81"><span class="nav-number">2.2.</span> <span class="nav-text">避免秩坍缩的验证</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B7%AF%E5%BE%84%E6%9C%89%E6%95%88%E6%80%A7"><span class="nav-number">2.3.</span> <span class="nav-text">路径有效性</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93"><span class="nav-number">3.</span> <span class="nav-text">小结</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="ashun"
      src="/uploads/avatar.jpg">
  <p class="site-author-name" itemprop="name">ashun</p>
  <div class="site-description" itemprop="description">个人博客</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">31</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/ashun989" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ashun989" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:ashun0606@gmail.com" title="E-Mail → mailto:ashun0606@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ashun989.github.io/2022/08/29/SA-inductive-bias/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="ashun">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ashun's Blog">
      <meta itemprop="description" content="个人博客">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="《Attention is not all you need》阅读笔记 | Ashun's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          《Attention is not all you need》阅读笔记
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-08-29 17:04:04" itemprop="dateCreated datePublished" datetime="2022-08-29T17:04:04+08:00">2022-08-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-02-12 20:24:32" itemprop="dateModified" datetime="2023-02-12T20:24:32+08:00">2023-02-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>本文是<a
target="_blank" rel="noopener" href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=AjX6hisAAAAJ&amp;citation_for_view=AjX6hisAAAAJ:5nxA0vEk-isC">Attention
is not all you need: Pure attention loses rank doubly exponentially with
depth</a>的阅读笔记，主要贡献为：</p>
<ul>
<li>提出了一种路径分解(path decomposition)的方式来理解自注意力网络(SA or
SANs)；</li>
<li>从理论和实验上证明了如果没有skip
connections和MLP，纯SANs将随着深度的增加，将以双倍指数的速度退化到一个秩为1的矩阵（重复的token），称为秩坍缩(rank
collapse)，也可以说SA具有很强的token uniformity的偏执假设;</li>
<li>结合skip-connection的有效性和路径分解的解释，可以将Transformer结构理解为浅层网络的集成；</li>
</ul>
<span id="more"></span>
<blockquote>
<p>这篇文章的公式很多，查了查作者是数学科班出身，笔者能力有限，文中还有很多推导和表述不能理解；阅读过程中我也参考了一位大佬很详细的解读：<a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/356956903">知乎 - 迷途小书僮</a></p>
</blockquote>
<p>首先，需要对后续公式的记号做个说明：</p>
<ul>
<li><p>加粗的字母表示列向量（小写）或者矩阵（大写），<span
class="math inline">\(\mathbf{1}\)</span>表示一个全为1的列向量；</p></li>
<li><p><span class="math inline">\([H]=(1,\dots,H)\)</span>；</p></li>
<li><p>作者定义了一种<span
class="math inline">\(l_1,l_\infty\)</span>混合的矩阵范数： <span
class="math display">\[
||X||_{1,\infty}=\sqrt{||X||_1||X||_\infty}
\]</span> 该范数不满足三角不等式，但是是正定的，齐次的；</p></li>
</ul>
<h2 id="理论部分">理论部分</h2>
<h3 id="路径分解">路径分解</h3>
<p>记输入为<span class="math inline">\(n\times d_{in}\)</span>矩阵<span
class="math inline">\(X\)</span>，有<span
class="math inline">\(L\)</span>层self-attention层，每层有<span
class="math inline">\(H\)</span>个头，输出序列的长度为<span
class="math inline">\(m\)</span>，第<span
class="math inline">\(h\)</span>个SA的输出可以记为：</p>
<p><img src="image-20220829182555652.png" /></p>
<p>其中，<span class="math inline">\(W_{V,h}\)</span>是一个<span
class="math inline">\(n\times d_v\)</span>的Value变换区镇，<span
class="math inline">\(P_h\)</span>是一个<span
class="math inline">\(n\times n\)</span>的行随机矩阵：</p>
<p><img src="image-20220829182917579.png" /></p>
<p>其中<span class="math inline">\(W_{K,h}\)</span>和<span
class="math inline">\(W_{Q,h}\)</span>都是<span
class="math inline">\(d_{in}\times d_{qk}\)</span>的变换矩阵，<span
class="math inline">\(W_{QK,h}=W_{Q,h}W_{K,h}^T\)</span></p>
<blockquote>
<p><span
class="math inline">\(P_h\)</span>是使用softmax归一化处理的Attention
Map，但是为什么称row-stochastic不太清楚；</p>
</blockquote>
<p><span
class="math inline">\(P_h\)</span>在展开后为什么少了两项？这里需要注意softmax的两个性质：</p>
<ol type="1">
<li><p>平移不变性 <span class="math display">\[
\sigma(x+C)_i =
\frac{e^{x_i}e^C}{\sum_je^{x_j}e^C}=\frac{e^{x_i}}{\sum_je^{x_j}}=\sigma({x})_i
\]</span> 因此，展开后与<span
class="math inline">\(X\)</span>无关的常数项<span
class="math inline">\(\mathbf{1b_{Q,h}^T
b_{K,h}1^T}\)</span>可以省略；</p></li>
<li><p>这里softmax是逐行归一化的，因此对于一个由相同的列向量组成的矩阵（每行都是同一个值），softmax对每行的权重都是平均分，整体上不改变结果；因此类似于右乘了一个<span
class="math inline">\(\mathbf{1}^T\)</span>的项，都可以在softmax内省略；</p></li>
</ol>
<p>接下来将多个头合并，给出一个多头注意力模块的简洁的表达式（不包含skip-connection）：</p>
<p><img src="image-20220829184907635.png" /></p>
<p>其中，<span
class="math inline">\([\text{SA}_1(X),\dotsm,\text{SA}_H(X)]\)</span>是<span
class="math inline">\(n\times Hd_v\)</span>的，<span
class="math inline">\([W_{O,1}^T,\dots,W_{O,H}^T]^T\)</span>是<span
class="math inline">\(Hd_v\times
d_h\)</span>的，偏移项写得好像有点问题（？不过不重要），之后使用矩阵的分块乘法法则得到第二行的求和式，其中<span
class="math inline">\(W_h=W_{V,h}W_{O,h}^T\)</span></p>
<blockquote>
<p>偏移项可以通过改写<span class="math inline">\(X\)</span>和<span
class="math inline">\(W\)</span>而消除，因此在后面的推导中不再使用偏移项</p>
</blockquote>
<p>当多个SA堆叠时，我们便可以通过如下的递推式：</p>
<p><img src="image-20220829191051128.png" /></p>
<p>得到一个第<span class="math inline">\(L\)</span>层输出<span
class="math inline">\(X^{L}\)</span>关于输入<span
class="math inline">\(X\)</span>的表达式</p>
<p><img src="image-20220829191440509.png" /></p>
<p>对于这个式子，可以看成是从输入到第L层的一个有向图，每一层有<span
class="math inline">\(H\)</span>条路；</p>
<p><img src="image-20220829192520446.png" /></p>
<p><span class="math inline">\(P_{path}\)</span>与输入有关，<span
class="math inline">\(W_{path}\)</span>与输入无关；</p>
<p><img src="image-20220829192613228.png" /></p>
<p>如图，整个SAN被分解为了若干条路径的组合；</p>
<h3 id="san收敛到rank-1矩阵">SAN收敛到rank-1矩阵</h3>
<h4 id="单头注意力的情况">单头注意力的情况</h4>
<p>假设要证明的收敛目标是<span
class="math inline">\(1\mathbf{x}^T\)</span>，这是一个所有行都相同的rank-1矩阵，那么只要证明第<span
class="math inline">\(h\)</span>层的输出<span
class="math inline">\(X\)</span>，与<span
class="math inline">\(1\mathbf{x}^T\)</span>的距离有一个上界，并且这个上界随着<span
class="math inline">\(h\)</span>的增加越来越小；</p>
<p>作者给出了如下目标函数，残差（期望与观测的差）：</p>
<p><img src="image-20220829193554480.png" /></p>
<p>在<span class="math inline">\(H=1\)</span>，<span
class="math inline">\(L\)</span>层，且满足</p>
<p><img src="image-20220829193800445.png" /></p>
<p>的条件下，作者证明（在附录中给出）：</p>
<p><img src="image-20220829193830091.png" /></p>
<p>那么只要<span
class="math inline">\(4\beta&lt;\sqrt{d_{qk}}\)</span>，不等式右边就会随着<span
class="math inline">\(L\)</span>的增加快速收敛到0；作者的实验证明这是一个很宽松的条件；</p>
<blockquote>
<p>附录中有简略的证明，<a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/356956903">知乎 -
迷途小书僮</a>对大部分的推导过程给出了详细的阐述，我就不再赘述了</p>
</blockquote>
<h4 id="多头注意力的情况">多头注意力的情况</h4>
<p>推广到有<span
class="math inline">\(H\)</span>个头，得到残差的上界：</p>
<p><img src="image-20220829194442073.png" /></p>
<p>那么只要<span class="math inline">\(4\beta
H&lt;\sqrt{d_{qk}}\)</span>，不等式右边就会随着<span
class="math inline">\(L\)</span>的增加快速收敛到0；</p>
<h3 id="抵消秩崩塌的方式">抵消秩崩塌的方式</h3>
<h4 id="skip-connections很有用">Skip connections很有用</h4>
<p>我们可以将选择Skip-connections的路径记为<span
class="math inline">\(h=0\)</span>，同时有<span
class="math inline">\(P_0=I\)</span>，<span
class="math inline">\(W_0=I\)</span>，则之前的式子可以重新写成：</p>
<p><img src="image-20220829200030809.png" /></p>
<p>作者在附录中同样给出了在这个式子下的res(X)的上限，但是这个上限非常大，Skip-connections增加了路径分布的多样性（路径参数组合的多样性）：因为skip-connection算一条路径长度为0的路，因此对于<span
class="math inline">\(L\)</span>层SA，长度为<span
class="math inline">\(l\)</span>的路径数量为： <span
class="math display">\[
\mathcal{|P_l|}=
\left(
\begin{array}{c}
L\\l
\end{array}
\right)
H^l
\]</span></p>
<p>相反地，还可以给出一个目标函数的下界，说明加入Skip-connections之后SANs结果不会收敛到rank-1矩阵：
<span class="math display">\[
||res(X^L)||\ge||res(X)||
\]</span> 当对于所有层都有<span
class="math inline">\(W_v^l=0\)</span>时，上式取等；即使在<span
class="math inline">\(L\rightarrow \infty\)</span>，<span
class="math inline">\(\beta\)</span>很小时，上式也成立；</p>
<blockquote>
<p>原文里有一句 for any parametrization that renders the contribution of
the SAN layers orthogonal to the input，我不太理解</p>
</blockquote>
<p>有此也可以得出结论，SANs是浅层网络的集成，只不过在各个组件之间不是完全解耦的，因为一个head可能出现在多个path中；</p>
<h4 id="mlp有用">MLP有用</h4>
<p>且不看严谨的证明，在weight和bias都是随机初始化的时候，并且输入的token也各不相同的前提下，也能直观地感觉到，反复使用MLP能够避免收敛到rank-1矩阵；</p>
<p>作者同样给出了加入MLP后，res(X)的上界：</p>
<p><img src="image-20220829201840986.png" /></p>
<p>主要是通过让收敛变慢来避免秩坍缩；</p>
<h4 id="layernorm没用">LayerNorm没用</h4>
<p>这里的无用是针对解决秩坍缩问题而言的，因为应用LN后，SA(X)仍然能改写成与原来相同的形式，之后也就可以得到同样形式的上界：</p>
<p><img src="image-20220829202101795.png" /></p>
<p><img src="image-20220829202137282.png" /></p>
<h2 id="实验部分">实验部分</h2>
<h3 id="导致秩坍缩的验证">导致秩坍缩的验证</h3>
<p>作者首先在几个常见的Transformer结构上进行了实验，BERT，Albert，XLNet，挥着了相对残差<span
class="math inline">\(||res(SAN(X^l))||_{1,\infty}/||SAN(X^l)||_{1，\infty}\)</span>随着层数增加的变化，使用在维基百科上的传记摘录的的32个样本训练模型，<span
class="math inline">\(d_{in}=128\)</span></p>
<p><img src="image-20220829202527114.png" /></p>
<blockquote>
<p>主要还是skip-connections对于避免秩坍缩的作用更明显</p>
</blockquote>
<h3 id="避免秩坍缩的验证">避免秩坍缩的验证</h3>
<p>之后，作者训练单层Transformer来学习两对圆弧序列，训练时时teacher
forcing的，那么如果在完全自回归的推理时，两条弧线收敛到同一点，而不是延续训练的轨迹，则认为发生了秩坍缩：</p>
<p><img src="image-20220829203753205.png" /></p>
<p>实验结果表明：</p>
<ul>
<li>skip-connections和MLP有效避免了秩坍缩；</li>
<li>当维度增大时，对于<span
class="math inline">\(4\beta&lt;\sqrt{d_{qk}}\)</span>的条件，看似是不等式右边增加使得收敛条件更宽，实际上<span
class="math inline">\(\beta\)</span>作为<span
class="math inline">\(|||W^l_{QK}||_1||W_V^l||_{1,\infty}\)</span>的上界也在增加，使得收敛条件更紧；</li>
</ul>
<h3 id="路径有效性">路径有效性</h3>
<p>下面的实验试图验证，随着路径长度的增加，即使所涉及的非线性操作数增加（可以理解为路径数量），路径有效性也在降低：</p>
<p>作者分别在序列记忆、学习排序和凸包预测任务上训练Transformer模型；</p>
<p>在上文中我们提到，路径之间不是完全解耦的，如何找到某一个路径的显示表示，如何衡量其单独的贡献？</p>
<p>对于前一个问题，没办法确定一个训练好的Transformer具体是哪些路径的组合，作者就从全体路径中，按照给定的长度随机采样一个路径集合的子集，用该子集中路径的贡献的归一化总和（平均贡献？）；</p>
<p>衡量任意一条给定路径序列<span class="math inline">\(h_1,\dots,h_L \in
[H\cup0]^L\)</span>，使用下面式子的结果作为该路径的输出： <span
class="math display">\[
(P_{h_L}^L\dots P_{h_1}^1)X(W_{h_1}^1\dots W_{h_L}^L)
\]</span> <img src="image-20220829210557076.png" /></p>
<p>记忆和排序任务都验证了路径越长效果越差的假设，凸包任务有明显的类别不平衡问题，上述假设体现不明显，但是随着路径的增加，模型预测结果准确率的方差在变大；</p>
<h2 id="小结">小结</h2>
<p>至此位置，论文理解只有一半，剩下一半几乎都是附录中的证明，这篇文章的理论性很充足，一般人学不来啊；</p>
<p>之前总是讲CNN的归纳偏置是局部性和平移不变性，这篇文章让我们知道了Transformer的token一致性的归纳偏置，以及纯SA竟然这么拉（如果证明严谨无误并且没有比秩坍缩更严重的问题的话）？那么是不是靠着MLP+skip
connection+别的什么结构会有更好的结果？</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Transformer/" rel="tag"><i class="fa fa-tag"></i> Transformer</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/08/28/Lipschitz/" rel="prev" title="度量空间与连续性">
                  <i class="fa fa-chevron-left"></i> 度量空间与连续性
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/08/30/Stable-Calculations/" rel="next" title="稳定的数值计算">
                  稳定的数值计算 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ashun</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  




  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"ashun989","repo":"ashun989.github.io","client_id":"987971263864eebace39","client_secret":"57175a12b83a806d519086800d02bf809f59c14c","admin_user":"ashun989","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":null,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"dab760b51bfd5262390851e36fd9f602"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>
