<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>在空间域上的低秩分解和在通道上的线性组合加速卷积运算</title>
    <url>/2023/01/12/Conv-Spatial-Low-Rank-Decomp/</url>
    <content><![CDATA[<img src="/2023/01/12/Conv-Spatial-Low-Rank-Decomp/fig1.png" class="">
<p><strong>论文</strong>：<a
href="https://arxiv.org/abs/1405.3866"><strong>Speeding up Convolutional
Neural Networks with Low Rank Expansions</strong></a></p>
<p><strong>作者</strong>：<em>Max Jaderberg, Andrea Vedaldi and Andrew
Zisserman</em>;</p>
<p><strong>一作单位</strong>：Visual Geometry Group Department of
Engineering Science University of Oxford</p>
<p><strong>录用情况</strong>：BMVC'2014</p>
<blockquote>
<p>本文在空间维度上对卷积的空间域进行低秩分解，提出了两种分解的Scheme，直接的和间接的，相比于只在空间域上分解卷积核，作者还利用了通道维度上的冗余性，将多个卷积核视为基的线性组合；对于从传统卷积核到分解后的结构，提出了两种优化方法，基于卷积核的重建和基于数据的重建；该可以用到任何一般的卷积层中去。</p>
</blockquote>
<span id="more"></span>
<h2 id="方法">方法</h2>
<h3 id="前置知识">前置知识</h3>
<p>记输入 <span class="math inline">\(x\in \mathbb{R}^{H\times
W}\)</span>，输出 <span class="math inline">\(Y = \{y_1, \dots,
y_N\}\)</span>，（其中<span class="math inline">\(y_n\in
\mathbb{R}^{H&#39;\times W&#39;}\)</span>），是由输入 <span
class="math inline">\(x\)</span> 与 <span
class="math inline">\(N\)</span> 个卷积核 <span class="math inline">\(F
= \{f_i\}~\forall i \in [1\dots N]\)</span> 卷积（<span
class="math inline">\(y_i = f_i * x\)</span>）得到的；</p>
<p>对于单通道的输入，<span class="math inline">\(N\)</span> 个 <span
class="math inline">\(d\times d\)</span>
的2D卷积核，传统卷积的时间复杂度 <span
class="math inline">\(O(d^2NH&#39;W&#39;)\)</span>；</p>
<p>一种基本的想法是将卷积核由一系列更少的基 <span
class="math inline">\(S = \{s_i\} ~\forall i \in [1\dots M]\)</span>
表示；有 <span class="math display">\[
y_i \simeq \sum_{k=1}^{M} a_{ik}s_{k} * x
\]</span></p>
<p>其中，<span class="math inline">\(a_{ik}\)</span> 是权重标量；</p>
<p>在计算时，可以先算出输入与所有基的卷积结果，之后在计算 <span
class="math inline">\(N\)</span> 种加权线性组合，因此时间复杂度是 <span
class="math inline">\(O((d^2M + MN)H&#39;W&#39;)\)</span>；</p>
<p>在满足 <span class="math inline">\(M&lt;\frac{d^2N}{d^2 + N}\)</span>
时，这种分解才有理论上的加速；</p>
<p>另一种基本的想法是将卷积核进行秩-1分解，有 <span
class="math inline">\(s_i * x = v_i * (h_i * x)\)</span>，其中，<span
class="math inline">\(s_i \in \mathbb{R}^{d\times d}\)</span>，<span
class="math inline">\(v_i \in \mathbb{R}^{d\times 1}\)</span>，<span
class="math inline">\(h_i \in \mathbb{R}^{1\times
d}\)</span>；这种分解下，时间复杂度为 <span
class="math inline">\(O(2dH&#39;W&#39;)\)</span>，理论上加速明显；</p>
<p>作者的主要想法就是，在扩展到3D卷积时，将上述两种方法结合起来；</p>
<h3 id="d卷积不过是2d卷积的组合">3D卷积不过是2D卷积的组合</h3>
<p>首先补充一些符号定义：</p>
<p>特征图 <span class="math inline">\(z_i(u, v)\)</span>，其中 <span
class="math inline">\((u, v)\in \Omega_i\)</span> 是空间坐标，<span
class="math inline">\(z_i(u, v)\in \mathbb{R}^C\)</span> 是 <span
class="math inline">\(C\)</span> 个标量组成的通道特征，某个通道 <span
class="math inline">\(c\)</span> 的特征图记为 <span
class="math inline">\(z_i^c(u, v)\)</span>；</p>
<p>经过卷积层后，得到下一层特征图 <span class="math inline">\(z_{i+1}
\in \mathbb{R}^{H&#39;\times W\times N}\)</span>，其中 <span
class="math inline">\(z_{i+1}^n = h_i(W_{i,n} * z_i + b_{i,n}) ~\forall
i \in [1\dots N]\)</span>；<span class="math inline">\(h_i\)</span>
是非线性激活函数；</p>
<p>我们可以把3D卷积视为2D卷积的组合： <span class="math display">\[
W_n * z = \sum_{c=1}^C W_n^c * z^c
\]</span></p>
<p>3D卷积的复杂度为 <span
class="math inline">\(O(CNd^2H&#39;W&#39;)\)</span>；依此，结合上一节的两种方法，作者首先提出scheme
1的分解方法：</p>
<h3 id="scheme-1">Scheme 1</h3>
<p>按照下式对卷积核直接进行分解： <span class="math display">\[
W_n * z = \sum_{c=1}^C W_n^c * z^c \simeq \sum_{c=1}^C\sum_{m=1}^M
a_n^{cm}(s_m^c * z^c)
\]</span></p>
<p>时间复杂度为 <span
class="math inline">\(O(MC(d^2+N)H&#39;W&#39;)\)</span>；</p>
<p>如果基 <span class="math inline">\(s_m^c\)</span>
是秩-1矩阵且表示为可分离的，那么时间复杂度可以进一步减小为 <span
class="math inline">\(O(MC(d+N)H&#39;W&#39;)\)</span>，那么，只要 <span
class="math inline">\(M &lt; d\min\{d, N\}\)</span>，scheme
1相比于传统卷积，在理论上更高效；</p>
<blockquote>
<p>这个条件似乎很苛刻，因为通常有 <span class="math inline">\(d \ll
N\)</span>，即 <span class="math inline">\(M \le d^2\)</span>，这么小的
<span class="math inline">\(M\)</span> 能得到好的拟合吗？</p>
</blockquote>
<p>在上式中，对于每一个通道 <span class="math inline">\(c\)</span>
上的卷积，使用不同的基 <span
class="math inline">\(S^c\)</span>，但是作者在实验中发现，所有通道共享相同的基，即
<span class="math inline">\(s_m^1 = \dots = s_m^C =
s_m\)</span>；但是，权重不可能再在通道维度上共享了，否则对原卷积的拟合结果在通道上一致，是非常差的拟合；学习到的权重是
<span class="math inline">\(NC\times M\)</span>
的张量，笔者怎么也不知道作者是怎么把这个权重放在 <span
class="math inline">\(N\)</span> 个 <span class="math inline">\(M \times
1 \times 1\)</span>
卷积中的；按照分解式，结合现有的api，笔者得到的scheme 1流程是：</p>
<ul>
<li>将 <span class="math inline">\(C\)</span> 通道输入分 <span
class="math inline">\(C\)</span> 组，每组用 <span
class="math inline">\(CM\)</span> 个单通道<span
class="math inline">\(d\times d\)</span>
卷积，这一步是计算输入的各个通道与<span
class="math inline">\(M\)</span>个基的各个通道的卷积；</li>
<li>使用 <span class="math inline">\(N\)</span> 个 <span
class="math inline">\(CM\)</span> 通道 <span
class="math inline">\(1\times 1\)</span> 卷积，这一步是加权求和；</li>
</ul>
<p>在学习到合适的基后，对 <span class="math inline">\(CM\times 1\times
d\times d\)</span> 的卷积核张量进行空间分解，得到如下的推理时流程：</p>
<ul>
<li>将 <span class="math inline">\(C\)</span> 通道输入分 <span
class="math inline">\(C\)</span> 组，每组用 <span
class="math inline">\(CM\)</span> 个单通道 <span
class="math inline">\(1\times d\)</span> 卷积；</li>
<li>将上一步结果分 <span class="math inline">\(CM\)</span> 组，每组用
<span class="math inline">\(CM\)</span> 个单通道 <span
class="math inline">\(d\times 1\)</span> 卷积；</li>
<li>使用 <span class="math inline">\(N\)</span> 个 <span
class="math inline">\(CM\)</span> 通道 <span
class="math inline">\(1\times 1\)</span> 卷积，这一步是加权求和；</li>
</ul>
<h3 id="scheme-2">Scheme 2</h3>
<p>这里作者希望用两次卷积解决问题：首先使用 <span
class="math inline">\(K\)</span> 组 <span class="math inline">\(d\times
1\)</span> 卷积 <span class="math inline">\(\{v_k \in
\mathbb{R}^{d\times 1\times C}: ~k\in [1\dots, K]\}\)</span>，得到 <span
class="math inline">\(V(u,v)\in \mathbb{R}^K\)</span>，再次使用 <span
class="math inline">\(N\)</span> 组 <span class="math inline">\(1\times
d\times K\)</span> 卷积 <span class="math inline">\(\{h_n\in
\mathbb{R}^{1\times d\times K}\}\)</span>；（图1(c)的标注有误）</p>
<p>由下式 <span class="math display">\[
W_n * z \simeq h_n * V = \sum_{k=1}^K h_n^k * V^k = \sum_{k=1}^K h_n^k *
(v_k * z) = \sum_{k=1}^K h_n^k * \sum_{c=1}^C v_k^c * z^c = \sum_{c=1}^C
\left[\sum_{k=1}^K h_n^k * v_k^c \right] * z^c
\]</span> 可得分解： <span class="math inline">\(W_n^c = \sum_{k=1}^K
h_n^k * v_k^c\)</span>；</p>
<p>Scheme 2的时间复杂度为 <span
class="math inline">\(O(K(N+C)dH&#39;W&#39;)\)</span>，要想获得理论加速，应该有
<span class="math inline">\(K(N + C) \ll NCd\)</span>；</p>
<h3 id="基于卷积核重建的优化">基于卷积核重建的优化</h3>
<h4 id="scheme-1-1">scheme 1</h4>
<p>对于scheme 1，使用先前的论文提出的目标函数即可：</p>
<p><span class="math display">\[
\min_{\{s_m\}, \{a_n\}} \sum_{n=1}^N\sum_{c=1}^C \left \Vert W_n^c -
\sum_{m=1}^M a_n^{cm} s_m \right\Vert_2^2 + \lambda \sum_{m=1}^M \Vert
s_m\Vert_*
\]</span></p>
<p>交替优化 <span class="math inline">\(\{s_m\}, \{a_n\}\)</span>；</p>
<p>值得注意的是，我们优化的是低秩的基 <span
class="math inline">\(\{s_m\}\)</span>，得到最优解后我们还要对 <span
class="math inline">\(\{s_m\}\)</span>
进行低秩分解（如SVD），得到最终的形式；</p>
<p>一个在实践中非常重要的问题就是 <span
class="math inline">\(\lambda\)</span> 的确定，作者建议使用小的 <span
class="math inline">\(\lambda\)</span> 开始，逐步增加；</p>
<h4 id="scheme-2-1">scheme 2</h4>
<p>对于scheme 2，使用如下目标函数，并使用共轭梯度下降优化：</p>
<p><span class="math display">\[
\min_{\{h_n^k\},\{v_k^c\}} \sum_{n=1}^{N}\sum_{c=1}^C \left \Vert W_n^c
- \sum_{k=1}^K h_n^k * v_k^c \right\Vert_2^2
\]</span></p>
<h3 id="基于数据重建的优化">基于数据重建的优化</h3>
<p>以scheme 2为例，目标函数为： <span class="math display">\[
\min_{\{h_n^k\},\{v_k^c\}} \sum_{i=1}^{|X|}\sum_{n=1}^{N}\left \Vert
W_n\Phi_{l-1}(x_i) - \sum_{c=1}^C \sum_{k=1}^K h_n^k * v_k^c *
\Phi_{l-1}(x_i)
\right \Vert_2^2
\]</span> 其中，<span class="math inline">\(\Phi_{l-1}(x_i)\)</span>
表示原网络在对于 <span class="math inline">\(x_i\)</span> 在 <span
class="math inline">\(l-1\)</span> 层的输出；</p>
<p>这种优化的实现相对简单，只需要同时运行费解版本和原始版本的卷积网络，并反向传播每层L2损失的梯度；</p>
<p>作者也在文中指出，使用分类损失回传的梯度来进行数据重建优化，在实验中，不如使用L2损失，容易过拟合；（作者的意思应该是只更新分解的卷积层参数，其它层冻结）</p>
<h2 id="实验">实验</h2>
<p>作者在实验部分指出，在测试集上，基于数据重建的优化要好于（重建损失更低，精度损失更低）基于卷积核的优化，甚至，"This
generally holds when data from a completely different distribution to
what the data optimization scheme has been trained on"；</p>
<p>而同等重建损失下scheme 1有比scheme
2的更好的理论加速比，但是由于彼时Caffe框架对于2D卷积没有很好的优化，因此作者的实现下scheme
1的实际耗时非常高；</p>
<p>最终，基于一个4层的CNN，做26个字母加10个数字的图像分类，作者在2,3层卷积上使用scheme
2 + 基于数据的重建，获得了1%的分类精度损失下的4.5倍的加速比；</p>
<h2 id="后来的研究">后来的研究</h2>
<p>在论文 <a href="https://arxiv.org/abs/1511.06067">Convolutional
Neural Networks with Low-Rank Regularziation</a> (<a
href="https://iclr.cc/archive/www/doku.php%3Fid=iclr2016:main.html">ICLR'2016</a>)
中，对本文的scheme2的工作进行了扩展：</p>
<ul>
<li>scheme2基于卷积核重建的优化，本文给出了解析解，避免依次优化迭代次数多陷入局部最优的问题；</li>
<li>证明了scheme2基于数据重建的优化是NP难问题；</li>
<li>仍然使用分类误差对整个模型进行微调，比逐层使用L2 loss微调更好；</li>
<li>从头训练带有低秩结构的模型，获得了不错的效果；</li>
</ul>
<p><img src="p2-tab5.png" /></p>
<p>下面笔者简单梳理一下解析解的由来：</p>
<p>本文基本沿用了相同的符号，目标函数是 <img src="p2-equ3.png" /></p>
<p>Frobenius 范数矩阵向量化后的二范数，有如下性质：</p>
<p><span class="math display">\[
\lVert W_1 + W_2 \rVert _F^2 = \lVert W_1 \rVert _F^2 + \lVert W_2
\rVert _F^2
\]</span></p>
<p>令</p>
<p><img src="p2-equ5.png" /></p>
<p>相当于N行C列，每个元素是一个 d<span
class="math inline">\(\times\)</span>d 的块的矩阵；显然有 <span
class="math inline">\(\operatorname{rank}\tilde{W} \le K\)</span>；</p>
<p>结合上述两点，我们可以将优化问题等价为：</p>
<p><span class="math display">\[
\begin{array}{ll}
  \text{min} &amp;\lVert W - \tilde{W}\rVert _F^2\\
  \text{s.t.} &amp;\operatorname{rank}\tilde{W} \le K
\end{array}
\]</span></p>
<p>该问题可以用 <span class="math inline">\(W\)</span>
的奇异值分解结果得到，因此有：</p>
<p><img src="p2-equ4.png" /></p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>模型压缩</tag>
        <tag>卷积分解</tag>
      </tags>
  </entry>
  <entry>
    <title>ConvNeXt论文阅读</title>
    <url>/2022/09/18/ConvNeXt/</url>
    <content><![CDATA[<img src="/2022/09/18/ConvNeXt/fig1.png" class="">
<p><strong>论文</strong>：<strong>A ConvNet for the 2020s</strong></p>
<p><strong>作者</strong>：<em>Zhuang Liu, Hanzi Mao, Chao-Yuan Wu,
Christoph Feichtenhofer, Trevor Darrell, Saining Xie</em>;</p>
<p><strong>一作单位</strong>：Facebook AI Research, UC Berkeley</p>
<p><strong>录用情况</strong>：CVPR'2022</p>
<blockquote>
<p>2020年以来，很多多层级的Transformer架构被提出，获得了优异的性能；本文找到一种更加现代的卷积神经网络，能够与Transformer相匹敌；本文的实验显然需要庞大的机器才能完成，但是这些结果给CNN爱好者们一个新的方向，正如题目所说，在2020年代，该如何设计、训练CNN？</p>
</blockquote>
<span id="more"></span>
<h2 id="convnets-v.s.-hierarchical-transformers">ConvNets v.s.
Hierarchical Transformers</h2>
<p>滑动窗口机制带来了，等变性 (translation
equivariance)，共享参数的计算；</p>
<p>Hierarchical Transformers
是为了解决ViT在面对高分辨率输入计算量大，训练难，尺度单一提出的架构，引入例如
patch merging 这样的操作，得到CNN式的多尺度特征图；</p>
<h2 id="convnet---convnext">ConvNet -&gt; ConvNeXt</h2>
<ol type="1">
<li>按照与DeiT和Swin Transformer类似的配置训练ResNet，性能从76.1% 提升到
78.8%；接下来的训练保持此训练配置；</li>
<li>按照Swin各层块比例为1:1:3:1，将ResNet-50的层数从(3,4,6,3)改变为(3,3,9,3)，性能从78.8%
提升到 79.4%；接下来的训练保持此层数；</li>
<li>将卷积网络第一层常用的stem
conv改为ViT所使用的patchify，即使用步长为4的4x4卷积，性能从79.4%变为79.5%；接下来的训练保持此设置；</li>
<li>在high
level上，策略是使用更多的组数、更大的宽度；使用深度可分离卷积，在空间和通道维度依次加权求和；使用depth-wise卷积减少FLOPs，增加宽度补偿性能，最终达到了80.5%；</li>
<li>使用inverted
bottleneck（两头细中间粗），尽管在depth-wise卷积处的计算量增加了，但是在下采样处shortcut的1x1卷积计算量减少了，最终网络的FLOPs是减少的(4.6G)，并且性能从80.5%提升到了80.6%；</li>
<li>更大的卷积核：将bottle neck中的depth
wise卷积提前（性能减小到79.9%，FLOPs减小到4.1G），之后将卷积核尺寸从3x3增加到7x7，性能从79.9提升到80.6%；</li>
<li>用GELU替换RELU，性能没有变化；</li>
<li>由于在Transformer中的每个MLP
Block中只有一个非线性函数，因此在每个残差块中仅保留一个GELU激活函数（两个1x1卷积之间），性能提升到81.3%；</li>
<li>在每个残差块中，将之保留在第一个1x1卷积之前的BN，性能提升到81.4%；</li>
<li>用LN代替每个残差块中的BN，性能提升到81.5%；</li>
<li>在ResNet中，使用步长为2的残差块来进行下采样，而在Swin中，下采样是用专门的模块实现的，并且实验表明，在分辨率改变时使用LN有助于使训练更加稳定，在Stem之后，每个下采样层之前，以及GAP之后使用LN，性能提升到82.0%;</li>
</ol>
<h2 id="实验">实验</h2>
<p>文章之后的部分记录了更多实验：不同尺寸的模型、与ViT相同的架构（无downsample)，在检测和分割任务上的表现；</p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
  </entry>
  <entry>
    <title>ACNet论文阅读</title>
    <url>/2022/10/05/ACNet/</url>
    <content><![CDATA[<img src="/2022/10/05/ACNet/image-20221005224216241.png" class="">
<p><strong>论文</strong>：<a
href="https://arxiv.org/pdf/1908.03930.pdf">ACNet: Strengthening the
Kernel Skeletons for Powerful CNN via Asymmetric Convolution
Blocks</a></p>
<p><strong>作者</strong>：<em>Xiaohan Ding</em> , <em>Yuchen Gu</em> ,
<em>Guiguang Ding</em> , <em>Jungong Han</em></p>
<p><strong>录用情况</strong>：ICCV'2019</p>
<p><strong>第一作者单位</strong>：Beijing National Research Center for
Information Science and Technology (BNRist); School of Software,
Tsinghua University, Beijing, China</p>
<blockquote>
<p>本文是结构重参数化的第一篇文章，使用1D的卷积来增强方形卷积，并且在推理前进行结构重参数化，用这样的块替换先前的网络架构进行训练，或多多少的都在CIFAR和ImageNet上获得了提升，原因可能是1D卷积对旋转、翻转变换具有更好的鲁棒性以及。网络上已经有很多介绍具体方法的文章（<a
href="https://zhuanlan.zhihu.com/p/131282789">知乎 -
【CNN结构设计】无痛的涨点技巧：ACNet</a>），笔者在这里主要关注论文最后剪裁(pruning)实验，感觉还挺独特的。</p>
</blockquote>
<span id="more"></span>
<h2 id="方法">方法</h2>
<p>本文提出的ACB块使用$dd $, <span class="math inline">\(d\times
1\)</span>, <span class="math inline">\(1\times
d\)</span>三种卷积并行训练，并在每个分支使用BN；在推理前，使用吸BN技巧以及卷积对加法的可分配性，将3个分支转换为与之等价的1个<span
class="math inline">\(d\times d\)</span>卷积用于推理。</p>
<p><img src="image-20221006220034278.png" /></p>
<p>作者提到，推理时等价并不意味着训练时等价，由于初始化的随机性，并行的几个卷积核可能朝着不同的方向优化，这也引出一个作者的隐忧：如果并行分支的卷积核符号相反，在相加后反而抵消彼此的作用从而削弱性能。不过作者通过后续实验观察到合并后模型每一层卷积的骨干都得到了提升。</p>
<blockquote>
<p>以3x3卷积为例，卷积的骨干(skeleton)指中心十字形位置上的参数，占整体的5/9；其余的称为边角(corner)，占整体的4/9；</p>
</blockquote>
<p>作者将<span class="math inline">\(d\times 1\)</span>, <span
class="math inline">\(1\times
d\)</span>形状的卷积称为非对称卷积，并且简单的理论和消融实验证明了这种卷积对于翻转、旋转具有较好的鲁棒性：</p>
<p><img src="image-20221006220145664.png" /></p>
<h2 id="实验">实验</h2>
<p>性能实验部分，作者使用ACB代替经典网络中的Conv-BN层，分别在CIFAR和ImageNet上测试性能，这种提升，是在推理时不需要任何额外代价（更多参数or更多计算量）就能够获得的；</p>
<p>消融实验部分，作者证明了3个分支都对性能有帮助，并且对于水平翻转，旋转有更好的鲁棒性；</p>
<h3 id="剪裁实验">剪裁实验</h3>
<p>剪裁实验部分，是笔者最感兴趣的部分，作者希望证明，ACB能够提升方形卷积核的骨架；</p>
<h4
id="不同训练方式下剪裁卷积核不同位置对性能的影响">不同训练方式下，剪裁卷积核不同位置对性能的影响</h4>
<p>剪裁，就是对于训练好的模型，将其卷积核的部分位置以一定比率设置为0，之后再进行推理，以比较卷积核不同部分的贡献差异。有三种剪裁方式：</p>
<ul>
<li>corner: 按一定比例将卷积核的四角设置为0，上限为4/9=44.4%；</li>
<li>skeleton: 按一定比例将卷积核的骨架设置为0；</li>
<li>global: 卷积核所有位置都时同等概率地被设置为0；</li>
</ul>
<p>作者在CIFAR 10上统计top1 acc。每个剪裁率下多次实验，绘制mean<span
class="math inline">\(\pm\)</span> std的曲线：</p>
<h5 id="原来的训练方式">原来的训练方式</h5>
<p><img src="image-20221006223639906.png" /></p>
<p>这幅图表明了，原本的卷积核就存在着，骨干比边缘贡献更大的情况；</p>
<h5 id="acnet中的训练合并方式">ACNet中的训练、合并方式</h5>
<p>即在合并分支时，将1x3和3x1卷积加到3x3卷积的骨干位置上，下图中，corner时的剪裁下，性能始终能保持在60%以上：</p>
<p><img src="image-20221006224305290.png" /></p>
<h5 id="acnet中的训练但合并到边缘">ACNet中的训练但合并到边缘</h5>
<p>即在合并分支时，将1x3和3x1卷积加到3x3卷积的右边和下边，为保证等价性，对应分支的吸BN也要移到右下边，这种情况下，尽管性能相比于ACNet只下降了0.42%，但在剪裁实验中，剪裁corner的仍然是最好的（相比前两种也大打折扣了）；而剪裁被增强的右下角5个位置，与剪裁左上角4个位置，没有特别明显的差异（后者稍微好一点），意味着尽管我们增强了边界，也不能忽视卷积核的其他部分。</p>
<p><img src="image-20221006225839924.png" /></p>
<h4 id="卷积核的绝对值">卷积核的绝对值</h4>
<p>作者将训练好的模型的各卷积层的各个通道求和，并使用最大值归一化，之后再对各个层归一化的结果求平均，得到一个“幅值”核，希望能够用来代表各个位置上的重要性，三种训练and合并方式下得到的结果如下：</p>
<p><img src="image-20221006230243484.png" /></p>
<p>能够吻合先前的结论。</p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>结构重参数化</tag>
      </tags>
  </entry>
  <entry>
    <title>Diverse Branch Block论文阅读</title>
    <url>/2022/11/07/DBB/</url>
    <content><![CDATA[<blockquote>
<p>本文是重参数化的作者把这一方法做“透”了的一篇文章，在训练时用多路可以重参数化的结构不同的分支，在推理时合并起来；相比于先前的工作（ACNet和RepVGG），训练时的模型本身就在ImageNet上提点了，结合重参数化关于推理速度的卖点，这波是性能与速度的双赢；本文将主要关注代码实现；</p>
</blockquote>
<span id="more"></span>
<h3 id="训练时结构">训练时结构</h3>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
  </entry>
  <entry>
    <title>英语学习笔记（持续更新）</title>
    <url>/2022/11/01/English/</url>
    <content><![CDATA[<p>s</p>
<span id="more"></span>
<h2 id="collection-of-sentences">Collection of sentences</h2>
<ol type="1">
<li><p>With irrigation water also came potential problems, the most
obvious being the susceptibility of low-lying farmlands to disastrous
flooding from and the longer-term problem of salinization.</p>
<p><strong>译文</strong>：有了灌溉水也会带来一些潜在的问题，最明显的就是低洼农田对灾难性洪水的敏感性以及长期的盐碱化。</p>
<p><strong>缩写</strong>：irrigation water <span
class="math inline">\(\rightarrow\)</span> low-lying farmlands are
susceptible to flooding and sanlinization.</p>
<p><strong>分析</strong>：这句话乍一看没有找到主语和谓语，仔细分析唯一可能做谓语的只有came，这里是一个介词短语做主语的用法，相似的句子还有：</p>
<ul>
<li><em>Between the six and seven will suit me.</em></li>
<li><em>After the meeting is the time to visit.</em></li>
</ul>
<p>此外，作为名词性用法，介词短语还可以做宾语或同位语，参考<a
href="https://zhuanlan.zhihu.com/p/26088391">知乎 -
介词短语就这三类用法</a>；</p></li>
<li><p>Coordinated social interactions tend to be weak when a colony is
first forming, but true colonies provide extra benefits.</p>
<p><strong>译文</strong>：当一个群体刚形成时，协调的社会互动往往很弱，但真正的群体聚居提供了额外的好处。</p>
<p><strong>缩写</strong>：Colonies provide benefits.</p>
<p><strong>分析</strong>：语法上不难。但是陌生的概念、用词以及逻辑关系让人摸不着头脑。什么是协调的社会互动？为什么要用一个转折强调true？</p></li>
<li><p>This behavior is especially valuable when the offsite food
supplies are restricted or variable in location, as are swarms of aerial
insects harvested by swallows.</p>
<p><strong>译文</strong>：当食物供给有限或者位置发生变化时，这种行为尤其有价值，如燕子捕获成群的飞虫。</p>
<p><strong>缩写</strong>：When foods are restricted, this behavior is
valuable.</p>
<p><strong>分析</strong>：as表示正如，引导一个被动的状语从句，正常语序应该是as
swarms of aerial insects are harvested by swallows. 但是这里倒装了？<a
href="https://www.usingenglish.com/forum/threads/as-are.125928/">这里给出了另一个类似的句子</a>：</p>
<ul>
<li><em>The city center is also being revitalized, as are the main
communication routes.</em></li>
</ul></li>
<li><p>On balance, the advantages of colonial nesting clearly outweigh
the disadvantages, given the many times at which colonial nesting has
evolved independently among different groups of birds.</p>
<p><strong>译文</strong>：总得来说，考虑到群体筑巢已经在不同鸟类群体中独立的进化了很多次，群体筑巢的好处还是明显大于坏处。</p>
<p><strong>缩写</strong>：many times evolution <span
class="math inline">\(\rightarrow\)</span> colonial nesting adva- <span
class="math inline">\(&gt;\)</span> disadva-</p>
<p><strong>分析</strong>：这句话可能是想强调多次的独立进化，证实了上述观点，所以把at
many times写成了一个从句。</p></li>
<li><p>Still lacking, however, is a general framework for testing
different hypotheses for the evolution of coloniality.</p>
<p><strong>译文</strong>：然而，对于测验不同群体进化的假说，至今仍然缺乏一个通用的框架</p>
<p><strong>缩写</strong>：different hypo- for evolu- of colon- lack a
framework</p>
<p><strong>分析</strong>：正确理解这句话的关键是要知道，lacking是一个形容词而不是什么动名词，因此这是一个倒装句。</p></li>
<li><p>Hairy-footed gerbils live in vegetated islands in a sea of sand
in the Namib Desert of southern Afria. Habitat use was determined by
tracks in the sand and by how quickly they gave up feeding at stations
containning seeds mixed with sand.</p>
<p><strong>译文</strong>：毛足的gerbils(沙鼠)生活在非洲南部Namib沙漠一片植被覆盖的沙土上。通过gerbils在沙子上的踪迹和它们在放弃混有种子的沙地的速度确定它们的栖息地。</p>
<p><strong>缩写</strong>：H.F. G. live in vegetated islands. Tracks
&amp; <span class="math inline">\(\vec{v}\)</span> of giving up feeding
<span class="math inline">\(\rightarrow\)</span> G.'s habitat.</p>
<p><strong>分析</strong>：第二句话理解的困难之处在于，一个被动句，and并列两个谓语动词，但是在一个分句中的tracks让人摸不着头脑，如果是their
tracks好像还好理解一些；and后句子的station，总是联想到“车站”，这里只是一个抽象的概念，类似于place；关键还是缺乏逻辑，这句话主要在说，研究栖息地的方法，这样下文才好进一步说明观察到沙鼠对于栖息地是如何选择的。</p></li>
</ol>
<h2 id="sentences-with-question">Sentences with Question</h2>
<h3 id="tpo-2.2">TPO 2.2</h3>
<p><strong>P1:</strong> It should be obvious that cetaceans—whales,
porpoises, and dolphins—are mammals. They breathe through lungs, not
through gills, and give birth to live young. <u>Their streamlined
bodies, the absence of hind legs, and the presence of a fluke1 and
blowhole2 cannot disguise their affinities with land dwelling
mammals</u>. <u>However, unlike the cases of sea otters and pinnipeds
(seals, sea lions, and walruses, whose limbs are functional both on land
and at sea), it is not easy to envision what the first whales looked
like.</u> Extinct but already fully marine cetaceans are known from the
fossil record. How was the gap between a walking mammal and a swimming
whale bridged? Missing until recently were fossils clearly intermediate,
or transitional, between land mammals and cetaceans.</p>
<blockquote>
<p><strong>Q1:</strong> In paragraph 1, what does the author say about
the presence of a blowhole in cetaceans?</p>
<p>A. It clearly indicates that cetaceans are mammals.</p>
<p><u>B. It cannot conceal the fact that cetaceans are mammals.</u></p>
<p>C. It is the main difference between cetaceans and land-dwelling
mammals.</p>
<p>D. It cannot yield clues about the origins of cetaceans.</p>
</blockquote>
<p>cannot disguise their
affinities，不能伪装亲近关系，也就是前述证据不足以证明cetaceans are
mammals。如果只注意到了cannot disguise
并理解为这些证据使得cetaceans不能再伪装成mammals，则会得到完全相反的答案。</p>
<blockquote>
<p><strong>Q2:</strong> Which of the following can be inferred from
paragraph 1 about early sea otters?</p>
<p><u>A. It is not difficult to imagine what they looked like.</u></p>
<p>B. There were great numbers of them.</p>
<p>C. They lived in the sea only.</p>
<p>D. They did not leave many fossil remains.</p>
</blockquote>
<p>注意题目中问的是sea otters，文中提到在unlike sea otters条件下，it is
not easy to envision。</p>
]]></content>
  </entry>
  <entry>
    <title>字典学习卷积模块以加速网络推理</title>
    <url>/2023/01/19/LCNN/</url>
    <content><![CDATA[<img src="/2023/01/19/LCNN/fig1.png" class="">
<p><strong>论文</strong>：<a
href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Bagherinezhad_LCNN_Lookup-Based_Convolutional_CVPR_2017_paper.pdf"><strong>LCNN:
Lookup-based Convolutional Neural Network</strong></a></p>
<p><strong>作者</strong>：<em>Hessam Bagherinezhad, Mohammad Rastegari,
Ali Farhadi</em></p>
<p><strong>一作单位</strong>：University of Washington, XNOR.AI</p>
<p><strong>录用情况</strong>：CVPR'2017</p>
<blockquote>
<p>本文将卷积运算替换为学习一个字典和一组线性组合的权重，并提出了直接从头开始训练的方式，除了一般的分类实验，本文还证明了这种方法在few
shot
learning的提升，以及对网络初始迭代时收敛水平的提升等；在AlexNet的架构上的修改，该方法获得了55.1%的ImageNet
top1准确率以及3.2倍的加速比；</p>
</blockquote>
<span id="more"></span>
<h2 id="方法">方法</h2>
<h3
id="在不同卷积核之间不同空间位置上建立共享的字典">在不同卷积核之间、不同空间位置上建立共享的字典</h3>
<p>本文将 <span class="math inline">\(n\)</span> 个 <span
class="math inline">\(m\times k_w\times k_h\)</span> 的视为 <span
class="math inline">\(nk_wk_h\)</span> 个长度为 <span
class="math inline">\(m\)</span> 的向量，并希望学习一个的字典 <span
class="math inline">\(D\in\mathbb{R}^{k\times m}\)</span>，以及在每个
(t, r, c) 位置使用的 <span class="math inline">\(s\)</span>
个字典向量索引 <span class="math inline">\(I_{[t,r,c]}\)</span>
和对应的权重 <span
class="math inline">\(C_{[t,r,c]}\)</span>，从而能够将一个卷积 <span
class="math inline">\(W\in \mathbb{R}^{m\times k_w\times k_h}\)</span>
的 <span class="math inline">\(r\)</span> 行 <span
class="math inline">\(c\)</span>
列的权重表示为如下字典向量的线性组合的形式：</p>
<p><span class="math display">\[
W_{[:, r, c]} = \sum_{t=1}^s C_{[t,r,c]} \cdot D_{[I_{[t,r,c]},:]} \quad
\forall r, c
\]</span></p>
<p>为了将这种表示下的卷积核应用到卷积计算中（在卷积运算中拆出 <span
class="math inline">\(W_{[:, r, c]}\)</span>
一项），作者从卷积核每个位置与输入做了怎样的运算的视角来看，整个卷积相当于输入与这些位置上卷积向量做1x1卷积后，经过位置相关的偏移操作，将结果求和；表达为如下形式：</p>
<p><span class="math display">\[
\begin{split}
  X*W &amp;= \sum_{r,c}^{k_h, k_w}\operatorname{shift}_{r,c}(X * W_{[:,
r, c]})\\
  &amp;= \sum_{r,c}^{k_h, k_w}\operatorname{shift}_{r,c}(X *
(\sum_{t=1}^s C_{[t,r,c]} \cdot D_{[I_{[t,r,c]},:]}))\\
  &amp;= \sum_{r,c}^{k_h, k_w}\operatorname{shift}_{r,c}(\sum_{t=1}^s
C_{[t,r,c]} (X * D_{[I_{[t,r,c]},:]}))\\
  &amp;= \sum_{r,c}^{k_h,
k_w}\operatorname{shift}_{r,c}\left(\sum_{t=1}^s C_{[t,r,c]}
S_{I_{[t,r,c]}}\right )\\
\end{split}
\]</span></p>
<p>这意味着，对于共享同一个字典中的同一个层的所有卷积，我们可以先计算出
<span class="math inline">\(X\)</span> 与字典中所有向量的1x1卷积结果
<span
class="math inline">\(S_{[i,:,:]}=X*D_{i,:}\)</span>，之后只需要索引到需要的结果，加权求和，放到需要的位置即可；</p>
<p>至此，只要 <span class="math inline">\(s\)</span>, <span
class="math inline">\(k\)</span>
设置得当，理论上计算量相比原来的卷积运算已经大大减少；</p>
<p>但是，我们关心的第一个问题是，<strong>如何将索引、加权和以及shift的过程向量化，从而极大地利用现有的高度并行化的算子，从而真正提升前向传播速度？</strong></p>
<p>第二个问题是，在使用了向量化的算子之后，并应用了其默认的梯度下降优化，那么<strong>如何在优化过程中建立并保持稀疏性？</strong>
（<span class="math inline">\(s\ll k\)</span>）</p>
<h3 id="训练lcnn">训练LCNN</h3>
<p>利用如下事实：</p>
<p>假设 <span class="math inline">\(T\in \mathbb{R}^{k\times k_w\times
k_h}\)</span> 是独热张量，只在 <span
class="math inline">\(T_{[t,r,c]}=1\)</span>，其余位置均为0，那么有
<span class="math inline">\(S*T =
\operatorname{shift}_{r,c}(S_{[t,:,:]})\)</span>；</p>
<p>我们可以在前向传播过程中学习一个稀疏张量 <span
class="math inline">\(P\in\mathbb{R}^{k\times w\times
h}\)</span>，其中，<span class="math inline">\(P_{[:,r,c]}\)</span>
表示了在r,c位置的稀疏权重：</p>
<p><span class="math display">\[
P_{j,r,c} = \left \{
\begin{matrix}
  C_{t,r,c}, &amp;\exists t: I_{t,r,c} = j\\
  0, &amp;\text{otherwise}
\end{matrix}
\right .
\]</span></p>
<p>这样一来，我们可以使用卷积来表示索引、加权和、偏移求和的过程，下式和图二说明了这一点：</p>
<p><span class="math display">\[
\begin{split}
  X*W &amp;= \sum_{r,c}^{k_h,
k_w}\operatorname{shift}_{r,c}\left(\sum_{t=1}^s C_{[t,r,c]}
S_{I_{[t,r,c]}}\right )\\
  &amp;= \sum_{r,c}^{k_h, k_w}\operatorname{shift}_{r,c} (S *
P_{[:,r,c]})\\
  &amp;= S*P
\end{split}
\]</span></p>
<p><img src="fig2.png" /></p>
<p>并且，如果我们能够在训练时得到稀疏的 <span
class="math inline">\(P\)</span>，就可以通过寻找非零元素的方式得到 <span
class="math inline">\(I\)</span> 和 <span
class="math inline">\(C\)</span>，用于高效的推理；</p>
<p>接下来，作者通过添加正则项使得 <span class="math inline">\(P\)</span>
稀疏，按照对 <span class="math inline">\(P\)</span> 的定义，应该有 <span
class="math inline">\(\lVert P_{[:,r,c]}\rVert _{\mathcal{L}_0} =
s\)</span>；但是 <span class="math inline">\(\mathcal{L}_0\)</span>
norm（非零元素的数量）是不连续的函数，不可导，因此作者将 <span
class="math inline">\(P_{[:,r,c]}\)</span> 的从大到小排列后，第 <span
class="math inline">\(s\)</span> 个之后的元素记为0，并计算 <span
class="math inline">\(\mathcal{L}_1\)</span>
norm（元素绝对值之和）；期望通过将前 <span
class="math inline">\(s\)</span> 个元素的绝对值变小，来让被在zero
out的那部分元素很接近0；</p>
<p>至此并没有完全达到稀疏化的目的，因为当 <span
class="math inline">\(P_{[t,r,c]}\)</span>
缩小到0时，其梯度不一定为0，这样在下一次迭代过程中其绝对值又会变大；</p>
<p>我们可以为 <span class="math inline">\(P\)</span>
包裹一层阈值函数，当 <span class="math inline">\(|P_{[t,r,c]}|\)</span>
的值小于 <span class="math inline">\(\epsilon\)</span>
时，该位置直接置0，意味着回传的梯度也是0，那么这里就永远保持在0；</p>
<h3 id="few-shot-learning">Few-shot learning</h3>
<p>对于普通的CNN，将预训练的模型的分类头替换成没有任何先验知识的分类头（参数量很多，容易过拟合），对少样本的类别进行学习；</p>
<p>对于LCNN，可以使用预训练模型分类层的字典，因此只用学习权重和索引，且对于其他层，可以固定索引，只微调权重；</p>
<h3 id="few-iteration-learning">Few-iteration learning</h3>
<p>浅层网络学习到的字典 <span
class="math inline">\(D\in\mathbb{R}^{k\times
m}\)</span>，可以与通道数为 <span class="math inline">\(m\)</span>
的深层网络共享；加速在段周期内的收敛过程；</p>
<h2 id="实验">实验</h2>
<p>通过调整字典大小 <span class="math inline">\(k\)</span>，正则化参数
<span class="math inline">\(\lambda\)</span> 和阈值 <span
class="math inline">\(\epsilon\)</span>
来调整模型学习后的稀疏率，从而调整推理时加速比；</p>
<blockquote>
<p>s 是间接调整的</p>
</blockquote>
<p>每一层的阈值和正则化参数与该层初始化方差成正比，是为了让每层的系数水平相近；</p>
<p>对于第一层而言，由于输入通道是3通道的（字典向量长度为3）因此字典最多只需要3个向量（再多就一定有线性相关了）；</p>
<p>对于线性层，将其视为1x1卷积后，输入通道比较大，因此字典大小也很大；其他层的字典大小作者凭借经验设定一组候选值进行实验；</p>
<p>实验结果与分析详见原文；</p>
<h2 id="总结">总结</h2>
<p>基于本文的工作，笔者有一些朴素的思考：</p>
<p>本文通过将传统卷积视为1x1卷积结果的shift和，认为这 <span
class="math inline">\(nk_wk_h\)</span> 个 <span
class="math inline">\(\mathbb{R}^m\)</span> 空间的卷积向量可以表示为
<span class="math inline">\(\mathbb{R}^k\)</span>
子空间的坐标（字典向量是基，权重是坐标），该过程利用了输入 <span
class="math inline">\(m\)</span>
个通道的冗余性；接着，作者假设，这些坐标是稀疏的，每个坐标的非零元素不超过
<span class="math inline">\(s\)</span> 个，这些利用的是 <span
class="math inline">\(nk_wk_h\)</span> 个向量的表示可以划分到 <span
class="math inline">\(\begin{pmatrix} nk_wk_h\\s \end{pmatrix}\)</span>
个簇中，这样一看，我们似乎可以找到一种从已经训练好的卷积核初始化字典的方法：</p>
<ol type="1">
<li>找到卷积核的低维投影（如PCA等）；</li>
<li>在低维空间使用聚类，并限制这些类中心坐标是稀疏的；</li>
</ol>
<p>在此之前，需要验证一下本文得到的字典的秩是否为 <span
class="math inline">\(k\)</span>？关于2中提到的聚类中心坐标稀疏的限制如何实现？以及，我们是否可以换别的维度进行第1步的降维？</p>
<blockquote>
<p>机器学习中有个经典方向：字典学习，笔者应该回去补一补了；</p>
</blockquote>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>模型压缩</tag>
        <tag>字典学习</tag>
      </tags>
  </entry>
  <entry>
    <title>探索卷积网络的线性性质来进行推理时加速</title>
    <url>/2022/10/13/Linear-Structure-Within-Conv/</url>
    <content><![CDATA[<img src="/2022/10/13/Linear-Structure-Within-Conv/image-20221017160950770.png" class="">
<p><strong>论文：</strong><a
href="https://arxiv.org/pdf/1404.0736.pdf">Exploiting Linear Structure
Within Convolutional Networks for Efficient Evaluation</a></p>
<p><strong>作者：</strong> <em>Emily Denton</em>, <em>Wojciech
Zaremba</em>, <em>Joan Bruna</em>, <em>Yann LeCun</em>, <em>Rob
Fergus</em></p>
<p><strong>录用情况：</strong>Neurips'2014</p>
<p><strong>第一作者单位：</strong>Dept. of Computer Science, Courant
Institute, New York University</p>
<blockquote>
<p>笔者最近在做一些用SVD分解卷积核的小实验，并且也在想该如何设计目标函数，让重参数化这件事变成可学习的？然后我发现早在14年就有人做了相关问题的研究；笔者看来，这篇文章的贡献为：</p>
<ul>
<li>给出了一种评估压缩后卷积核好坏的方式，并且通过在线学习的方式，修改了反向传播的梯度（不确定是不是这样）</li>
<li>归纳并尝试了对于高维张量的两种分解方法：SVD分解和外积分解；</li>
<li>结合聚类方法，针对浅层卷积和深层卷积的不同性质，设计了单色估计方法和双色估计方法；</li>
<li>给出一种微调方式：从最浅层开始，压缩一层，然后微调其上所有层，直至性能恢复，如此循环，看能最多压缩多少层；</li>
</ul>
<p>作者在实验部分用了比较老的模型，仅4层卷积和3层全连接层，现在来看非常简单，但是很适合做一个demo，然而作者原始的<a
href="">C++代码链接</a>已经失效了</p>
</blockquote>
<span id="more"></span>
<h2 id="基础知识">基础知识</h2>
<p>有关主成分分析和奇异值分解的知识在<a
href="https://ashun989.github.io/2022/08/10/linear-algebra/">线性代数要点总结</a>的最后部分有一些干货；</p>
<h3 id="mahalanobis-distance">Mahalanobis distance</h3>
<p>马氏距离用于衡量一个点<span
class="math inline">\(P\)</span>到一个分布<span
class="math inline">\(D\)</span>的距离，即<span
class="math inline">\(P\)</span>距离<span
class="math inline">\(D\)</span>的质心有多少标准差距离，因而也可以用于衡量两个分布的距离；马氏距离考虑到了数据集的相关性、并且具有尺度不变性，说人话就是，无论数据是否沿着各个轴分布，无论在各个维度上的尺度是否一致，如下图所示，马氏距离都能得到A相比于B，与分布的距离更远；</p>
<p><img src="v2-3cee35b79d272dda86e2604c160934ee_720w.jpeg" /></p>
只要将原始数据平移到以原点为中心，再旋转至主成分方向再除以各个方向的方差，之后再计算欧式距离，这就是马氏距离公式的由来，设数据为<span
class="math inline">\(X\in \mathbb R^{m\times n}\)</span>，其均值为<span
class="math inline">\(\mu_X\in \mathbb R^n\)</span>，其协方差矩阵为<span
class="math inline">\(\Sigma_X \in \mathbb R^{n\times
n}\)</span>，<strong>假设协方差矩阵是满秩的</strong>（即没有为0的特征值，又因为协方差矩阵半正定，意味着所有特征值为正），其特征值按照非增顺序依次为<span
class="math inline">\(\lambda_1, \lambda_2, \lambda_n\)</span>；<span
class="math inline">\(Y\)</span>是<span
class="math inline">\(X\)</span>平移至原点为中心后再旋转至主成分方向的形式，即<span
class="math inline">\(Y=U^TX\)</span>，其中<span
class="math inline">\(U\)</span>是单位正交矩阵，使得<span
class="math inline">\(\text{diag}\{\lambda_1,\cdots,\lambda_n\}=U^T\Sigma_XU\)</span>，对于数据点<span
class="math inline">\(x\)</span>，其到<span
class="math inline">\(X\)</span>的马氏距离的平方为： $$
<span class="math display">\[\begin{aligned}
D_M^2 &amp;= \frac{U^T(x_1-{\mu_X}_1)}{\lambda_1} + \cdots +
\frac{U^T(x_n-{\mu_X}_n)}{\lambda_n}\\
&amp;= (y_1, \cdots, y_n)\text{diag}\{\frac 1 {\lambda_1}, \cdots, \frac
1 {\lambda_n} \}
\left(
\begin{array}{c}
y_1\\
\vdots\\
y_n
\end{array}
\right)\\
&amp;= y^T(U^T\Sigma_XU)^{-1}y\\
&amp;= (x-\mu_X)^TUU^T\Sigma_X^{-1}UU^T(x-\mu_X)\\
&amp;= (x-\mu_X)^T\Sigma_X^{-1}(x-\mu_X)

\end{aligned}\]</span>
<p><span class="math display">\[
因此，点$x$到分布$X$的马氏距离为：
\]</span> D_M = <span class="math display">\[
如果$x$与$y$是两个服从同一分布且协方差矩阵为$\Sigma$的随机变量，则他们的距离为：
\]</span> D_M = $$
如果协方差矩阵是单位矩阵，则退化成欧式距离；如果协方差矩阵是对角矩阵，则退化成归一化的欧氏距离；</p>
<h3 id="度量学习">度量学习</h3>
<p>详见西瓜书第10章；</p>
<h3 id="dirac-distribution">Dirac distribution</h3>
<p>这种分布是理想电荷的分布：集中在一点。详见<a
href="https://wuli.wiki/online/Delta.html">小时百科 - 狄拉克<span
class="math inline">\(\delta\)</span>函数</a>。笔者理解为，实现时即one-hot编码；</p>
<h3 id="矩阵的范数">矩阵的范数</h3>
<p><span class="math display">\[
||A||_p = (\sum_{i,j}|a_{i,j}|^p)^{1/p}
\]</span></p>
<p>其中，<span class="math inline">\(||A||_F=||A||_2\)</span></p>
<h2 id="方法">方法</h2>
<h3 id="符号定义">符号定义</h3>
<p><img src="image-20221017161636024.png" /></p>
<h3 id="估计度量">估计度量</h3>
<p>我们希望获得一个<span
class="math inline">\(\tilde{W}\)</span>，来估计网络中的<span
class="math inline">\(W\)</span>，能够在减少计算量的情况下保持性能；一个非常朴素的想法是最小化<span
class="math inline">\(||\tilde W -
W||_F\)</span>，但是，这假设了在模型权重张量上每个位置的参数都对预测做出均等的贡献，这是不合理的；作者提出了两种度量：</p>
<blockquote>
<p>这两种度量无非都是在给<span
class="math inline">\(W\)</span>加权，然后预测到这个加权的张量的一个近似常量，对于文中使用的SVD分解，这不是一个可迭代的算法（不能利用pytorch的自动微分偷懒了），因此直接把近似的目标设置为加权后的<span
class="math inline">\(W\)</span>，得到的结果再乘以权重的倒数即可；</p>
</blockquote>
<h4 id="马氏距离度量">马氏距离度量</h4>
<p>记<span class="math inline">\(\Theta = \{W_1, \cdots,
W_S\}\)</span>是<span
class="math inline">\(S\)</span>层网络的参数，<span
class="math inline">\(U(I;\Theta)\)</span>是网络对于输入图像<span
class="math inline">\(I\)</span>的softmax层的输出（各个类别的概率）；大小为<span
class="math inline">\(N\)</span>的训练集，训练对记为<span
class="math inline">\((I_n, y_n)\)</span>，记<span
class="math inline">\(U(I_n;\Theta)\)</span>中概率最高的而索引不等于真实标签<span
class="math inline">\(y_n\)</span>的<span
class="math inline">\(h\)</span>个索引组成序列<span
class="math inline">\(\{\beta_n\}\)</span>，那么对于第<span
class="math inline">\(s\)</span>层，在反向传播过程中为权重<span
class="math inline">\(W_s\)</span>计算一个（额外的）导数项： <span
class="math display">\[
d_{n,l,s}= \nabla_{W_s}(U(I_n;\Theta)-\delta(i-l)), n\le N, l\in
\{\beta_n\}, s\le S
\]</span> 其中，<span
class="math inline">\(\delta(i-l)\)</span>是以<span
class="math inline">\(l\)</span>为中心的狄拉克分布，应该就是在<span
class="math inline">\(l\)</span>为1其他位置为0的one-hot编码向量；</p>
<p>笔者认为，作者的意思是在一般的损失函数中添加了一个正则项（但只是在推理时计算梯度），目的是表示各层权重的各个位置对于避开那几个“最危险”的错误的能力；
<span class="math display">\[
\frac 1 2\sum_{n}^N\sum_{l\in \{\beta_n\}}
||U(I_n;\Theta)-\delta(i-l)||_2^2
\]</span> 有了这个正则项之后，作者再来计算<span
class="math inline">\(\tilde W\)</span>到<span
class="math inline">\(W\)</span>的马氏距离，将<span
class="math inline">\(\tilde W - W\)</span>可以展平成一个高维向量<span
class="math inline">\(w\in \mathbb R^P\)</span>，其导数<span
class="math inline">\(d\)</span>也可以展平成同样长度的向量，而对于<span
class="math inline">\(N\)</span>个样本在<span
class="math inline">\(h\)</span>个易错位置得到的正则化损失在<span
class="math inline">\(s\)</span>层的导数是一个基数为<span
class="math inline">\(N\times h\)</span>的导数向量的集合<span
class="math inline">\((d_{n,l,s})_{n,l}\)</span>，<span
class="math inline">\(\Sigma \in \mathbb R^{P\times
P}\)</span>是导数集合的协方差矩阵，精确的马氏距离为： <span
class="math display">\[
||\tilde W - W||^2_{maha} = w^T\Sigma^{-1}w
\]</span> 考虑到求<span
class="math inline">\(\Sigma^{-1}\)</span>的成本极高，因此作者进行了简化，即只使用<span
class="math inline">\(\Sigma\)</span>的对角线部分（<span
class="math inline">\(P\)</span>个维度的方差），因此，近似的马氏距离为：
<span class="math display">\[
||\tilde W-W||_\widetilde{maha} = \sum_p^P\alpha_pw_p, \ \ \text{where}
\ \ \alpha_p = \sqrt{\sum_{n,l}d_{n,l,s}(p)^2}
\]</span> 我们可以定义变量代换，<span class="math inline">\(\tilde
W&#39; - W’ = \alpha .* \tilde W - \alpha .* W\)</span>，则继续使用L2
norm，最小化<span class="math inline">\(||\tilde
W&#39;-W&#39;||_2\)</span>得到<span class="math inline">\(\tilde
W&#39;\)</span>，则<span class="math inline">\(\tilde W =
\alpha^{-1}.*\tilde W&#39;\)</span>；</p>
<h4 id="数据协方差度量">数据协方差度量</h4>
<p><img src="image-20221102223532182.png" /></p>
<h3 id="低秩矩阵分解估计">低秩矩阵分解估计</h3>
<h4 id="高维张量的分解">高维张量的分解</h4>
<p>对于张量<span class="math inline">\(W\in \mathbb R^{m\times n\times
k}\)</span>，首先将后两个维度合并成一个维度，使用SVD估计<span
class="math inline">\(\tilde W_m \in \mathbb R ^{m\times
(nk)}\)</span>，<span class="math inline">\(\tilde W_m \approx \tilde U
\tilde S \tilde V^T\)</span>；之后再对<span class="math inline">\(\tilde
V\in \mathbb R^{n\times
k}\)</span>使用SVD；记两次奇异值分解保留的秩分别为<span
class="math inline">\(K_1\)</span>，<span
class="math inline">\(K_2\)</span>;</p>
<p>另一种方法是使用矩阵的外积分解，最小化： <span
class="math display">\[
||W-\alpha\otimes\beta\otimes\gamma||_F
\]</span> 其中<span class="math inline">\(\alpha\in \mathbb
R^m\)</span>, <span class="math inline">\(\beta \in \mathbb
R^n\)</span>, <span class="math inline">\(\gamma \in \mathbb
R^k\)</span>；如果是对于秩为<span
class="math inline">\(K\)</span>的矩阵进行分解，只需要将<span
class="math inline">\(W\)</span>分解为多个外积之和；、</p>
<p>论文中给出了参考文献和实现细节，再次不再赘述；</p>
<h4 id="单色卷积估计">单色卷积估计</h4>
<p>对于某层卷积权重<span class="math inline">\(W\in \mathbb R^{C\times X
\times Y\times W}\)</span>，记第<span
class="math inline">\(w\)</span>个卷积核为<span
class="math inline">\(W_f \in \mathbb R^{C\times
(XY)}\)</span>；首先使秩为1的SVD，得到<span class="math inline">\(\tilde
W_f = \tilde U_f \tilde S_f \tilde
V_f^T\)</span>；之后为了进一步压缩，作者观察到，每个卷积核的<span
class="math inline">\(C\)</span>的通道也是比较冗余的，可以投影到<span
class="math inline">\(C&#39;\)</span>维的空间；对<span
class="math inline">\(\tilde U_f \in \mathbb R^{C\times
1}\)</span>使用聚类，得到<span
class="math inline">\(c_f\)</span>个聚类，因此<span
class="math inline">\(\tilde W_f = \tilde U_{c_f} \tilde S_f \tilde
V_f^T\)</span>其中<span class="math inline">\(\tilde
U_{c_f}\)</span>是原左奇异向量所在聚类的聚类中心；</p>
<h4 id="双色卷积估计">双色卷积估计</h4>
<p>除了卷积核各个通道间的冗余性，还有可能存在不同卷积核之间的冗余性。因此在双色方法中，作者首先对<span
class="math inline">\(W_C\in \mathbb R^{C\times
(XYF)}\)</span>进行聚类，得到<span
class="math inline">\(a\)</span>个类别，之后再对<span
class="math inline">\(W_F\in \mathbb R^{(CXY)\times
F}\)</span>进行聚类，得到<span
class="math inline">\(b\)</span>个类别，这样我们可以将<span
class="math inline">\(W\)</span>压缩为<span
class="math inline">\(a\times
b\)</span>个$W_SR^{C(XY)F}$3D张量，再使用先前的SVD分解或者外积分解即可；</p>
<blockquote>
<p>上述使用到的聚类，都是在每次迭代中平衡各个类别个数的（各个类数量相同），这两利于并行化</p>
</blockquote>
<h3 id="微调">微调</h3>
<p>相比于设计度量学习中的度量目标，作者还尝试了一种简单粗暴的方式，仅仅使用最简单的度量为目标进行逐层分解，之后冻结分解层及其先前的层，微调后面的层至性能恢复，然后再继续分解下一层；实验证明，这种方式，，要比手工设计的度量性能更好；</p>
<h2 id="参考链接">参考链接</h2>
<p><a
href="https://www.cvmart.net/community/detail/3616">深入浅出的模型压缩：你一定从未见过如此通俗易懂的
Slimming 操作</a>，这里有一些有关模型压缩论文列表，有空可以看一看</p>
<p><a href="https://zhuanlan.zhihu.com/p/46626607">知乎 -
马氏距离</a></p>
<p><a href="https://wuli.wiki/online/Delta.html">小时百科 - 狄拉克<span
class="math inline">\(\delta\)</span>函数</a></p>
<p><a href="https://en.wikipedia.org/wiki/Matrix_norm">wikipedia -
matrix norm</a></p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>模型压缩</tag>
        <tag>卷积分解</tag>
      </tags>
  </entry>
  <entry>
    <title>低秩分解与稀疏化的统一</title>
    <url>/2023/02/06/Low-Rank-and-Sparse/</url>
    <content><![CDATA[
<p><strong>论文</strong>：<a href="url"><strong>Title</strong></a></p>
<p><strong>作者</strong>：<em>Authors</em></p>
<p><strong>一作单位</strong>：XXXX</p>
<p><strong>录用情况</strong>：Conference'Year</p>
<blockquote>
<p>My abstract</p>
</blockquote>
<span id="more"></span>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>模型压缩</tag>
      </tags>
  </entry>
  <entry>
    <title>度量空间与连续性</title>
    <url>/2022/08/28/Lipschitz/</url>
    <content><![CDATA[<p>前几日接触到一个新概念，Lipschitz
Constant，本文是笔者在Wikipedia上递归搜索归纳的部分数学公里、定义、例子；抽象代数与拓扑几何的部分暂不在本文讨论；笔者不是数学专业，因此逻辑与表达多有不严谨之处，还请多多指正；</p>
<span id="more"></span>
<h2 id="度量空间-metric-space">度量空间 (Metric space)</h2>
<h3 id="定义">定义</h3>
<p>对于一个集合<span
class="math inline">\(M\)</span>中的两个元素，可以给出一种度量<span
class="math inline">\(d\)</span>，称为度量空间，记作<span
class="math inline">\((M,d)\)</span>，也是一个函数：<span
class="math inline">\(d:M\times M \rightarrow \mathbb{R}\)</span>:</p>
<blockquote>
<p>跳出直观的欧几里得距离，度量是一种抽象的、隐喻的概念，又是也可以理解为从一个点到另一个点需要花费的成本；</p>
</blockquote>
<p>对于任意的<span class="math inline">\(x,y,z\in
M\)</span>，度量空间满足下面4条公理，</p>
<ol type="1">
<li><span class="math inline">\(d(x,x)=0\)</span></li>
<li>如果<span class="math inline">\(x\ne y\)</span>，则<span
class="math inline">\(d(x,y)&gt;0\)</span></li>
<li><span class="math inline">\(d(x,y)=d(y,x)\)</span></li>
<li><span class="math inline">\(d(x,z)\le d(x,y) + d(y, z)\)</span></li>
</ol>
<p>常见的距离函数有：欧几里得距离<span
class="math inline">\(d_2\)</span>，曼哈顿距离<span
class="math inline">\(d_1\)</span>，切比雪夫距离<span
class="math inline">\(d_{\infty}\)</span>，球面距离，汉明距离等等，并且有：
<span class="math display">\[
d_\infty(p,q)\le d_2(p,q) \le d_1(p,q) \le 2d_\infty(p,q)
\]</span></p>
<h3 id="子空间">子空间</h3>
<p>度量空间<span class="math inline">\((M,d)\)</span>，有<span
class="math inline">\(A\subseteq M\)</span>，则<span
class="math inline">\(A\)</span>的诱导度量<span
class="math inline">\(d_A:A\times A\rightarrow \mathbb{R}\)</span>定义为
<span class="math display">\[
d_A(x,y)=d(x,y)
\]</span> 球面集合<span class="math inline">\(S^2\)</span>是<span
class="math inline">\(\mathbb{R^3}\)</span>的子集，球面上的诱导度量是3维空间中的欧几里得距离；</p>
<h3 id="收敛性">收敛性</h3>
<p>这里先给出欧式空间上的表述，可以推广到度量空间：</p>
<p>一个数列<span class="math inline">\((x_n)\)</span>收敛到<span
class="math inline">\(x\)</span>，当且仅当对于任意的<span
class="math inline">\(\epsilon&gt;0\)</span>，存在整数<span
class="math inline">\(N\)</span>，对于所有的<span
class="math inline">\(n&gt;N\)</span>都有<span
class="math inline">\(d(x_n,x)&lt; \epsilon\)</span>；</p>
<h3 id="完备性-completeness">完备性 (completeness)</h3>
<p>直观地说，如果没有“缺失点”，即任何看起来收敛的序列（随着叙述的增加愈发靠近），确实都是收敛的，则称该度量空间是完备的；</p>
<p>一个数列<span class="math inline">\((x_n)\)</span>是<span
class="math inline">\((M,d)\)</span>中的柯西数列，当且仅当对于任意的<span
class="math inline">\(\epsilon&gt;0\)</span>，存在整数<span
class="math inline">\(N\)</span>，对于所有的<span
class="math inline">\(m,n&gt;N\)</span>都有<span
class="math inline">\(d(x_m,x_n)&lt;\epsilon\)</span>；</p>
<p>收敛数列一定是柯西数列，(如果<span
class="math inline">\(x_m\)</span>与<span
class="math inline">\(x_n\)</span>与极限的距离都不超过<span
class="math inline">\(\epsilon\)</span>，则通过三角不等式推出相同条件（<span
class="math inline">\(N\)</span>）下<span
class="math inline">\(x_m\)</span>与<span
class="math inline">\(x_n\)</span>的距离不超过<span
class="math inline">\(2\epsilon\)</span>；</p>
<p>如果度量空间中所有的柯西数列都是收敛的，则度量空间是完备的；</p>
<p>例如，区间<span class="math inline">\((0,1)\)</span>不完备而<span
class="math inline">\([0,1]\)</span>完备，有理数集不完备而实数集完备；</p>
<h3 id="有界性-bounded">有界性 (bounded)</h3>
<p>在<span class="math inline">\(M\)</span>中存在一个距离上届，称为<span
class="math inline">\(M\)</span>的直径；</p>
<p>完全有界(or precompact)，定义涉及拓扑；</p>
<h3 id="紧凑性-compactness">紧凑性 (compactness)</h3>
<p>有几个紧凑性的等价定义，这里忽略涉及到拓扑概念的：</p>
<ol type="1">
<li>每一个数列都有一个收敛的子数列；</li>
<li>完备的且完全有界的；</li>
</ol>
<h3 id="度量空间之间的映射">度量空间之间的映射</h3>
<blockquote>
<p>从度量空间到度量空间，有时称映射(maps)、有时称函数(functions)</p>
</blockquote>
<h4 id="等距映射-isometries">等距映射 (Isometries)</h4>
<p>一个函数<span class="math inline">\(f:M_1\rightarrow
M_2\)</span>是等距的，如果对于<span
class="math inline">\(M_1\)</span>中任意的点都有： <span
class="math display">\[
d_2(f(x),f(y))=d_1(x,y)
\]</span></p>
<blockquote>
<p><span
class="math inline">\(f\)</span>应该是二元组到二元组的函数，上述<span
class="math inline">\(f(x)\)</span>理解为<span
class="math inline">\(f(x,y)\)</span>的第一项，<span
class="math inline">\(f(y)\)</span>是第二项</p>
</blockquote>
<p>例如<span
class="math inline">\(f:(\mathbb{R}^2,d_1)\rightarrow(\mathbb{R}^2,d_\infty)\)</span>有：
<span class="math display">\[
f(x,y)=(x+y,x-y)
\]</span> 因为<span
class="math inline">\(|x|+|y|=\max({|x+y|,|x-y|})\)</span></p>
<h4 id="连续映射continuous-maps">连续映射(continuous maps)</h4>
<p>存在多种等价的定义，其中<span
class="math inline">\(\epsilon-\delta\)</span>定义为：</p>
<p>一个函数<span class="math inline">\(f:M_1\rightarrow
M_2\)</span>是连续的，如果对于<span
class="math inline">\(M_1\)</span>中任意的点<span
class="math inline">\(x\)</span>，和任意的<span
class="math inline">\(\epsilon &gt; 0\)</span>，都有<span
class="math inline">\(\delta &gt; 0\)</span>使得对于<span
class="math inline">\(M_1\)</span>中任意的<span
class="math inline">\(y\)</span>有： <span class="math display">\[
d_1(x,y)&lt;\delta \Rightarrow d_2(f(x),f(y))&lt;\epsilon
\]</span></p>
<h4 id="一致连续映射uniformly-continuous-maps">一致连续映射(uniformly
continuous maps)</h4>
<p>一个函数<span class="math inline">\(f:M_1\rightarrow
M_2\)</span>是一致连续的，如果对于任意的实数<span
class="math inline">\(\epsilon &gt; 0\)</span>，存在<span
class="math inline">\(\delta &gt;0\)</span>使得对于<span
class="math inline">\(M_1\)</span>中任意的点<span
class="math inline">\(x,y\)</span>满足<span
class="math inline">\(d(x,y)&lt;\delta\)</span>的，有 <span
class="math display">\[
d_2(f(x),f(y))&lt;\epsilon
\]</span></p>
<blockquote>
<p>在连续的定义中，<span class="math inline">\(\epsilon\)</span>是<span
class="math inline">\(x\)</span>和<span
class="math inline">\(\delta\)</span>的函数；在一致连续的定义中，<span
class="math inline">\(\epsilon\)</span>仅是<span
class="math inline">\(\delta\)</span>的函数；后者的条件更强；</p>
</blockquote>
<h4 id="李普希兹映射-lipschitz-maps">李普希兹映射 (Lipschitz maps)</h4>
<p>经过这种映射，距离的拉伸不会超过一个上界：给定一个实数<span
class="math inline">\(K&gt;0\)</span>，函数<span
class="math inline">\(f:M_1\rightarrow M_2\)</span>是K-Lipschitz映射当：
<span class="math display">\[
d_2(f(x),f(y))\le Kd_1(x,y)
\]</span> 特殊地，当<span
class="math inline">\(K&lt;1\)</span>时称为收缩(contraction)</p>
<h4 id="准等距映射quasi-isometries">准等距映射(Quasi-isometries)</h4>
<p>函数<span class="math inline">\(f:M_1\rightarrow
M_2\)</span>是准等距映射当存在常数<span class="math inline">\(A\ge
1\)</span>且<span class="math inline">\(B\ge 0\)</span>有： <span
class="math display">\[
\frac{1}{A}d_2(f(x),f(y))-B\le d_1(x,y) \le Ad_2(f(x),f(y))+B
\]</span></p>
<h2 id="李普希兹连续">李普希兹连续</h2>
<p>上文中的李普希兹映射的定义也是李普希兹连续的定义，其中<span
class="math inline">\(K\)</span>是李普希兹常量，最小的<span
class="math inline">\(K\)</span>称为the best Lipschitz
constant或者dilation。</p>
<p>一种特殊情况是当<span
class="math inline">\(f\)</span>是实函数时，上述定义相当于限制了函数在其定义域上任意两点的割的绝对值存在上限<span
class="math inline">\(K\)</span>：</p>
<p><span class="math inline">\(|f(x_1)-f(x_2)|\le
K|x_1-x_2|\)</span></p>
<img src="/2022/08/28/Lipschitz/lipschitz.png" class="" title="lipschitz">
<blockquote>
<p>如上图所示，无论怎么移动，两条斜率绝对值为<span
class="math inline">\(K\)</span>的斜线割开的上下部分均不会有<span
class="math inline">\((x,f(x))\)</span>，即满足李普希兹连续条件</p>
</blockquote>
<p>在神经网络中，我们假设模型函数具有一个小于1的李普希兹常量，通常是为了避免梯度传播过程中发生数值问题；</p>
<h2 id="参考链接">参考链接</h2>
<p><a href="https://en.wikipedia.org/wiki/Metric_space">Metric
space</a></p>
]]></content>
      <categories>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>度量空间</tag>
        <tag>李普希兹连续</tag>
      </tags>
  </entry>
  <entry>
    <title>模型压缩论文阅读列表</title>
    <url>/2022/11/03/Model-Compression/</url>
    <content><![CDATA[<blockquote>
<p>模型压缩，包括低秩分解、剪枝、量化、蒸馏等技术，旨在精度损失很小的前提下，获得参数更小、推理速度更快的模型</p>
</blockquote>
<span id="more"></span>
<h2 id="方法">方法</h2>
<h3 id="低秩分解">低秩分解</h3>
<h4 id="学习可分离滤波器">学习可分离滤波器</h4>
<p><a
href="http://openaccess.thecvf.com/content_cvpr_2013/html/Rigamonti_Learning_Separable_Filters_2013_CVPR_paper.html">Learning
Separable Filters</a></p>
<p>本文提出，通过学习可分离卷积核的方法降低卷积运算的复杂度。
一种方法是，为卷积添加“可分离”的约束，但是作者发现先学习不可分离卷积，之后<strong>学习用可分离卷积线性组合得到不可分离卷积更好</strong>。</p>
<p>可以抛开应用背景看，本文对卷积核的分解使用现在看来非常直观的方法：先在模型优化目标中引入卷积核矩阵的核范数，得到低秩的核之后，把核分解过程从模型优化过程解耦出来，找到次优的分解基和权重；</p>
<p>对于3D卷积，由于SVD只能分解2D矩阵，核范数的优化不能实现，因此，在分解的过程作者使用了Canonical
Polyadic Decomposition(CPD)的分解方法；</p>
<h4 id="探索卷积核的线性结构">探索卷积核的线性结构</h4>
<p><a href="https://arxiv.org/pdf/1404.0736.pdf">Exploiting Linear
Structure Within Convolutional Networks for Efficient Evaluation</a></p>
<p>见先前的<a
href="https://ashun989.github.io/2022/10/13/Linear-Structure-Within-Conv/#more">文章</a></p>
<p>这篇文章的方法，在保持分类误差不超过1%的前提下，只是将1个卷积层进行分解并且获得了大约1.6倍的加速比；</p>
<h4 id="用低秩扩展加速卷积神经网络">用低秩扩展加速卷积神经网络</h4>
<p><a href="https://arxiv.org/abs/1405.3866">Speeding up convolutional
neural networks with low rank expansions</a></p>
<p>详细分析见<a href="">文章</a></p>
<p>在空间维度上使用rank-1分解，在通道维度上使用基的线性组合，通过调整基组成的子空间的维度（基的个数）来权衡加速比和精度；作者基于这样的思想设计了两种分解模式；并对比最小化卷积核重建损失和数据重建损失的效果；</p>
<h4 id="非线性卷积层的高效和精确估计">非线性卷积层的高效和精确估计</h4>
<p><a
href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Zhang_Efficient_and_Accurate_2015_CVPR_paper.html">Efficient
and accurate approximations of nonlinear convolutional networks</a></p>
<p>本文的主要目标是加速CNN的推理。</p>
<h3 id="剪枝">剪枝</h3>
<h4
id="模型剪枝学习网络的权重和连接">模型剪枝：学习网络的权重和连接</h4>
<p><a
href="https://proceedings.neurips.cc/paper/2015/hash/ae0eb3eed39d2bcef4622b2499a05fe6-Abstract.html">Learning
both weights and connections for efficient neural network</a></p>
<p>我们知道，dropout过程是一种在训练阶段随机剪裁神经元之间的连接，但是在推理时恢复所有连接的技术；而剪枝则是在训练时减去的连接，推理时也不再恢复；</p>
<p>剪枝可以分为单步剪枝和迭代剪枝，本文使用迭代剪枝，减除更多参数；剪枝选择的策略也有很多，一种直观地策略是，去除那些绝对值小的神经元，本文的实验发现这种方法损害性能；本文中的一次迭代是：减去那些输入为0或者输出为0的神经元，之后微调模型；为了提高效率，以及避免网络在反向传播过程中梯度消失，如果做卷积层的剪枝则在微调时冻结全连接层，反之亦然；</p>
<p>是什么保证了存在输入或输出为0的神经元？正则项；L1正则项更具有使得模型参数稀疏化的能力，而在使用微调后，L2正则项的表现更好；</p>
<p>随着模型的稀疏，输出方差也在变小，这意味着减小过拟合，因此随着剪枝迭代的进行，dropout率应该减小；</p>
<h4 id="模型压缩剪枝量化huffman">模型压缩：剪枝+量化+Huffman</h4>
<p><a href="https://arxiv.org/abs/1510.00149">Deep compression:
Compressing deep neural networks with pruning, trained quantization and
huffman coding</a></p>
<p>一种结合剪枝、量化和霍夫曼编码的深度神经网络压缩技术，将AlexNet从240MB压缩到6.9MB，将VGG-16从552MB压缩到11.3MB；</p>
<p>剪枝方法沿用作者先前的无损剪枝操作；量化方法使用K均值聚类，根据训练好的模型权重建立codebook以及原始权重在codebook中的索引，探索了三种聚类中心的初始化方式；</p>
<p>观察到量化后的索引与聚类权重分布不均，使用霍夫曼编码进一步压缩存储；</p>
<h4 id="剪枝卷积网络中的滤波器">剪枝卷积网络中的滤波器</h4>
<p><a href="https://arxiv.org/abs/1608.08710">Pruning filters for
efficient convnets</a></p>
<p>使用简单的magnitude
based策略来分析滤波器权重，剪裁滤波器与响应的特征图，具体的剪裁方法很简单，记卷积层参数<span
class="math inline">\(F\in \mathbb R^{C_2\times C_1\times H\times
W}\)</span>，其中<span class="math inline">\(C_2\)</span>与<span
class="math inline">\(C_1\)</span>分别为输出通道数和输入通道数，可以看成<span
class="math inline">\(C_2\)</span>个卷积核，现在希望从中去除<span
class="math inline">\(m\)</span>个卷积核：</p>
<ol type="1">
<li>计算<span class="math inline">\(C_2\)</span>个卷积核各自的L1
norm，排序，选出最小的那几个卷积核，移除；</li>
<li>由于输出通道数变为<span
class="math inline">\(C_2-m\)</span>，那么接下来的一个卷积层的卷积核的对应输入通道也要移除<span
class="math inline">\(m\)</span>个；</li>
</ol>
<p>作者还在文中做了一些其他讨论，比如</p>
<ul>
<li>各个卷积层的剪枝会影响到后续卷积层，这里存在一个简单的组合优化问题，每一个卷积层，是先完成前面层对其输入通道数的影响，再根据前文的策略缩减输出特征维度；如果考虑前面的影响，又该考虑前面几层影响，是最优的？作者实验了两种方案，各自独立剪裁，后传播影响；以及贪心策略；后者稍好一些</li>
<li>存在残差连接的情况下，shortcut上的1x1卷积和主干上的第二层卷积层只能二选一剪枝（一个会影响另一个），作者选择以shortcut上的剪枝为主，因为shortcut更重要一些；</li>
<li>一次性全剪枝完再微调，还是迭代地剪枝、微调；后者效果稍好，但是对于更深的网络，后者微调的时间更长一些；</li>
<li>使用什么样的norm来表示卷积核的重要性</li>
</ul>
<h3 id="量化">量化</h3>
<p><a href="https://arxiv.org/abs/1412.6115">Compressing Deep
Convolutional Networks using Vector Quantization</a></p>
<h3 id="其他">其他</h3>
<h4 id="频域压缩">频域压缩</h4>
<p><a
href="https://ieeexplore.ieee.org/abstract/document/8413170/">Packing
convolutional neural networks in the frequency domain</a></p>
<p>把卷积核视为图像，在频域中分解为公共部分和个体方差，之后丢弃这两者大部分的低频信号，同时不会带来严重的精度损失；作者还探索了数据驱动的消除冗余的方法；</p>
<p>在CNNPackv1中，主要步骤为：</p>
<ul>
<li>对所有卷积核使用DCT变换，之后剪裁到固定的维度<span
class="math inline">\(d\)</span>，（卷积核的高频信息在该过程被抛弃）；</li>
<li>学习卷积核的k个聚类中心，并计算各个卷积核相对于聚类中心的残差，分别对聚类中心和残差进行稀疏化（L1
norm）；</li>
<li>固定已经完成剪枝和稀疏化的参数不变，微调网络，并量化残差，重复至收敛；最后再使用CSR保存参数的稀疏形式并使用Huffman编码；</li>
</ul>
<p>在CNNPackv2中，压缩过程使用上了输入数据，修改了目标函数；对于这个目标函数，作者还用相当的篇幅描述了如何优化；</p>
<p>由于压缩、量化都是在频域中完成的，如果要在推理时使用原来空间域中卷积的方式，需要将频域中的卷积核转换回去，得到的空间域中的卷积核将不再稀疏，这就意味着，在推理时，模型参数在内存中的占用量远大于在磁盘上存储的大小；因此作者设计了在频域中完成卷积的加速方法；</p>
<blockquote>
<p>本文数学推导较多，需要DCT相关的背景知识，笔者将来有空时也许会单独开一篇关于本文的论文阅读；</p>
</blockquote>
<p>data-driven的CNNPackv2方法对AlexNet和VGG-16
Net实现了43.5和49.1倍的压缩率，以及26.2和10.2倍的理论加速比；对于ResNet-50，压缩率为14.0，理论加速比为5.0；对于ResNeXt-50，压缩率为14.3，理论加速比为5,1；以上压缩造成的在imagenet上的top-1
acc损失都在1%左右；</p>
<h2 id="参考链接">参考链接</h2>
<p><a href="https://cloud.tencent.com/developer/article/1631704">腾讯云
- 闲话模型压缩之网络剪枝（Network Pruning）</a></p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>模型压缩</tag>
      </tags>
  </entry>
  <entry>
    <title>凸优化学习笔记(一)</title>
    <url>/2022/11/14/Convex-Optimization/</url>
    <content><![CDATA[<blockquote>
<p>内容主要来自<a href="https://web.stanford.edu/class/ee364a/">MIT
EE364a</a>课程slides和book；穿插着网络资源和少量自己的理解（这些内容不保证准确）</p>
</blockquote>
<span id="more"></span>
<h2 id="第1章">第1章</h2>
<h2 id="第2章-凸集">第2章 凸集</h2>
<h3 id="仿射集和凸集">2.1 仿射集和凸集</h3>
<h4 id="直线和线段">直线和线段</h4>
<p>设<span class="math inline">\(x_1\)</span>与<span
class="math inline">\(x_2\)</span>是<span class="math inline">\(\mathbb
R^n\)</span>中两个点，且<span class="math inline">\(x_1\ne
x_2\)</span>，<span class="math inline">\(\theta \in \mathbb
R\)</span>，则过这两点的直线为 <span class="math display">\[
y = \theta x_1 + (1 -\theta) x_2
\]</span> 如果限制<span class="math inline">\(\theta \in [0,
1]\)</span>，则表示以<span
class="math inline">\(x_1,x_2\)</span>为端点的线段；</p>
<p>将上式稍加变形，得到“中心+偏移”的表示形式： <span
class="math display">\[
y = x_2 + \theta(x_1 - x_2) = x + \theta r
\]</span></p>
<h4 id="仿射集">仿射集</h4>
<h5 id="仿射组合">仿射组合</h5>
<p>首先定义<strong>仿射组合</strong>(affine
combination)，是在线性组合的基础上，对组合的权重添加了和为1的限制；
<span class="math display">\[
\theta_1x_1 + \cdots + \theta_kx_k, \ \ \text{where} \ \
\sum_i^k\theta_i = 1
\]</span>
线性组合的结果是这些向量张成的<strong>子空间</strong>(subspace)，可以想象到，仿射组合，让这个得到的空间“收缩”了；比如两个不平行向量线性组合成一个与<span
class="math inline">\(\mathbb
R^2\)</span>同构的平面，而两个不重合的点仿射组合成一个与<span
class="math inline">\(\mathbb R^1\)</span>同构的直线；</p>
<blockquote>
<p>当我们提到<span class="math inline">\(\mathbb
R^n\)</span>中的向量/点，似乎都可以用<span
class="math inline">\(n\)</span>个数字来表示，但是，向量具有方向</p>
</blockquote>
<h5 id="仿射集-1">仿射集</h5>
<p>集合<span class="math inline">\(C\subseteq \mathbb
R^n\)</span>是仿射集(affine set)，当且仅当对于任意的<span
class="math inline">\(\theta \in R\)</span>, <span
class="math inline">\(x_1, x_2\in C\)</span>且<span
class="math inline">\(x_1\ne x_2\)</span>，有<span
class="math inline">\(\theta x_1 + (1 -\theta) x_2 \in C\)</span>；</p>
<blockquote>
<p>书中对于充分条件、必要条件、充要条件划分的好像不是很细致，这里在书中的描述是，is
a affine set if xxxx，但是<a
href="https://math.stackexchange.com/questions/1931810/definition-of-an-affine-set">这里</a>使用的是iff.</p>
</blockquote>
<p>接下来，引入仿射集的另一个定义：<span
class="math inline">\(C\)</span>是仿射集，当且仅当<span
class="math inline">\(C\)</span>中所有元素的仿射组合在<span
class="math inline">\(C\)</span>中；</p>
<details>
<summary>
两个定义等价的证明
</summary>
<blockquote>
<p>笔者在第一次看到这两个概念的时候，总觉得第一个概念定义的是一堆直线的上的点的集合，然而这些线上的点又可以互相连线，如此反复递归下去，直到铺满整个平面；下面，笔者给出了这两个定义等价的证明；</p>
</blockquote>
<p>先证明，<span
class="math inline">\(C\)</span>中所有元素的仿射组合在<span
class="math inline">\(C\)</span>中 <span
class="math inline">\(\Rightarrow\)</span> <span
class="math inline">\(C\)</span>是仿射集：</p>
<p>使用数学归纳法，设<span class="math inline">\(C\)</span>中有<span
class="math inline">\(k\)</span>个元素：</p>
<ol type="1">
<li><p>特殊地，任意两个元素的仿射组合在<span
class="math inline">\(C\)</span>中 <span
class="math inline">\(\Rightarrow\)</span> <span
class="math inline">\(C\)</span>是仿射集；</p></li>
<li><p>假设：任意<span
class="math inline">\(k-1\)</span>个元素的仿射组合在<span
class="math inline">\(C\)</span>中 <span
class="math inline">\(\Rightarrow\)</span> <span
class="math inline">\(C\)</span>是仿射集；</p></li>
<li><p>证明：<span
class="math inline">\(k\)</span>个元素的仿射组合在<span
class="math inline">\(C\)</span>中 <span
class="math inline">\(\Rightarrow\)</span> <span
class="math inline">\(C\)</span>是仿射集；</p>
<p>满足<span class="math inline">\(\sum_{i=1}^k\theta_ix_i \in
C\)</span>，且<span class="math inline">\(\sum_{i=1}^k\theta_i =
1\)</span>；</p>
<p>能找到<span class="math inline">\(\theta_k \ne 1\)</span>，即 <span
class="math display">\[
\sum_{i=1}^{k-1}\theta_ix_i + \theta_kx_k =
(1-\theta_k)\sum_{i=1}^{k-1}\frac{\theta_i}{1-\theta_k}x_i + \theta_kx_k
\in C
\]</span> 那么，对于<span
class="math inline">\(\sum_{i=1}^{k-1}\frac{\theta_i}{1-\theta_k}x_i\)</span>，注意到，<span
class="math inline">\(\sum_{i=1}^{k-1}\frac{\theta_i}{1-\theta_k}=1\)</span>，根据假设，有<span
class="math inline">\(x&#39; =
\sum_{i=1}^{k-1}\frac{\theta_i}{1-\theta_k}x_i \in C\)</span>，则<span
class="math inline">\((1-\theta_k)x&#39;+\theta_k x_k \in
C\)</span>；</p>
<p>若总是有<span class="math inline">\(\theta_k =
1\)</span>，那只可能是<span
class="math inline">\(k=1\)</span>，单元素集是仿射集；</p>
<p>综上，得证；</p></li>
</ol>
<p>再证明，<span class="math inline">\(C\)</span>是仿射集 <span
class="math inline">\(\Rightarrow\)</span> <span
class="math inline">\(C\)</span>中所有元素的仿射组合在<span
class="math inline">\(C\)</span>中：</p>
<p><span class="math inline">\(x&#39; = \theta x_1 + (1 -\theta) x_2 \in
C\)</span>，设<span class="math inline">\(p_1+p_2+p_3 =
1\)</span>，<span class="math inline">\(\theta = \frac{p_1}{p_1 +
p_2}\)</span>，任取一个<span class="math inline">\(x_3\in C\)</span>，则
<span class="math display">\[
\begin{split}
p_1x_1 + p_2x_2 + p_3x_3 &amp;= (p_1 + p_2)(\theta x_1 + (1-\theta)x_2)
+ p_3x_3\\
&amp;=(1 - p_3)x&#39; + p_3x_3 \in C
\end{split}
\]</span> 如此重复，直到推出所有元素的仿射组合属于<span
class="math inline">\(C\)</span>；</p>
</details>
<h5 id="相关子空间">相关子空间</h5>
<p>在线代中我们提到过线性子空间的定义，满足：包含0向量，对加法和数乘封闭；显然，仿射集不是一个线性子空间，但是我们可以从一个仿射集得到线性子空间；</p>
<p>集合<span class="math inline">\(V = C-x_0 = \{x - x_0 | x\in
C\}\)</span>是<span
class="math inline">\(C\)</span>的相关子空间：显然<span
class="math inline">\(\mathbf 0 = x-x_0 \in
V\)</span>，我们需要证明其对加法、数乘的封闭性：设<span
class="math inline">\(v_1, v_2\in V\)</span>，<span
class="math inline">\(\alpha, \beta\in \mathbb
R\)</span>，根据子空间定义，有<span class="math inline">\(v_1+x_0, v_2 +
x_0 \in C\)</span>，要证明<span class="math inline">\(\alpha v_1 + \beta
v_2 \in V\)</span>，只要证明<span class="math inline">\(\alpha v_1 +
\beta v_2 + x_0 \in C\)</span>；变换一下，有 <span
class="math display">\[
\alpha v_1 + \beta v_2 + x_0 = \alpha(v_1 + x_0) + \beta (v_2 + x_0) +
(1 - \alpha -\beta)x_0
\]</span> 由于<span class="math inline">\(\alpha + \beta +
(1-\alpha-\beta) = 1\)</span>，则可证<span class="math inline">\(\alpha
v_1 + \beta v_2 + x_0 \in C\)</span>；</p>
<p>因此，仿射集还可以表示为线性子空间+偏移的形式，其中<span
class="math inline">\(x_0\)</span>是<span
class="math inline">\(C\)</span>中任意一点： <span
class="math display">\[
C=V+x_0 = \{v+x_0|v\in V\}
\]</span></p>
<blockquote>
<p>易证，<span
class="math inline">\(\{x|Ax=b\}\)</span>是仿射集，且任何仿射集都可以解释为线性方程组的解集；从而也不难得到，该仿射集对应的子空间是<span
class="math inline">\(A\)</span>的零空间；</p>
<p><a
href="https://blog.csdn.net/csyifanZhang/article/details/105759260">这里</a>对于“为什么要引出这些概念”，解释得不错；</p>
</blockquote>
<h5 id="任意集合的仿射包">任意集合的仿射包</h5>
<p>能否对于任何一个一个集合<span
class="math inline">\(C\)</span>，构造一个最小仿射集？</p>
<p>集合<span class="math inline">\(C\in \mathbb
R^n\)</span>中所有的<strong>仿射组合</strong>构成的集合，称为<span
class="math inline">\(C\)</span>的<strong>仿射包</strong>（affine
hull），记为<span class="math inline">\(\textbf{aff}\ \ C\)</span>；</p>
<blockquote>
<p>仿射集的仿射包就是它自己；</p>
</blockquote>
<p>包含集合<span
class="math inline">\(C\)</span>的仿射集有无穷多个，而<span
class="math inline">\(\textbf{aff}\ \ C\)</span>是其中最小的一个；</p>
<h5 id="仿射相关与仿射无关">仿射相关与仿射无关</h5>
<p><a
href="https://math.stackexchange.com/questions/2334363/what-is-the-difference-between-linearly-and-affinely-independent-vectors">仿射相关与线性相关的区别</a></p>
<p>仿射组合是（特殊的）线性组合，仿射相关一定意味着线性相关，而线性无关一定意味着仿射无关</p>
<blockquote>
<p>这三个命令的逆命题不成立</p>
</blockquote>
<blockquote>
<p>例如在<span class="math inline">\(\mathbb
R^2\)</span>上，三个不共线的点是仿射无关的，但是它们已经仿射组合得到整个<span
class="math inline">\(\mathbb
R^2\)</span>平面，因此再加入任何点都将与它们仿射相关；</p>
</blockquote>
<h4 id="仿射维度和相对内部">仿射维度和相对内部</h4>
<p>定义任意集合<span
class="math inline">\(C\)</span>的仿射维度=C的仿射包的维度（可以由多少个线性无关向量张成）；仿射维度通常与其他“维度”不一致；</p>
<p>相对内部 (<span
class="math inline">\(\operatorname{relint}\)</span>)：（略）</p>
<h4 id="凸集">凸集</h4>
<p><span class="math inline">\(C\)</span>是<strong>凸集</strong>(convex
set)，当且仅当集合<span
class="math inline">\(C\)</span>中任意两点的线段都在<span
class="math inline">\(C\)</span>中；类似于定义仿射集的过程，我们同样可以先定义<span
class="math inline">\(x_1,\cdots, x_k\)</span>的凸组合为： <span
class="math display">\[
\theta_1x_1 + \cdots + \theta_k x_k, ~\text{where}~\sum_{i=1}^k\theta_i
= 1 ~\text{and}~ \theta_i \ge 0
\]</span> 与仿射集类似的，凸集的定义可扩展为：<span
class="math inline">\(C\)</span>是凸集，当且仅当<span
class="math inline">\(C\)</span>中所有点的凸组合都在<span
class="math inline">\(C\)</span>中；</p>
<p>凸集的性质还可以扩展到无限个点的凸组合，设对于任意的<span
class="math inline">\(x\in C\)</span>，有<span class="math inline">\(p:
\mathbb R^n \rightarrow \mathbb R\)</span>，满足<span
class="math inline">\(p(x)\ge 0\)</span>且<span
class="math inline">\(=1\)</span>，<span
class="math inline">\(C\subseteq \mathbb R^n\)</span>是凸集，则有：
<span class="math display">\[
\int_C p(x)dx \in C
\]</span></p>
<blockquote>
<p>将<span
class="math inline">\(p\)</span>视为连续随机变量概率密度函数/离散随机变量的概率分布，则上式说明，凸集中随机变量的期望，也属于该凸集；</p>
</blockquote>
<p>任意集合<span
class="math inline">\(C\)</span>的<strong>凸包</strong>(convex
hull)<span class="math inline">\(\operatorname{conv}C\)</span>，是<span
class="math inline">\(C\)</span>中所有点的凸组合的集合；</p>
<h5 id="凸集与任意直线的交集">凸集与任意直线的交集</h5>
<p>根据凸集的定义，我们可以得到一条推论，<span class="math inline">\(C
\in \mathbb{R}^m\)</span> 是凸集，当且仅当 <span
class="math inline">\(C\)</span> 与任意直线 <span
class="math inline">\(\{x_0 + vt ~|~ t\in \mathbb{R}\} ~\text{for all }
x_0, v \in \mathbb{R}^n\)</span> 的交集是凸集；</p>
<details>
<summary>
练习
</summary>
<p>练习：设 <span class="math inline">\(C\)</span>
是如下二次不等式的解集： <span class="math display">\[
C = \{x\in\mathbb{R}^n ~|~ x^TAx + b^Tx + c \le 0\}
\]</span> 其中，<span class="math inline">\(A\in \mathbf{S}^n, b\in
\mathbb{R}^n c \in \mathbb{R}\)</span>；证明若 <span
class="math inline">\(A\succeq 0\)</span> 时，<span
class="math inline">\(C\)</span> 是凸集；</p>
<p>证明：<span class="math inline">\(\{x_0 + vt ~|~ t\in
\mathbb{R}\}\)</span> 与 <span class="math inline">\(C\)</span>
的交集为：</p>
<p><span class="math display">\[
\{x_0 + vt ~|~ \alpha t^2 + \beta t + \gamma \le 0\}
\]</span></p>
<p>其中，<span class="math inline">\(\alpha = v^TAv\)</span>，<span
class="math inline">\(\beta = b^Tv + 2x_0^TAv\)</span>， <span
class="math inline">\(\gamma = c + b^Tx_0 + x_0^T A x_0\)</span>；</p>
<p>该一元二次不等式的解集在 <span class="math inline">\(\alpha &gt;=
0\)</span> 时为凸集；对于任意直线该条件成立，即对于任意的 <span
class="math inline">\(v\)</span>，有<span class="math inline">\(v^TAv
\ge 0\)</span>，即 <span class="math inline">\(A\succeq
0\)</span>；证毕；</p>
需要注意的是，<span class="math inline">\(A\succeq 0\)</span> 是 <span
class="math inline">\(C\)</span> 是凸集的充分非必要条件；例如在 <span
class="math inline">\(A = -1, b = 0, c = -1\)</span> 时，<span
class="math inline">\(C = \mathbb{R}\)</span> 是凸集；
</details>
<h4 id="凸锥">凸锥</h4>
<p>相比于仿射组合要求组合系数和为1，凸组合要求组合系数和为1且非负，<strong>锥组合</strong>要求组合系数非负：
<span class="math display">\[
\theta_1x_1+\cdots+\theta_kx_k, ~\text{where}~\theta_i &gt;0,~ i = 1,
\cdots, k
\]</span> <span
class="math inline">\(C\)</span>是一个<strong>凸锥</strong>(convex
cone)，当且仅当<span
class="math inline">\(C\)</span>中任意两点的锥组合都在<span
class="math inline">\(C\)</span>中，可以扩展到，<span
class="math inline">\(C\)</span>中所有点的锥组合都在<span
class="math inline">\(C\)</span>中；</p>
<blockquote>
<p><span class="math inline">\(C\)</span>是一个锥，当任意的<span
class="math inline">\(x\in C\)</span>，<span
class="math inline">\(\theta \ge 0\)</span>，有<span
class="math inline">\(\theta x \in C\)</span>；</p>
</blockquote>
<p>任意集合的锥包(conic hull)是<span
class="math inline">\(C\)</span>中所有点的锥组合；</p>
<h3 id="一些重要的的例子">2.2 一些重要的的例子</h3>
<blockquote>
<p>仿射集一定是凸集，但凸集不一定是仿射集；</p>
<p>凸锥一定是凸集，但凸集不一定是凸锥；</p>
<p>笔者初学时突然不能接受这两条，因为在形状上，凸集总是包含于仿射集或者凸锥，例如，线段是直线的一部分，也是射线的一部分（当然，只有过原点的射线才是凸锥），而先前<span
class="math inline">\((A\subseteq B) \iff (A\Rightarrow
B)\)</span>的逻辑关系，被我在潜意识中错误的套用在了这里的空间关系上；并且，在定义的形式上，凸集的定义最长，好像看起来，凸集的条件是最“强”的；</p>
<p>事实恰恰相反，凸集的条件最弱；如果任意两点的连线上的所有的点，都在该集合中，那又何尝不能满足任意两点间线段上的点，都在该集合中的条件呢？</p>
</blockquote>
<ul>
<li><span class="math inline">\(\emptyset\)</span>，<span
class="math inline">\(\{x_0\}\)</span>，<span
class="math inline">\(\mathbb R^n\)</span>都是<span
class="math inline">\(\mathbb
R^n\)</span>中的仿射集（因此，是凸集）</li>
<li>所有的直线都是仿射集，如果过原点，那么是子空间，也是凸锥；</li>
<li>线段是凸集，但不是仿射集（除非退化成一点）；</li>
<li>射线<span class="math inline">\(\{x_0+\theta v|\theta \ge 0\},
~\text{where}~ v\ne 0\)</span>是凸集，但不是仿射集；如果<span
class="math inline">\(x_0=0\)</span>，则</li>
</ul>
<p>下面介绍的仍然是一些具有凸性的结构：</p>
<h4 id="超平面与半空间">超平面与半空间</h4>
<p><strong>超平面</strong>(hyperplane)是形如<span
class="math inline">\(\{x|a^Tx=b\}\)</span>，其中<span
class="math inline">\(a\in \mathbb R^n\)</span>，<span
class="math inline">\(a\ne 0\)</span>， <span class="math inline">\(b\in
R\)</span>的集合；</p>
<p><strong>半空间</strong>(halfspaces)是形如<span
class="math inline">\(\{x|a^Tx\le b\}\)</span>，其中<span
class="math inline">\(a\in \mathbb R^n\)</span>，<span
class="math inline">\(a\ne 0\)</span>， <span class="math inline">\(b\in
R\)</span>；</p>
<h4 id="欧几里得球与椭球">欧几里得球与椭球</h4>
<p><span class="math inline">\(\mathbb
R^n\)</span>中的<strong>欧几里得球</strong>(ball)： <span
class="math display">\[
B(x_c, r) = \{x~|~\Vert x-x_c\Vert _2\le r\} = \{x_c + ru~|~\Vert u\Vert
_2\le 1\}, ~r &gt; 0
\]</span> <strong>椭球</strong>(ellipisoid)： <span
class="math display">\[
\begin{split}
\epsilon &amp;= \{x~|~(x-x_c)^TP^{-1}(x-x_c)\le 1\}\\
&amp;= \{x_c + Au ~|~ \Vert u\Vert _2 \le 1\}
\end{split}
\]</span> 其中，<span
class="math inline">\(P\)</span>是对称的正定矩阵，<span
class="math inline">\(P \succ
0\)</span>，其特征值是椭球的半轴长的平方，对于球体，<span
class="math inline">\(P=r^2I\)</span>；<span
class="math inline">\(A=P^{1/2}\)</span>，是对称的，正定的，（意味着非奇异的）的方阵；</p>
<p>如果<span
class="math inline">\(A\)</span>是半正定的，（意味着可能有0特征值，即非奇异），则为<strong>退化椭球</strong>(degenerate
ellipsoid)，同样也是凸集；</p>
<h4 id="范式球与范式锥">范式球与范式锥</h4>
<p>将欧几里得球中的二范数换成其他任意范数，即是范式球(norm ball)；</p>
<p><strong>范式锥</strong>(norm cone)： <span class="math display">\[
C = \{(x, t)~|~\Vert x\Vert &lt;t\}\subseteq \mathbb R^n
\]</span></p>
<blockquote>
<p>范数的三角不等式的性质，使得norm ball是凸的；</p>
</blockquote>
<p><strong>二次锥</strong>(second-order cone / quadradic cone / Lorentz
cone / ice-cream cone)，是使用欧几里得范数（二范数）的范式锥</p>
<h4 id="多面体">多面体</h4>
<p><strong>多面体</strong>(polyhedron)由无穷多个线性等式和不等式组成：
<span class="math display">\[
\begin{split}
\mathcal {P} &amp;= \{x~|~a^T_jx\le b_j, ~j=1, \dots, m, ~c^T_jx = d_j,
~j=1, \dots, p\}\\
&amp;= \{Ax\preceq b, ~Cx=d\}
\end{split}
\]</span> <span class="math inline">\(\mathbb
R^n\)</span>中的<strong>非负象限</strong>(nonnegative
orthant)，既是多面体，又是锥，因而又称为<strong>多面锥</strong>(polyhedral
cone)： <span class="math display">\[
\mathbb R^n_+ = \{x\in \mathbb R^n |x_i \ge 0, ~i=1, \dots, n\} = \{x\in
\mathbb R^n |x \succeq 0 \}
\]</span>
<strong>单纯形</strong>(simplexes)是另一类重要的多面体，是<span
class="math inline">\(k+1\)</span>个<span class="math inline">\(\mathbb
R^n\)</span>中<strong>仿射无关</strong>的向量凸组合得到的，又称为<span
class="math inline">\(k\)</span>维（指仿射维度）单纯形 <span
class="math display">\[
C = \{\theta_0v_0+\dots+\theta_kv_k~|~\theta \succeq 0, ~\mathbf
1^T\theta=1\}
\]</span></p>
<blockquote>
<p>1维单纯形是线段，2维单纯形是三角形，3维单纯性是立方体；</p>
<p>如果使用0向量和<span
class="math inline">\(k\)</span>个单位向量凸组合，则称unit
simplex：<span class="math inline">\(x\succeq 0, \mathbf 1^Tx\le
1\)</span></p>
<p>如果使用<span
class="math inline">\(k\)</span>个单位向量凸组合，则称probability
simplex：<span class="math inline">\(x\succeq 0, \mathbf 1^Tx =
1\)</span></p>
</blockquote>
<p>书中的P33给出了将单纯性的定义转化为多面体定义的推导，这里不再赘述；</p>
<p>不加证明的，作者还给出了多面体的另一种表示形式： <span
class="math display">\[
\{\theta_1v_1 + \cdots + \theta_kv_k ~|~ \sum_{i=1}^m\theta_i=1,
~\theta_i\ge0, ~i=1,\cdots, k\}
\]</span> 其中，<span class="math inline">\(m\le k\)</span>，即<span
class="math inline">\(m\)</span>个点的凸包和<span
class="math inline">\(k-m\)</span>个点的凸锥的交集；</p>
<h4 id="半正定锥">半正定锥</h4>
<p>记<span class="math inline">\(S^n\)</span>是<span
class="math inline">\(n\times n\)</span>的对称矩阵：<span
class="math inline">\(S^n = \{X\in \mathbb R^{n\times n} ~|~ X =
X^T\}\)</span>，使用<span
class="math inline">\(S^n_+\)</span>表示半正定对称矩阵， <span
class="math inline">\(S^n_{++}\)</span>表示正定对称矩阵；</p>
<p>矩阵集合<span
class="math inline">\(S^n\)</span>是凸锥，即半正定矩阵的锥组合仍然为半正定矩阵，可以使用半正定矩阵的二次型定义证明；</p>
<blockquote>
<p>书中对于<span class="math inline">\(S^n_+\)</span>的定义是<span
class="math inline">\(\{X\in S^n ~|~ X \succeq 0\}\)</span>，这里的<span
class="math inline">\(X \succeq
0\)</span>并不意味着矩阵的各个元素都非负，而是<span
class="math inline">\(X\)</span>是半正定的，也等价于<span
class="math inline">\(X\)</span>的各阶主子式非负、特征值非负、<span
class="math inline">\(p^TXp \ge 0\)</span>等等；</p>
</blockquote>
<h3 id="保持凸性的运算">2.3 保持凸性的运算</h3>
<h4 id="求交">求交</h4>
<p>如果<span class="math inline">\(S_1\)</span>和<span
class="math inline">\(S_2\)</span>是凸集，那么<span
class="math inline">\(S_1 \cap
S_2\)</span>是凸集；该定义可以扩展到：无限数量各凸集相交结果为凸集；</p>
<p>每个闭合的凸集都可以表示为（通常是无限个）半空间的交集： <span
class="math display">\[
S = \cap \{\mathcal H ~|~ \mathcal H ~\mathrm{halfspace}, ~ S\subseteq
H\}
\]</span></p>
<h4 id="仿射函数">仿射函数</h4>
<p>仿射函数，<span class="math inline">\(f:\mathbb R^m \rightarrow
\mathbb R^n\)</span>，是线性函数和常数的和；记<span
class="math inline">\(S\)</span>是凸集，有： <span
class="math display">\[
\begin{split}
f(S) &amp;= \{f(x) ~|~ x\in S\} \in S \\
f^{-1}(S) &amp;= \{x | f(x) \in S\} \in S
\end{split}
\]</span> 缩放，平移，投影，笛卡尔积等都是仿射函数；</p>
<h4 id="线性分数函数">线性分数函数</h4>
<p>首先给出一类特殊的线性分数函数：<strong>透视函数</strong>(perspective
function)，<span class="math inline">\(P: \mathbb R^{n+1}\rightarrow
\mathbb R^n\)</span>，定义域<span
class="math inline">\(\operatorname{dom} P = \mathbb R^n\times \mathbb
R_{++}\)</span>，<span class="math inline">\(P(z, t) =
z/t\)</span>；透视函数是依向量的最后一个维度进行归一化，之后抛弃最后一个维度；</p>
<blockquote>
<p>齐次坐标==&gt;笛卡尔坐标；小孔成像；</p>
</blockquote>
<p><strong>线性分数函数</strong>(linear-fraction
function)是透视函数和仿射函数的复合函数，记仿射函数<span
class="math inline">\(g:\mathbb R^n \rightarrow \mathbb R^{m +
1}\)</span>： <span class="math display">\[
g(x) =
\left[
\begin{array}{c}
A\\
c^T
\end{array}
\right] x +
\left[
\begin{array}{c}
b\\
d
\end{array}
\right]
\]</span> 其中<span class="math inline">\(A\in \mathbb R^{m \times
n}\)</span>， <span class="math inline">\(c\in \mathbb R^n\)</span>，
<span class="math inline">\(b\in \mathbb R^m\)</span>， <span
class="math inline">\(d\in \mathbb R\)</span>，则线性分数函数,，<span
class="math inline">\(f = P\circ g\)</span>； <span
class="math display">\[
f(x) = \frac{Ax + b}{c^Tx + d}, ~\operatorname{dom}f =\{x ~|~ c^Tx +d
&gt; 0\}
\]</span></p>
<p>凸集<span
class="math inline">\(S\)</span>使用线性分数函数或者逆线性分数函数后，仍然得到凸集；</p>
<h3 id="广义不等式">2.4 广义不等式</h3>
<h4 id="真锥和广义不等式">真锥和广义不等式</h4>
<p>锥<span class="math inline">\(K\subseteq \mathbb
R^n\)</span>是<strong>真锥</strong>(proper cone)，当满足如下条件：</p>
<ul>
<li><span class="math inline">\(K\)</span>是凸的</li>
<li><span class="math inline">\(K\)</span>是闭合的（包含边界）</li>
<li><span
class="math inline">\(K\)</span>的内部是非空的（不能是扁的）</li>
<li><span class="math inline">\(K\)</span>是pointed（反对称）</li>
</ul>
<p>将<span class="math inline">\(\mathbb
R^n\)</span>上的偏序关系与真锥<span
class="math inline">\(K\)</span>联系起来： <span class="math display">\[
x \preceq_K y \iff y - x \in K
\]</span></p>
<p><span class="math display">\[
x \prec_K y \iff y - x \in \operatorname{int}K
\]</span></p>
<p>例如，非负象限、半正定锥等都是真锥；</p>
<h4 id="广义不等式的性质">广义不等式的性质</h4>
<ul>
<li>加法保号性：若<span class="math inline">\(x\preceq_K
y\)</span>且<span class="math inline">\(u\preceq_K v\)</span>，则<span
class="math inline">\(x + u \preceq_K y + v\)</span></li>
<li>非负数乘保号性</li>
<li>传递性</li>
<li>自反性</li>
<li>反对称性</li>
</ul>
<h4 id="最小值和极小值">最小值和极小值</h4>
<p><span class="math inline">\(x \in S\)</span>是<span
class="math inline">\(S\)</span>的最小值(the minimum)，当且仅当<span
class="math inline">\(S\subseteq x + K\)</span>，其中<span
class="math inline">\(x + K\)</span>含义是<span
class="math inline">\(S\)</span>中其他点均与<span
class="math inline">\(x\)</span>是可比较的，且大于等于（<span
class="math inline">\(\succeq_K\)</span>）<span
class="math inline">\(x\)</span>；显然，最小值不一定存在；</p>
<p><span class="math inline">\(x \in S\)</span>是<span
class="math inline">\(S\)</span>的极小值(a minimal)，当且仅当<span
class="math inline">\((x - K) \cap S = \{x\}\)</span>，含义是在<span
class="math inline">\(S\)</span>中能够与<span
class="math inline">\(x\)</span>比较且小于等于<span
class="math inline">\(x\)</span>的点只有<span
class="math inline">\(x\)</span>自己；显然，极小值不唯一，多个极小值之间不能相互比较；</p>
<h3 id="分离支持超平面">2.5 分离&amp;支持超平面</h3>
<h4 id="分离平面理论">分离平面理论</h4>
<p>记<span
class="math inline">\(C,D\)</span>是非空的不相交的凸集，那么存在<span
class="math inline">\(a\ne 0\)</span>和<span
class="math inline">\(b\)</span>，使得<span
class="math inline">\(a^Tx\le b\)</span>对于所有的<span
class="math inline">\(x\in C\)</span>，且<span
class="math inline">\(a^Tx\ge b\)</span>对于所有的<span
class="math inline">\(x\in D\)</span>。超平面<span
class="math inline">\(\{x|a^Tx=b\}\)</span>就是集合<span
class="math inline">\(C\)</span>和<span
class="math inline">\(D\)</span>的<strong>分离超平面</strong>(separating
hyperplane)。</p>
<details>
<summary>
一个未解决的问题
</summary>
<p>下面我们用欧式距离为例，验证一下分离平面理论（非严格证明）：</p>
<p>记集合<span class="math inline">\(C\)</span>,<span
class="math inline">\(D\)</span>之间的距离是 <span
class="math display">\[
\operatorname{dist}(C, D) = \inf\{\Vert u - v\Vert _2 ~|~ u\in C, v \in
D\}
\]</span></p>
<blockquote>
<p><span class="math inline">\(\inf\)</span>表示下界，<span
class="math inline">\(\sup\)</span>表示上界</p>
</blockquote>
<p>假设<span class="math inline">\(\operatorname{dist}(C, D) &gt;
0\)</span>，并且<span class="math inline">\(c\in C\)</span>，<span
class="math inline">\(d \in D\)</span>是满足<span
class="math inline">\(\Vert c - d\Vert _2 =
\operatorname{dist}(C,D)\)</span>的点对；</p>
<p>定义： <span class="math display">\[
a = d - c, \quad b =\frac{\Vert d\Vert ^2_2 - \Vert c\Vert ^2_2}{2}
\]</span> 我们将证明，仿射函数 <span class="math display">\[
f(x) = a^Tx - b = (d - c)^T(x - \frac{d + c}{2})
\]</span> 在<span class="math inline">\(C\)</span>上是非正的，在<span
class="math inline">\(D\)</span>上是非负的，下面我们将证明对于<span
class="math inline">\(\forall u\in D\)</span>，<span
class="math inline">\(f(u)\ge 0\)</span>，另一边的证明与之类似：</p>
<p>使用反证法，假设： <span class="math display">\[
\begin{split}
f(u) = (d - c)^T(u - \frac{d + c}{2}) &amp;&lt; 0 \\
(d-c)^T(u - d + \frac{d - c}{2}) &amp;&lt; 0 \\
(d-c)^T(u-d) + \frac{\Vert d - c\Vert ^2_2}{2} &amp;&lt; 0 \\
\Vert (d-c) + (u-d)\Vert _2^2 &amp;&lt; \Vert u - d\Vert _2^2 \\
\Vert u-c\Vert _2 &amp;&lt; \Vert u-d\Vert _2
\end{split}
\]</span> 由于<span class="math inline">\(D\)</span>是凸的，<span
class="math inline">\(\overrightarrow{CD}\)</span>和<span
class="math inline">\(\overrightarrow{DU}\)</span>的夹角是钝角，即<span
class="math inline">\((d-c)^T(u-d)\)</span> &lt; 0，</p>
<p>另外，记<span class="math inline">\(g(t) = \Vert d + t(u - d) -
c\Vert ^2_2\)</span>，观察到 <span class="math display">\[
\left.\frac{d}{dt}g(t) \right|_{t=0} = 2(d-c)^T(u-d) &lt; 0
\]</span> 说明该关于<span
class="math inline">\(t\)</span>的二次函数在<span
class="math inline">\(t=0\)</span>附近函数时单调递减的，对于很小的<span
class="math inline">\(t &gt; 0\)</span>，有： <span
class="math display">\[
g(t) &lt; g(0) = \Vert d-c\Vert _2^2
\]</span> 而在<span class="math inline">\(t=1\)</span>时，由于<span
class="math inline">\(d\)</span>是<span
class="math inline">\(D\)</span>中距离<span
class="math inline">\(C\)</span>最近的点，有<span
class="math inline">\(g(1) = \Vert u-c\Vert ^2_2 \ge \Vert d - c\Vert
^2_2\)</span></p>
<blockquote>
<p>这里是怎么推出与假设矛盾的？</p>
</blockquote>
</details>
<h5 id="仿射集与凸集也是可分的">仿射集与凸集也是可分的</h5>
<p>假设凸集<span class="math inline">\(C\)</span>和仿射集<span
class="math inline">\(D = \{Fu+g ~|~ u\in \mathbb R^m\}\)</span>（<span
class="math inline">\(F\in \mathbb R^{n \times
m}\)</span>）不相交，令<span class="math inline">\(a\ne
0\)</span>，<span class="math inline">\(b\)</span>满足<span
class="math inline">\(\forall x \in C, ~a^Tx\le b\)</span>，<span
class="math inline">\(\forall x \in D, ~a^Tx\ge b\)</span>，那么有 <span
class="math display">\[
\forall u \in \mathbb R^{m}, ~a^TFu \ge b -  a^Tg
\]</span> 而关于<span class="math inline">\(u\)</span>的线性方程<span
class="math inline">\(a^TFu\)</span>是无界的，除非<span
class="math inline">\(a^TF = 0\)</span>，因而有<span
class="math inline">\(b \ge a^Tg\)</span>；结合上文，我们得到了给出<span
class="math inline">\(C\)</span>,<span
class="math inline">\(D\)</span>的分离平面的方式： <span
class="math display">\[
a^TF = 0, \quad \forall x \in C, ~a^Tx \le a^Tg
\]</span></p>
<h5 id="严格可分">严格可分</h5>
<p>把可分离的定义中的<span class="math inline">\(\ge\)</span>换成<span
class="math inline">\(&gt;\)</span>，<span
class="math inline">\(\le\)</span>换成<span
class="math inline">\(&lt;\)</span>；</p>
<h4 id="支持超平面">支持超平面</h4>
<p>集合<span class="math inline">\(C\subseteq \mathbb
R^n\)</span>，<span class="math inline">\(x_0\)</span>是其边界<span
class="math inline">\(\operatorname{bd}C\)</span>上的一点，若<span
class="math inline">\(a\ne 0\)</span>满足<span
class="math inline">\(\forall x\in C, ~a^Tx \le
a^Tx_0\)</span>，则超平面<span class="math inline">\(\{x|a^Tx =
a^Tx_0\}\)</span>就是<span class="math inline">\(C\)</span>在<span
class="math inline">\(x_0\)</span>的<strong>支持超平面</strong>(supporting
hyperplane)。</p>
<h3 id="对偶锥和广义不等式">2.6 对偶锥和广义不等式</h3>
<h4 id="对偶锥">对偶锥</h4>
<p><span class="math inline">\(K\)</span>是一个锥，那么 <span
class="math display">\[
K^*=\{y~|~x^Ty\ge 0 ~\text{for all}~ x \in K\}
\]</span> 是<span
class="math inline">\(K\)</span>的<strong>对偶锥</strong>(dual
cone)；</p>
<p>若<span class="math inline">\(K^*=K\)</span>，则称<span
class="math inline">\(K\)</span>是<strong>自对偶</strong>(self-dual)的，例如：</p>
<ul>
<li>非负象限</li>
<li>半正定锥</li>
</ul>
<blockquote>
<p>矩阵<span class="math inline">\(X\)</span>,<span
class="math inline">\(Y\)</span>的点积的定义是<span
class="math inline">\(\operatorname{tr}(XY) = \sum_{i,j}X_{i,j}Y_{i,
j}\)</span></p>
</blockquote>
<p>对偶锥有如下性质：</p>
<ul>
<li><span class="math inline">\(K^*\)</span>是闭合的且凸的</li>
<li><span class="math inline">\(K_1\subseteq K_2\)</span>意味着<span
class="math inline">\(K_2^* \subseteq K_1^*\)</span></li>
<li><span class="math inline">\(K^{**}\)</span>是<span
class="math inline">\(K\)</span>的闭合的凸包</li>
</ul>
<h4 id="对偶广义不等式">对偶广义不等式</h4>
<p>真锥<span
class="math inline">\(K\)</span>的对偶锥还是真锥，由此我们有对偶的广义不等式：</p>
<ul>
<li><p><span class="math inline">\(x \preceq_K
y\)</span>当且仅当对于任意的<span class="math inline">\(\lambda
\succeq_{K^*} 0\)</span>，有<span class="math inline">\(\lambda^Tx \le
\lambda^T y\)</span></p>
<blockquote>
<p><span class="math inline">\(y - x \in K\)</span>，则对于<span
class="math inline">\(\lambda \in K^*\)</span>，满足<span
class="math inline">\((y - x)^T\lambda \ge 0\)</span>，即<span
class="math inline">\(y^T\lambda \ge x^T \lambda\)</span>，<span
class="math inline">\(\lambda^Ty = y^T\lambda\)</span>，<span
class="math inline">\(\lambda^Tx = x^T\lambda\)</span></p>
</blockquote></li>
<li><p><span class="math inline">\(x\prec_K
y\)</span>当且仅当对于任意的<span class="math inline">\(\lambda
\succeq_{K^*} 0\)</span>且<span class="math inline">\(\lambda \ne
0\)</span>，有<span class="math inline">\(\lambda^Tx &lt;
\lambda^Ty\)</span></p></li>
</ul>
<p>由于<span class="math inline">\(K^{**} =
K\)</span>，所以上述广义不等式中，<span
class="math inline">\(K\)</span>和<span
class="math inline">\(K^*\)</span>可以互换；</p>
<h4 id="对偶不等式下的最小值和极小值">对偶不等式下的最小值和极小值</h4>
<p>记<span class="math inline">\(x\)</span>是<span
class="math inline">\(S\)</span>的最小值（在<span
class="math inline">\(\preceq_K\)</span>意义下），<strong>当且仅当</strong><span
class="math inline">\(\forall \lambda\succ_{K*}0\)</span>，<span
class="math inline">\(x\)</span>是使得<span
class="math inline">\(\lambda^Tz\)</span>在<span
class="math inline">\(z\in S\)</span>上取得最小值的唯一解；</p>
<p><span class="math inline">\(\lambda^Tx \le \lambda^Tz, ~\forall z \in
S\)</span>，这也就让我们得到了集合<span
class="math inline">\(S\)</span>在<span
class="math inline">\(x\)</span>位置的严格支持平面（严格性来自于<span
class="math inline">\(x\)</span>是惟一的）： <span
class="math display">\[
\{z ~|~ \lambda^T(z-x) = 0\}
\]</span> 记<span class="math inline">\(x\)</span>是<span
class="math inline">\(S\)</span>的极小值（在<span
class="math inline">\(\preceq_K\)</span>意义下），<strong>当</strong><span
class="math inline">\(\forall \lambda\succ_{K*}0\)</span>，<span
class="math inline">\(x\)</span>是使得<span
class="math inline">\(\lambda^Tz\)</span>在<span
class="math inline">\(z\in S\)</span>上取得最小值的解；</p>
<ul>
<li><span class="math inline">\(x\)</span>是<span
class="math inline">\(S\)</span>的极小值，<span
class="math inline">\(\implies\)</span> <span
class="math inline">\(\forall \lambda\succ_{K*}0\)</span>，<span
class="math inline">\(x\)</span>是使得<span
class="math inline">\(\lambda^Tz\)</span>在<span
class="math inline">\(z\in
S\)</span>上取得最小值的解，<strong>是错的</strong></li>
<li><span class="math inline">\(S\)</span>是凸集，<span
class="math inline">\(x\)</span>是<span
class="math inline">\(S\)</span>的极小值，<span
class="math inline">\(\implies\)</span> <span
class="math inline">\(\forall \lambda\succ_{K*}0\)</span>，<span
class="math inline">\(x\)</span>是使得<span
class="math inline">\(\lambda^Tz\)</span>在<span
class="math inline">\(z\in
S\)</span>上取得最小值的解，<strong>是错的</strong></li>
<li><span class="math inline">\(S\)</span>是凸集，<span
class="math inline">\(x\)</span>是<span
class="math inline">\(S\)</span>的极小值，<span
class="math inline">\(\implies\)</span> <span
class="math inline">\(\forall \lambda\succeq_{K*}0\)</span>，<span
class="math inline">\(x\)</span>是使得<span
class="math inline">\(\lambda^Tz\)</span>在<span
class="math inline">\(z\in
S\)</span>上取得最小值的解，<strong>是对的</strong></li>
<li><span class="math inline">\(\forall
\lambda\succeq_{K*}0\)</span>，<span
class="math inline">\(x\)</span>是使得<span
class="math inline">\(\lambda^Tz\)</span>在<span
class="math inline">\(z\in S\)</span>上取得最小值的解 <span
class="math inline">\(\implies\)</span> <span
class="math inline">\(x\)</span>是<span
class="math inline">\(S\)</span>的极小值，<strong>是错的</strong></li>
</ul>
<h2 id="第3章-凸函数">第3章 凸函数</h2>
<h3 id="基本属性和例子">3.1 基本属性和例子</h3>
<h4 id="定义">定义</h4>
<p>函数<span class="math inline">\(f: \mathbb{R}^n \rightarrow
\mathbb{R}\)</span>是<strong>凸函数</strong>，当且仅当<span
class="math inline">\(\operatorname{dom} f\)</span>是凸集且对于<span
class="math inline">\(\forall x\)</span>，<span
class="math inline">\(y\in \operatorname{dom} f\)</span>，且有<span
class="math inline">\(0\le \theta \le 1\)</span>，满足： <span
class="math display">\[
f(\theta x + (1 - \theta)y) \le \theta f(x) + (1 - \theta)f(y)
\]</span></p>
<p>在上述定理的基础上，满足在<span class="math inline">\(x \ne
y\)</span>且<span class="math inline">\(0 &lt; \theta &lt;
1\)</span>时不等式严格取<span class="math inline">\(&lt;\)</span>，
<span class="math inline">\(f\)</span>是严格凸函数；</p>
<p>如果<span
class="math inline">\(-f\)</span>是（严格）凸函数，那么<span
class="math inline">\(f\)</span>是（严格）<strong>凹(concave)</strong>函数；</p>
<h4 id="把凸函数限制到直线上">把凸函数限制到直线上</h4>
<p><span class="math inline">\(f: \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> 是凸函数，当且仅当对于任意的 <span
class="math inline">\(x\in \operatorname{dom} f, ~v\in
\mathbb{R}^n\)</span>，函数 <span class="math inline">\(g:
\mathbb{R}\rightarrow \mathbb{R}\)</span> <span class="math display">\[
\begin{array}{ll}
  g(t) = f(x + tv), &amp;\operatorname{dom}g = \{t ~|~ x + tv \in
\operatorname{dom} f\}
\end{array}
\]</span> 是关于<span class="math inline">\(t\)</span>的凸函数；</p>
<p>该方法可以用于检查函数的非凸性：随机生成线条，判断是否是<span
class="math inline">\(\mathbb
R\)</span>上的凸函数，如果出现非凸的，既可以证明原函数非凸；</p>
<h4 id="值域扩展">值域扩展</h4>
<p><span class="math display">\[
\tilde{f}(x) = \left\{
\begin{array}{ll}
  f(x)&amp;x\in \operatorname{dom}f\\
  \infty&amp;x\notin \operatorname{dom}f
\end{array}
\right.
\]</span></p>
<blockquote>
<p>使用值域扩展可以在描述函数的一些性质时（例如上述的凸性）省略对定义域的强调，在之后所有的凸函数都经过了隐式的值域扩展；</p>
<p><span class="math inline">\(f\)</span>是凸函数时，<span
class="math inline">\(\operatorname{dom}f\)</span> 之外的值域扩展到
<span class="math inline">\(\infty\)</span>，<span
class="math inline">\(f\)</span>是凹函数时，<span
class="math inline">\(\operatorname{dom}f\)</span> 之外的值域扩展到
<span class="math inline">\(-\infty\)</span></p>
</blockquote>
<h4 id="一阶导条件-first-order-condition">一阶导条件 (first-order
condition)</h4>
<p>如果<span class="math inline">\(f\)</span>是可导的，那么<span
class="math inline">\(f\)</span>是凸函数当且仅当<span
class="math inline">\(\operatorname{dom}f\)</span>是凸集且碎对于<span
class="math inline">\(\forall x, y\in \operatorname{dom} f\)</span>，有
<span class="math display">\[
f(y) \ge f(x) + \nabla f(x)^T(y - x)
\]</span></p>
<blockquote>
<p>泰勒展开在<span
class="math inline">\(x\)</span>处的一阶展开函数总是低于原函数，即说明原函数是凸的</p>
</blockquote>
<p><span class="math inline">\(f\)</span>是严格凸的，当且仅当<span
class="math inline">\(\operatorname{dom} f\)</span>是凸集且<span
class="math inline">\(\forall x, y \in \operatorname{dom} f, ~x\ne
y\)</span>，有： <span class="math display">\[
f(y) &gt; f(x) + \nabla f(x)^T(y - x)
\]</span></p>
<h4 id="二阶导条件-second-order-conditions">二阶导条件 (second-order
conditions)</h4>
<p>如果<span class="math inline">\(f\)</span>是二次可导的，那么<span
class="math inline">\(f\)</span>是凸函数，当且仅当对于<span
class="math inline">\(\forall x \in \operatorname{dom} f\)</span>，有
<span class="math display">\[
\nabla^2f(x) \succeq 0
\]</span></p>
<blockquote>
<p>对于一元函数，上式意味着二阶导非负，对于多元函数，上式意味着Hessian矩阵是对称半正定的；
直观地考虑，凸性意味着函数在任何一点都至少是向上“弯曲”的</p>
</blockquote>
<p><span class="math inline">\(f\)</span>是严格凸的，<strong>当</strong>
<span class="math inline">\(\forall x \in
\operatorname{dom}f\)</span>，有 <span class="math display">\[
\nabla^2f(x) \succ 0
\]</span></p>
<h4 id="更多凸凹函数的例子">更多凸（凹）函数的例子</h4>
<p>首先是一些简单的一元函数，使用基本的不等式定义或者二阶导条件就可以证明：</p>
<ul>
<li><span class="math inline">\(e^{ax}, ~x\in \mathbb{R}\)</span>，
<span class="math inline">\(\forall a \in
\mathbb{R}\)</span>，是凸函数</li>
<li><span class="math inline">\(x^{a}, ~x\in
\mathbb{R}_{++}\)</span>，<span class="math inline">\(a\ge
1\)</span>或者<span class="math inline">\(a \le
0\)</span>时是凸函数，<span class="math inline">\(0\le a \le
1\)</span>时是凹函数</li>
<li><span class="math inline">\(|x|^p, ~x\in \mathbb{R}\)</span>，<span
class="math inline">\(p \ge 1\)</span>，是凸函数</li>
<li><span class="math inline">\(\log x, ~x\in
\mathbb{R}_{++}\)</span>是凹函数；</li>
<li><span class="math inline">\(x\log x, ~x \in
\mathbb{R}_{+}\)</span>是凸函数；</li>
</ul>
<p>接下来是一些多元函数（向量/矩阵函数）</p>
<ul>
<li><span
class="math inline">\(\mathbb{R}^n\)</span>上的范数都是凸函数；</li>
<li><span
class="math inline">\(\mathbb{R}^n\)</span>上求最大值是凸函数；</li>
<li>二次比一次函数，<span class="math inline">\(f(x, y) = x^2 / y,
(x,y)\in \mathbb{R} \times
\mathbb{R}_{++}是凸函数\)</span>；扩展到矩阵分数函数，<span
class="math inline">\(f(x, Y) = x^TY^{-1}x, (x, Y) \in
\mathbb{R}^n\times \mathbb{S}^n_{++}\)</span>是凸函数；</li>
<li>Log-sum-exp函数，是凸函数（该函数也可以视为max函数的soft版本）</li>
<li>几何均值函数，是<span
class="math inline">\(\mathbb{R}_{++}^n\)</span>上的凹函数；</li>
<li>Log-determinant函数，<span class="math inline">\(f(X) = \log \det X,
~X \in S^n_{++}\)</span>是凹函数；</li>
</ul>
<blockquote>
<p>对于范数而言，其定义中保证满足三角不等式条件，使得范数都是凸的；</p>
<p>其他函数的证明见原教材P73-74, 以及P76 Example3.4</p>
</blockquote>
<h4 id="sublevel-sets">Sublevel sets</h4>
<p>函数<span class="math inline">\(f: \mathbb{R}^n \rightarrow
\mathbb{R}\)</span>的<span
class="math inline">\(\alpha\)</span>-sublevel set是： <span
class="math display">\[
C_\alpha = \{x \in \operatorname{dom} f ~|~ f(x) \le \alpha\}
\]</span> 凸函数的sublevel sets也都是凸集；反之并不成立；</p>
<h4 id="epigraph">Epigraph</h4>
<p>函数<span class="math inline">\(f: \mathbb{R}^n \rightarrow
\mathbb{R}\)</span>的图(graph)定义为如下集合： <span
class="math display">\[
\{(x, f(x)) ~|~ x \in \operatorname{dom} f\}
\]</span> 其epigraph定义为如下集合： <span class="math display">\[
\{(x, t) ~|~ x \in \operatorname{dom}f, f(x) \le t\}
\]</span></p>
<blockquote>
<p>epigraph想象为一个函数图像上面的所有部分</p>
</blockquote>
<p>函数<span class="math inline">\(f: \mathbb{R}^n \rightarrow
\mathbb{R}\)</span>是凸函数当且仅当其epigraph是凸集；</p>
<p>相对的，还有hypograph： <span class="math display">\[
\{(x, t) ~|~ x \in \operatorname{dom}f, t \le f(x)\}
\]</span></p>
<p>函数<span class="math inline">\(f: \mathbb{R}^n \rightarrow
\mathbb{R}\)</span>是凹函数当且仅当其hypograph是凸集；</p>
<h4 id="琴生不等式jesens-inequality">琴生不等式(Jesen's inequality)</h4>
<p>凸函数的基本不等式又叫琴声不等式，它可以扩展到多个元素，在扩展到无限个元素（积分），从而也可以写成期望形式：
<span class="math display">\[
f(\mathbf{E}x) \le \mathbf{E}f(x)
\]</span></p>
<blockquote>
<p>在高中数学中我们见到的琴声不等式是最基础的一种形式，<span
class="math inline">\(f\)</span>是凸函数，有：</p>
<p><span class="math display">\[
f(\frac{x + y}{2}) \le \frac{f(x) + f(y)}{2}
\]</span></p>
</blockquote>
<h4 id="更多琴声不等式的推广">更多琴声不等式的推广</h4>
<p>琴声不等式应用到各种凸函数上可以得到许多有趣的结果，见教材P78的算术-集合均值不等式和Holder's不等式；</p>
<h3 id="维持凸性的操作">3.2 维持凸性的操作</h3>
<p>经过如下对函数的操作，得到的函数仍然保持其凸性；</p>
<h4 id="非负加权和">非负加权和</h4>
<p><span class="math display">\[
f = \sum_{i=1}^m w_if_i, ~w_i \ge 0
\]</span></p>
<p><span class="math display">\[
g(x) = \int_A w(y)f(x, y)dy, ~\forall y \in \mathcal{A}, w(y) \ge 0
\]</span> 其中，<span class="math inline">\(f(x, y)\)</span>是<span
class="math inline">\(\forall y \in \mathcal{A}\)</span>下，对<span
class="math inline">\(x\)</span>的凸函数；</p>
<h4 id="仿射组合-1">仿射组合</h4>
<p><span class="math inline">\(f: \mathbb{R}^n \rightarrow
\mathbb{R}\)</span>，<span class="math inline">\(A \in \mathbb{R}^{n
\times m}\)</span>， <span class="math inline">\(b \in
\mathbb{R}^m\)</span>，<span class="math inline">\(g: \mathbb{R}^m
\rightarrow \mathbb{R}\)</span>：</p>
<p><span class="math display">\[
g(x) = f(Ax + b)
\]</span></p>
<h4 id="逐点最大值或上确界">逐点最大值或上确界</h4>
<p><span class="math inline">\(f_1, \dots,
f_m\)</span>是凸函数，那么逐点最大值<span class="math inline">\(f =
\max\{f_1(x), \dots, f_m(x)\}\)</span>是凸函数；</p>
<p>扩展到无限的凸函数集合的逐点上确界，仍然为凸函数：</p>
<p><span class="math display">\[
g(x) = \sup_{y\in \mathcal{A}}f(x, y)
\]</span></p>
<p>其中<span class="math inline">\(f(x,y)\)</span>是<span
class="math inline">\(x\)</span>的凸函数，<span
class="math inline">\(y\)</span>可以是标量、向量、矩阵等等，则<span
class="math inline">\(g(x)\)</span>是<span
class="math inline">\(x\)</span>的凸函数； <span
class="math inline">\(g(x)\)</span>的定义域为：</p>
<p><span class="math display">\[
\operatorname{dom}g = \{x ~|~ (x, y) \in \operatorname{dom}f ~\forall y
\in \mathcal{A}, ~\sup_{y\in \mathcal{A}}f(x, y) &lt; \infty\}
\]</span></p>
<p>从epigraph的角度看，求凸函数集合逐点上确界，相当于对它们的epigraph求交，而凸集的交仍然为凸集，因此，逐点上确界也是凸函数；</p>
<p>凹函数集合逐点下确界得到凹函数；</p>
<details>
<summary>
几乎所有的凸函数都可以表示成仿射函数族的上确界的形式
</summary>
</details>
<h4 id="复合函数">复合函数</h4>
<p>本节研究 <span class="math inline">\(h:
\mathbb{R}^k\rightarrow\mathbb{R}\)</span>， <span
class="math inline">\(g: \mathbb{R}^n\rightarrow\mathbb{R}^k\)</span>
的条件对复合函数 <span class="math inline">\(f = h\circ g:
\mathbb{R^n}\rightarrow\mathbb{R}\)</span> 的凸性的影响；</p>
<h5 id="标量复合函数">标量复合函数</h5>
<p>即 <span class="math inline">\(k=1\)</span> 的情况；</p>
<p>假设<span class="math inline">\(f\)</span>的二阶导存在，即 <span
class="math display">\[
f&#39;&#39;(x) = h&#39;&#39;(g(x))g&#39;(x)^2 +
h&#39;(g(x))g&#39;&#39;(x)
\]</span></p>
<p>根据二阶导条件，我们可以得出如下结论：</p>
<ul>
<li><span class="math inline">\(h\)</span> 是凸的（<span
class="math inline">\(h&#39;&#39;\ge 0\)</span>），<span
class="math inline">\(g\)</span> 是凸的（<span
class="math inline">\(g&#39;&#39;\ge 0\)</span>），且 <span
class="math inline">\(\tilde{h}\)</span> 非减( <span
class="math inline">\(h&#39;\ge 0\)</span> )，那么 <span
class="math inline">\(f\)</span> 是凸的；</li>
<li><span class="math inline">\(h\)</span> 是凸的（<span
class="math inline">\(h&#39;&#39;\ge 0\)</span>），<span
class="math inline">\(g\)</span> 是凹的（<span
class="math inline">\(g&#39;&#39;\le 0\)</span>），且 <span
class="math inline">\(\tilde{h}\)</span> 非增( <span
class="math inline">\(h&#39;\le 0\)</span> )，那么 <span
class="math inline">\(f\)</span> 是凸的；</li>
<li><span class="math inline">\(h\)</span> 是凹的（<span
class="math inline">\(h&#39;&#39;\le 0\)</span>），<span
class="math inline">\(g\)</span> 是凸的（<span
class="math inline">\(g&#39;&#39;\ge 0\)</span>），且 <span
class="math inline">\(\tilde{h}\)</span> 非增( <span
class="math inline">\(h&#39;\le 0\)</span> )，那么 <span
class="math inline">\(f\)</span> 是凹的；</li>
<li><span class="math inline">\(h\)</span> 是凹的（<span
class="math inline">\(h&#39;&#39;\ge 0\)</span>），<span
class="math inline">\(g\)</span> 是凹的（<span
class="math inline">\(g&#39;&#39;\le 0\)</span>），且 <span
class="math inline">\(\tilde{h}\)</span> 非减( <span
class="math inline">\(h&#39;\ge 0\)</span> )，那么 <span
class="math inline">\(f\)</span> 是凹的；</li>
</ul>
<blockquote>
<p>注意，这里给出的都是判断<span
class="math inline">\(f\)</span>的凸性的充分非必要条件；</p>
<p>事实上，该定理在<span
class="math inline">\(f\)</span>二阶导不存在的情况下依然适用，教材P85给出了基于基本不等式的证明；</p>
</blockquote>
<h5 id="向量复合函数">向量复合函数</h5>
<blockquote>
<p>本节的内容非常重要，因为以后的几乎每个知识点都要用到，且先前学的一些维持函数凸性的操作可以视为该操作的特殊情况
—— Prof. Boyd的课堂</p>
</blockquote>
<p><span class="math display">\[
f(x) = h(g_1(x), \dots, g_k(x))
\]</span></p>
<p>有如下结论：</p>
<ul>
<li><span class="math inline">\(h\)</span> 是凸的，<span
class="math inline">\(g\)</span> 是凸的，且 <span
class="math inline">\(\tilde{h}\)</span> 在每个方向上非减，那么 <span
class="math inline">\(f\)</span> 是凸的；</li>
<li><span class="math inline">\(h\)</span> 是凸的，<span
class="math inline">\(g\)</span> 是凹的，且 <span
class="math inline">\(\tilde{h}\)</span> 在每个方向上非增，那么 <span
class="math inline">\(f\)</span> 是凸的；</li>
<li><span class="math inline">\(h\)</span> 是凹的，<span
class="math inline">\(g\)</span> 是凸的，且 <span
class="math inline">\(\tilde{h}\)</span> 在每个方向上非增，那么 <span
class="math inline">\(f\)</span> 是凹的；</li>
<li><span class="math inline">\(h\)</span> 是凹的，<span
class="math inline">\(g\)</span> 是凹的，且 <span
class="math inline">\(\tilde{h}\)</span> 在每个方向上非减，那么 <span
class="math inline">\(f\)</span> 是凹的；</li>
</ul>
<h4 id="部分最小化">部分最小化</h4>
<p>如果<span class="math inline">\(f(x, y)\)</span>是(x,
y)的凸函数，<span class="math inline">\(C\)</span>是凸集，那么 <span
class="math display">\[
g(x) = \inf_{y\in C}f(x, y)
\]</span> 是凸函数；其中，<span
class="math inline">\(g\)</span>的定义域是<span
class="math inline">\(f\)</span>的定义域在<span
class="math inline">\(x\)</span>方向的投影： <span
class="math display">\[
\operatorname{dom}g = \{x ~|~ (x, y) \in \operatorname{dom}f ~\text{for
some}~ y \in C\}
\]</span></p>
<h4 id="函数的透视">函数的透视</h4>
<p>如果<span
class="math inline">\(f:\mathbb{R}^n\rightarrow\mathbb{R}\)</span>，那么其透视(perspective
of a function)为函数<span class="math inline">\(g:
\mathbb{R}^{n+1}\rightarrow\mathbb{R}\)</span>： <span
class="math display">\[
g(x, t) = tf(x/t)
\]</span> 定义域为： <span class="math display">\[
\operatorname{dom}g = \{(x, t) ~|~ x/t \in \operatorname{dom}f, t &gt;
0\}
\]</span></p>
<p>前文介绍过一种perspective function，即<span
class="math inline">\(P(x,t) =
x/t\)</span>，按照最后一个维度归一化后去除该维度，是维持凸集性质不变的操作，可以用epigraph将这里的perspective
of a function联系起来：</p>
<p><span class="math display">\[
\begin{split}
  (x, t, s) \in \operatorname{epi} g &amp;\iff tf(x/t) \le s \\
  &amp;\iff f(x/t) \le s/t \\
  &amp;\iff (x/t, s/t) \in \operatorname{epi} f
\end{split}
\]</span> 上式说明，<span class="math inline">\(\operatorname{epi}
g\)</span> 是 <span class="math inline">\(\operatorname{epi} f\)</span>
投影的原像，因为 <span class="math inline">\(\operatorname{epi}
f\)</span> 是凸集， 因此<span class="math inline">\(\operatorname{epi}
g\)</span> 是凸集；</p>
<h3 id="共轭函数the-conjugate-function">3.3 共轭函数(The conjugate
function)</h3>
<h4 id="定义-1">定义</h4>
<p>函数<span
class="math inline">\(f:\mathbb{R}^n\rightarrow\mathbb{R}\)</span>
的共轭函数<span
class="math inline">\(f^*:\mathbb{R}^n\rightarrow\mathbb{R}\)</span>
定义为： <span class="math display">\[
f^*(y) = \sup_{x\in \operatorname{dom}f}(y^Tx - f(x))
\]</span></p>
<p>此上确界在<span
class="math inline">\(\operatorname{dom}f\)</span>内应当是有限的（bounded
above）；</p>
<p>直观地看，共轭函数的含义是在每个<span
class="math inline">\(x\)</span>处最大化线性函数<span
class="math inline">\(y^Tx\)</span>与原函数<span
class="math inline">\(f(x)\)</span>的差，若<span
class="math inline">\(f\)</span>可导，那么可以得到<span
class="math inline">\(y = f&#39;(x)\)</span>；</p>
<ul>
<li><span class="math inline">\(f(x) = ax+b\)</span> 的共轭函数显然为
<span class="math inline">\(f^*(y) = -b, y = \{a\}\)</span>；</li>
<li><span class="math inline">\(f(x) = \log x\)</span>，<span
class="math inline">\(xy + \log x\)</span> 在<span
class="math inline">\(y\ge 0\)</span>时是<span
class="math inline">\(x\)</span>的单增函数，没有有限的上确界，在<span
class="math inline">\(y &lt; 0\)</span>时，在<span
class="math inline">\(x = -\frac{1}{y}\)</span>取得最大值，得到<span
class="math inline">\(f^*(y) = -1 - \log(-y)\)</span>；</li>
</ul>
<blockquote>
<p>更多例子见教材P92-94</p>
</blockquote>
<h4 id="基本性质">基本性质</h4>
<h5 id="fenchels-inequality">Fenchel's inequality</h5>
<p>根据定义，我们可得对于任意的<span
class="math inline">\(x,y\)</span>，有： <span class="math display">\[
f(x) + f^*(y) \le y^Tx
\]</span></p>
<blockquote>
<p>如果<span
class="math inline">\(f\)</span>是可导的，该不等式又称为Young's
inequality</p>
</blockquote>
<h5 id="共轭的共轭">共轭的共轭</h5>
<p>如果函数<span
class="math inline">\(f\)</span>是凸的，且是闭合的(closed)，那么有<span
class="math inline">\(f^{**} = f\)</span></p>
<details>
<summary>
Closed funtions
</summary>
<p>函数<span class="math inline">\(f: \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> 是闭的，当：对于<span class="math inline">\(\forall
\alpha in \mathbb{R}\)</span>，其sublevel set <span
class="math display">\[
\{x \in \operatorname{dom} f ~|~ f(x) \le \alpha\}
\]</span> 是闭的；这也等价于<span
class="math inline">\(\operatorname{epi} f\)</span>是闭的；</p>
<p>下面两个推论必要便于判断closed functions: - 如果函数<span
class="math inline">\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)</span>
是连续的，且<span
class="math inline">\(\operatorname{dom}f\)</span>是闭的， 则<span
class="math inline">\(f\)</span>是闭的； - 如果函数<span
class="math inline">\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)</span>
是连续的，且<span
class="math inline">\(\operatorname{dom}f\)</span>是开的，<span
class="math inline">\(f\)</span>是闭的当且仅当从任意序列收敛到<span
class="math inline">\(\operatorname{dom}f\)</span>
的边界上的一点时，<span class="math inline">\(f\)</span>收敛到<span
class="math inline">\(\infty\)</span>；</p>
<blockquote>
<p>注意，<span class="math inline">\([0, \infty]\)</span> 是闭的，<span
class="math inline">\((0, \infty]\)</span> 是开的</p>
</blockquote>
</details>
<h5 id="可导函数">可导函数</h5>
<p>求可导函数的共轭函数，又称<span
class="math inline">\(f\)</span>的勒让德变换(Legendre transform)；</p>
<p>假设<span class="math inline">\(f\)</span>是可导的凸函数，且<span
class="math inline">\(\operatorname{dom}f =
\mathbb{R}^n\)</span>，那么<span class="math inline">\(y = \nabla
f(x^*)\)</span>等价于<span class="math inline">\(x^*\)</span>使得<span
class="math inline">\(y^Tx-f(x)\)</span>最小；因此，将<span
class="math inline">\(y = \nabla
f(x^*)\)</span>作为新变量，有新的关于<span
class="math inline">\(y\)</span>的函数： <span class="math display">\[
f^*(y) = x^{*T}\nabla f(x^*) - f(x^*)
\]</span></p>
<h3 id="quasiconvex-函数">3.4 Quasiconvex 函数</h3>
<h4 id="定义-2">定义</h4>
<p>函数 <span
class="math inline">\(f:\mathbb{R}^n\rightarrow\mathbb{R}\)</span>，满足：其定义域及所有的sublevel
sets都是凸集，即 <span class="math display">\[
\forall \alpha \in \mathbb{R}，~S_\alpha = \{x\in \operatorname{dom} f
~|~ f(x) \le \alpha\}
\]</span> 是凸集，则称<span
class="math inline">\(f\)</span>是quasiconvex（或者称unimodel）的；</p>
<p>若<span class="math inline">\(-f\)</span>是quasiconvex的，则称<span
class="math inline">\(f\)</span>是quasiconcave的；如果<span
class="math inline">\(f\)</span>既是quasiconvex又是quasiconcave的，则称<span
class="math inline">\(f\)</span>是quasilinear的；</p>
<p>显然，convex的函数一定是quasiconvex的，但是反之并不成立；</p>
<h4 id="基本属性">基本属性</h4>
<p>琴生不等式的quasiconvex版本：<span
class="math inline">\(f\)</span>是quasiconvex，当且仅当<span
class="math inline">\(\operatorname{dom}f\)</span>是凸集且对于任意的
<span class="math inline">\(x, y \in \operatorname{dom}
f\)</span>，<span class="math inline">\(0 \le \theta \le
1\)</span>，有： <span class="math display">\[
f(\theta x + (1 - \theta)y) \le \max\{f(x), f(y)\}
\]</span></p>
<p>与convex类似的，可以选取<span
class="math inline">\(f\)</span>中的任意一条直线，来检查<span
class="math inline">\(f\)</span>是否是非quasiconvex的；在<span
class="math inline">\(\mathbb{R}\)</span>上的连续函数<span
class="math inline">\(f\)</span>是quasiconvex，当且仅当满足如下至少一个条件：</p>
<ul>
<li><span class="math inline">\(f\)</span> 是非减的；</li>
<li><span class="math inline">\(f\)</span> 是非增的；</li>
<li>存在 <span class="math inline">\(c \in \operatorname{dom}
f\)</span>，使得存在 <span class="math inline">\(t \in
\operatorname{dom} f\)</span>，在 <span class="math inline">\(t \le
c\)</span> 时 <span class="math inline">\(f\)</span> 是非增的，<span
class="math inline">\(t\ge c\)</span> 时 <span
class="math inline">\(f\)</span> 是非减的；</li>
</ul>
<h4 id="一阶导条件">一阶导条件</h4>
<p>假设函数 <span
class="math inline">\(f:\mathbb{R}^n\rightarrow\mathbb{R}\)</span>
是可导的，<span class="math inline">\(f\)</span> 是quasiconvex当且仅当
<span class="math inline">\(\operatorname{dom} f\)</span> 是凸集且 <span
class="math inline">\(\forall x, y \in \operatorname{dom} f\)</span>，
<span class="math display">\[
f(y) \le f(x) \implies \nabla f(x)^T(y - x) \le 0
\]</span></p>
<p>也就是说，<span class="math inline">\(\nabla f(x) \ne 0\)</span>
时，它是 <span class="math inline">\(\{y ~|~ f(y) \le f(x) \}\)</span>
在<span class="math inline">\(x\)</span>处的支持平面的法向量；</p>
<h4 id="二阶导条件">二阶导条件</h4>
<p>假设<span class="math inline">\(f\)</span> 是可二次导的，若<span
class="math inline">\(f\)</span>是quasiconvex，那么对于任意的<span
class="math inline">\(x\in \operatorname{dom}f, ~y \in
\mathbb{R}^n\)</span>，有： <span class="math display">\[
y^T\nabla f(x) = 0 \implies y^T\nabla^2 f(x)y \ge 0
\]</span></p>
<blockquote>
<p>如果<span class="math inline">\(\nabla f(x) =
0\)</span>，该条件意味着<span class="math inline">\(\nabla^2 f(x) \succ
0\)</span>；如果<span class="math inline">\(\nabla f(x) \ne
0\)</span>，该条件意味着<span class="math inline">\(\nabla^2
f(x)\)</span>在<span class="math inline">\(\nabla
f(x)^{\bot}\)</span>的(n-1)维子空间中是半正定的，即<span
class="math inline">\(\nabla^2 f(x)\)</span>最多只有1个负特征值；</p>
</blockquote>
<p>对于在<span
class="math inline">\(\mathbb{R}\)</span>上的quasiconvex函数，有： <span
class="math display">\[
\nabla f(x) = 0 \implies \nabla^2 f(x) \ge 0
\]</span></p>
<p>反之，如果<span class="math inline">\(f\)</span>满足对于任意的<span
class="math inline">\(x\in \operatorname{dom}f, ~y \in \mathbb{R}^n,
~y\ne 0\)</span>，有： <span class="math display">\[
y^T\nabla f(x) = 0 \implies y^T\nabla^2 f(x)y &gt; 0
\]</span> 则<span class="math inline">\(f\)</span>是quasiconvex；</p>
<h4 id="保持quasiconvexity的操作">保持quasiconvexity的操作</h4>
<h5 id="非负权重逐点最大">非负权重逐点最大</h5>
<h5 id="复合函数-1">复合函数</h5>
<p>函数 <span class="math inline">\(g: \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> 是quasiconvex，<span class="math inline">\(h:
\mathbb{R} \rightarrow \mathbb{R}\)</span> 是非减的，则 <span
class="math inline">\(f = h \circ g\)</span> 是 quasiconex；</p>
<h5 id="部分最小化-1">部分最小化</h5>
<h4
id="用凸函数族来表示quasiconvex函数">用凸函数族来表示quasiconvex函数</h4>
<p>利用quasiconvex函数<span class="math inline">\(f\)</span>的sublevel
sets都是凸集，我们可以构造一个凸函数族<span
class="math inline">\(\phi_t\)</span>（<span
class="math inline">\(t\)</span>相当于索引）： <span
class="math display">\[
f(x) \le t \iff \phi_t(x) \le 0
\]</span></p>
<blockquote>
<p>用<span class="math inline">\(f\)</span>的t-sublevel set是<span
class="math inline">\(\phi_t\)</span>的0-sublevel set</p>
</blockquote>
<p>由于对于任意的<span class="math inline">\(x\)</span>，<span
class="math inline">\(s \ge t\)</span>，有<span
class="math inline">\(\phi_t(x) \le 0 \implies \phi_s(x) \le
0\)</span>，即<span
class="math inline">\(\phi_t(x)\ge\phi_s(x)\)</span>，因此该函数族关于<span
class="math inline">\(t\)</span>是非增的；</p>
<h3 id="log-concave">3.5 Log concave</h3>
<h3 id="广义不等式下的凸性">3.6 广义不等式下的凸性</h3>
<h4 id="广义不等式下的单调性">广义不等式下的单调性</h4>
<p>假设 <span class="math inline">\(K\in \mathbb{R}^n\)</span> 是真锥，
与 <span class="math inline">\(\preceq_K\)</span> 有关，函数 <span
class="math inline">\(f: \mathbb{R}^n\rightarrow \mathbb{R}\)</span>
是K-非减的，如果： <span class="math display">\[
x\preceq_K y \implies f(x) \le f(y)
\]</span></p>
<p>是K-增的，如果： <span class="math display">\[
x\preceq_K y, ~x\ne y \implies f(x) &lt; f(y)
\]</span></p>
<p>一个定义域为凸集的，可导的函数 <span class="math inline">\(f\)</span>
是K-非减的，当且仅当： <span class="math display">\[
\nabla f(x) \succeq_{K^*} 0
\]</span></p>
<p>当对于任意的 <span class="math inline">\(x\in \operatorname{dom}
f\)</span>，都有： <span class="math display">\[
\nabla f(x)\succ_{K^*} 0
\]</span> 那么，<span class="math inline">\(f\)</span> 是K-增的；</p>
<h4 id="广义不等式下的凸性-1">广义不等式下的凸性</h4>
<p>假设<span class="math inline">\(K \subseteq \mathbb{R}^m\)</span>
是真锥，<span class="math inline">\(f: \mathbb{R}^n \rightarrow
\mathbb{R}^n\)</span> 是 K-convex 的，如果对于任意的 <span
class="math inline">\(x, y\)</span>，<span
class="math inline">\(0\le\theta\le 1\)</span> ： <span
class="math display">\[
f(\theta x+(1-\theta) y)\preceq_K \theta f(x) + (1-\theta)f(y)
\]</span></p>
<h5 id="matrix-convexity">Matrix convexity</h5>
<p>对于函数 <span class="math inline">\(f:\mathbb{R}^n\rightarrow
\mathbf{S}^m\)</span> 是广义不等式下的凸函数的（<span
class="math inline">\(K=\mathbf{S}_+^m\)</span>），称该函数具有matrix
convexity；</p>
<p>其等价定义是，对于任意的向量 <span
class="math inline">\(z\)</span>，满足：关于<span
class="math inline">\(z\)</span>的标量函数 <span
class="math inline">\(z^Tf(x)z\)</span> 总是凸函数；</p>
<blockquote>
<p>按照二阶导条件，matrix convexity是可以等价于 <span
class="math inline">\(f(x) \succeq 0\)</span> 的吗？</p>
</blockquote>
<h2 id="第4章-凸优化问题">第4章 凸优化问题</h2>
<h3 id="优化问题">4.1 优化问题</h3>
<h4 id="基本术语">基本术语</h4>
<p>优化问题有如下标准形式(standard form)：</p>
<p><span class="math display">\[
\begin{array}{lll}
  \text{minimize} &amp;f_0(x)\\
  \text{subject to} &amp;f_i(x)\le 0, &amp;i=1, \dots, m\\
  &amp;h_i(x) = 0, &amp;i=1,\dots, p\\
\end{array}
\]</span></p>
<blockquote>
<p><span class="math inline">\(\min\)</span> 是求得最小值的函数，<span
class="math inline">\(\text{minimize}\)</span> 是最小化的目标</p>
</blockquote>
<ul>
<li><span class="math inline">\(x \in \mathbb{R}^n\)</span>
称为优化变量(optimization variable)</li>
<li><span class="math inline">\(f_0: \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> 称为目标函数(object function)或者代价函数(cost
function)</li>
<li><span class="math inline">\(f_i(x)\le 0\)</span>
称为不等式限制(inequality constraints)，函数 <span
class="math inline">\(f_i: \mathbb{R}^n \rightarrow \mathbb{R}\)</span>
称为不等式限制函数(inequality constraint functions)</li>
<li><span class="math inline">\(h_i(x)= 0\)</span> 称为等式限制(equality
constraints)，函数 <span class="math inline">\(h_i: \mathbb{R}^n
\rightarrow \mathbb{R}\)</span> 称为等式限制函数(equality constraint
functions)</li>
<li>如果没有限制，即 <span class="math inline">\(m = p =
0\)</span>，则称问题是unconstrained</li>
</ul>
<p>问题的定义域是： <span class="math display">\[
\mathcal{D} = \bigcap_{i=0}^m\operatorname{dom}f_i \cap
\bigcap_{i=1}^p\operatorname{dom}h_i
\]</span></p>
<p>满足所有限制条件的的解称为可行(feasible)解，可行解的集合称为可行集合(feasible
set)或者限制集合(constrain set)</p>
<p>问题的最优值(optimal value) <span
class="math inline">\(p^\star\)</span>： <span class="math display">\[
p^\star = \inf\{f_0(x)~|~ f_i(x)\le 0, ~i=1\dots, m, ~h_i(x) = 0,
~i=1\dots p\}
\]</span></p>
<p>当 <span class="math inline">\(p^\star = \infty\)</span>（<span
class="math inline">\(\inf \emptyset =
\infty\)</span>）时，问题是不可行(infeasible)的；如果有可行解 <span
class="math inline">\(x_k\)</span>，在 <span
class="math inline">\(k\rightarrow \infty\)</span> 时有 <span
class="math inline">\(f_0(x_k) \rightarrow
-\infty\)</span>，就称该问题是无下界的(unbounded below)</p>
<h5 id="局部最优与全局最优">局部最优与全局最优</h5>
<p><span class="math inline">\(x^\star\)</span> 是最优解(optimal
point)，如果 <span class="math inline">\(x\star\)</span> 是可行的且
<span class="math inline">\(f(x^\star) =
p^\star\)</span>，最优解的集合称为最优集(optimal set)；</p>
<p>如果最优集非空，就称最优值是attained或者achieved，问题就是可解的(solvable)；</p>
<p>可行解 <span class="math inline">\(x\)</span> 满足 <span
class="math inline">\(f_0(x) \le p^\star + \epsilon\)</span>，则称其为
<span class="math inline">\(\epsilon\)</span>-suboptimal 的；</p>
<p>存在 <span class="math inline">\(R&gt;0\)</span>，有局部最优解 <span
class="math inline">\(x\)</span>，满足如下最优化问题（引入了变量<span
class="math inline">\(z\)</span>）: <span class="math display">\[
\begin{array}{lll}
  \text{minimize} &amp;f_0(z)\\
  \text{subject to} &amp;f_i(z)\le 0, &amp;i=1, \dots, m\\
  &amp;h_i(z) = 0, &amp;i=1,\dots, p\\
  &amp;\Vert z-x\Vert _2 \le R
\end{array}
\]</span></p>
<p>如果 <span class="math inline">\(x\)</span> 是可行的，且 <span
class="math inline">\(f_i(x) = 0\)</span>，则称第 <span
class="math inline">\(i\)</span> 个不等式限制在<span
class="math inline">\(x\)</span>处是active的，否则是inactive的；</p>
<h5 id="可行问题">可行问题</h5>
<p>只是找到是否有满足限制条件的解： <span class="math display">\[
\begin{array}{lll}
  \text{find} &amp;x\\
  \text{subject to} &amp;f_i(x)\le 0, &amp;i=1,\dots, m\\
  &amp;h_i(x) = 0, &amp;i=1,\dots, p\\
\end{array}
\]</span></p>
<h4 id="等价问题">等价问题</h4>
<p>什么是等价问题？简单地描述就是两个问题的最优集是相同的；注意，两个问题的最优值并不要求一样；</p>
<h5 id="变量代换">变量代换</h5>
<p>假设 <span
class="math inline">\(\phi:\mathbb{R}^n\rightarrow\mathbb{R}^n\)</span>
是一对一函数，且 <span
class="math inline">\(\phi(\operatorname{dom}\phi) \supseteq
\mathcal{D}\)</span> ，定义： <span class="math display">\[
\begin{array}{ll}
  \tilde{f}_i(z) = f_i(\phi(z)), &amp;i = 0, \dots, m\\
  \tilde{h}_i(z) = h_i(\phi(z)), &amp;i = 1, \dots, p\\
\end{array}
\]</span></p>
<p>那么对原来的问题使用变量代换 <span class="math inline">\(x =
\phi(z)\)</span>，得到：</p>
<p><span class="math display">\[
\begin{array}{lll}
  \text{minimize} &amp;\tilde f_0(z)\\
  \text{subject to} &amp;\tilde f_i(z)\le 0, &amp;i=1\dots, m\\
  &amp;\tilde h_i(z) = 0, &amp;i=1,\dots, p\\
\end{array}
\]</span></p>
<p>该问题与标准形式是等价的；</p>
<h5 id="函数变换">函数变换</h5>
<p>假设 <span class="math inline">\(\psi_0:
\mathbb{R}\rightarrow\mathbb{R}\)</span> 是单调递增的，<span
class="math inline">\(\psi_1, \dots, \psi_m:
\mathbb{R}\rightarrow\mathbb{R}\)</span> 满足 <span
class="math inline">\(u\le 0 \iff \psi_i(u) \le 0\)</span>，<span
class="math inline">\(\psi_{m+1}, \dots, \psi_{m+p}:
\mathbb{R}\rightarrow\mathbb{R}\)</span> 满足 <span
class="math inline">\(u = 0 \iff \psi_{u} = 0\)</span>，并定义： <span
class="math display">\[
\begin{array}{ll}
  \tilde{f}_i(x) = \psi_{i}(f_i(x)), &amp;i = 0, \dots, m\\
  \tilde{h}_i(x) = \psi_{m+i}(h_i(x)), &amp;i = 1, \dots, p\\
\end{array}
\]</span></p>
<p>则有等价的问题： <span class="math display">\[
\begin{array}{lll}
  \text{minimize} &amp;\tilde f_0(x)\\
  \text{subject to} &amp;\tilde f_i(x)\le 0, &amp;i=1\dots, m\\
  &amp;\tilde h_i(x) = 0, &amp;i=1,\dots, p\\
\end{array}
\]</span></p>
<h5 id="松弛变量slack-variables">松弛变量(slack variables)</h5>
<p>为不等式限制引入松弛变量 <span class="math inline">\(s_i \in
\mathbb{R}^m, ~i=1,\dots, m\)</span>： <span class="math display">\[
\begin{array}{lll}
  \text{minimize} &amp;f_0(x)\\
  \text{subject to} &amp;s_i \ge 0, &amp;i=1\dots, m\\
  &amp;f_i(x) + s_i =  0, &amp;i=1\dots, m\\
  &amp;h_i(x) = 0, &amp;i=1,\dots, p\\
\end{array}
\]</span></p>
<h5 id="消除等式限制">消除等式限制</h5>
<p>如果我们可以引入新的参数 <span class="math inline">\(z\in
\mathbb{R}^k\)</span>，表示出等式限制 <span class="math inline">\(h_i(x)
= 0\)</span> 的通解<span class="math inline">\(x =
\phi(z)\)</span>，得到等价的优化问题： <span class="math display">\[
\begin{array}{lll}
  \text{minimize} &amp;\tilde f_0(z) = f_0(\phi(z))\\
  \text{subject to} &amp;\tilde f_i(z) = f_i(\phi(z)) \le 0,
&amp;i=1\dots, m\\
\end{array}
\]</span></p>
<p>特殊地，当等式限制函数全部为线性函数时（可以表示为线性方程组<span
class="math inline">\(Ax=b\)</span>），我们可以表示线性方程组的通解（假设有解）：<span
class="math inline">\(\phi(z) = Fz + x_0\)</span>，其中<span
class="math inline">\(F\in \mathbb{R}^{n\times k}\)</span>满足<span
class="math inline">\(\mathcal{R}(F) =
\mathcal{N}(A)\)</span>（F的解空间等于A的零空间），<span
class="math inline">\(x_0\)</span> 是一个特解；</p>
<h5 id="增加等式限制">增加等式限制</h5>
<p>假设优化问题可以写成如下形式： <span class="math display">\[
\begin{array}{lll}
  \text{minimize} &amp;f_0(A_0x+b_0)\\
  \text{subject to} &amp;f_i(A_ix+b_i)\le 0, &amp;i=1,\dots, m\\
  &amp;h_i(x) = 0, &amp;i=1,\dots, p\\
\end{array}
\]</span></p>
<p>其中，<span class="math inline">\(x \in \mathbb{R}^n\)</span>，<span
class="math inline">\(A_i \in \mathbb{R}^{k_i\times n}\)</span>，<span
class="math inline">\(f_i :
\mathbb{R}^{k_i}\rightarrow\mathbb{R}\)</span>；</p>
<p>我们引入新的变量 <span class="math inline">\(y_i \in
\mathbb{R}^{k_i}\)</span>，有 <span class="math inline">\(y_i =
A_ix+b_i\)</span>，得到新的问题： <span class="math display">\[
\begin{array}{lll}
  \text{minimize} &amp;f_0(y_0)\\
  \text{subject to} &amp;f_i(y_i)\le 0, &amp;i=1,\dots, m\\
  &amp;y_i=A_ix+b_i, &amp;i=0,\dots, m\\
  &amp;h_i(x) = 0, &amp;i=1,\dots, p\\
\end{array}
\]</span></p>
<h5 id="优化部分变量">优化部分变量</h5>
<p>我们总是有： <span class="math display">\[
\inf_{x, y}f(x, y) = \inf_{x} \tilde{f}(x)
\]</span> 其中，<span class="math inline">\(\tilde{f}(x) = \inf_{y} f(x,
y)\)</span>；</p>
<blockquote>
<p>笔者的疑问，这条定理是对任何多元函数都成立的吗？</p>
</blockquote>
<p>假设，<span class="math inline">\(x=(x_1,
x_2)\)</span>，且问题形式如下（这显然与标准形式的问题并不等价）： <span
class="math display">\[
\begin{array}{lll}
  \text{minimize} &amp;f_0(x_1, x_2)\\
  \text{subject to} &amp;f_i(x_1)\le 0, &amp;i=1, \dots, m_1\\
  &amp;\tilde f_i(x_2)\le 0, &amp;i=1, \dots, m_2\\
\end{array}
\]</span></p>
<p>定义关于<span class="math inline">\(x_1\)</span>的函数<span
class="math inline">\(\tilde{f}_0\)</span>: <span
class="math display">\[
\tilde{f}_0(x_1) = \inf\{f_0(x_1, z) ~|~ \tilde f_i(z) \le 0\}
\]</span> 那么上述问题等价于： <span class="math display">\[
\begin{array}{lll}
  \text{minimize} &amp;\tilde f_0(x_1)\\
  \text{subject to} &amp;f_i(x_1)\le 0, &amp;i=1, \dots, m_1\\
\end{array}
\]</span></p>
<h5 id="epigraph形式">epigraph形式</h5>
<p>找到epigraph中最低的点<span class="math inline">\((x, t)\)</span>
<span class="math display">\[
\begin{array}{lll}
  \text{minimize} &amp;t\\
  \text{subject to} &amp;f_0(x) - t \le 0\\
  &amp;f_i(x)\le 0, &amp;i=1, \dots, m\\
  &amp;h_i(x) = 0, &amp;i=1,\dots, p\\
\end{array}
\]</span></p>
<h3 id="凸优化">4.2 凸优化</h3>
<p>凸优化问题的标准形式为： <span class="math display">\[
\begin{array}{lll}
  \text{minimize} &amp;f_0(x)\\
  \text{subject to} &amp;f_i(x)\le 0, &amp;i=1, \dots, m\\
  &amp;a^T_ix=b_i, &amp;i=1,\dots, p\\
\end{array}
\]</span></p>
<p>其中，<span class="math inline">\(f_0, \dots,
f_m\)</span>都是凸函数；</p>
<p>存在一些问题，他们的形式上不满足凸优化问题，但是可以转化为凸优化问题，一些作者将其称为抽象凸优化问题；</p>
<p>对于凸优化问题，局部最优等价于全局最优；</p>
<details>
<summary>
证明
</summary>
<p>设<span
class="math inline">\(x\)</span>是凸优化问题的局部最优解，即设某个<span
class="math inline">\(R&gt;0\)</span>有： <span class="math display">\[
f_0(x) = \inf\{f_0(z) ~|~ z ~\text{feasible}, ~\Vert z-x\Vert _2\le R \}
\]</span></p>
<p>假设<span
class="math inline">\(x\)</span>不是全局最优解，即存在可行解<span
class="math inline">\(y\)</span>使得<span
class="math inline">\(f_0(y)&lt;f_0(x)\)</span>，显然，有<span
class="math inline">\(\Vert y-x\Vert _2&gt;R\)</span>； 考虑点<span
class="math inline">\(z\)</span>满足： <span class="math display">\[
\begin{array}{ll}
  z = (1-\theta) x + \theta y, &amp;\theta = \frac{R}{2\Vert y-x\Vert
_2}
\end{array}
\]</span> 那么，<span class="math inline">\(\Vert z-x\Vert _2 = R/2 &lt;
R\)</span>，且可行集是凸集，则<span
class="math inline">\(z\)</span>也是可行的，那么就有<span
class="math inline">\(f_0(z) \ge f_0(x)\)</span>；</p>
<p>另一方面： <span class="math display">\[
f_0(z) \le (1-\theta)f_0(x) + \theta f_0(y) \le f_0(x)
\]</span></p>
<p>产生矛盾；</p>
</details>
<h4 id="可导函数f_0的最优性准则an-optimality-criterion">可导函数<span
class="math inline">\(f_0\)</span>的最优性准则(An optimality
criterion)</h4>
<p>假设<span
class="math inline">\(f_0\)</span>是凸优化问题的目标函数，根据一阶导条件，对于任意的<span
class="math inline">\(x, y \in \operatorname{dom} f\)</span>： <span
class="math display">\[
f_0(y) \ge f_0(x) + \nabla f_0(x)^T(y - x)
\]</span></p>
<p>假设<span class="math inline">\(X\)</span>是可行集，有：<span
class="math inline">\(x\)</span>是最优解<strong>当且仅当</strong> <span
class="math display">\[
\forall y \in X, ~\nabla f_0(x)^T(y - x) \ge 0
\]</span> 也就是当 <span class="math inline">\(\nabla f_0(x) \ne
0\)</span> 时，<span class="math inline">\(-\nabla f_0(x)\)</span>
是<span class="math inline">\(X\)</span>的支持平面的外法向量；</p>
<h5 id="无限制问题">无限制问题</h5>
<p>如果该最优化问题是unconstrained，那么上述最优性准则退化为我们熟知的：<span
class="math inline">\(x\)</span>是最优解<strong>当且仅当</strong> <span
class="math display">\[
\nabla f_0(x) = 0, ~x\in \operatorname{dom} f
\]</span></p>
<h5 id="仅包含等式限制">仅包含等式限制</h5>
<p><span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp;f_0(x)\\
  \text{subject to} &amp;Ax = b
\end{array}
\]</span></p>
<p>假设可行集<span class="math inline">\(X\)</span>非空，对于最优解<span
class="math inline">\(x\)</span>，满足上述的最优性准则：对于任意满足<span
class="math inline">\(Ay=b\)</span>的<span class="math inline">\(y\in
X\)</span>，有 <span class="math display">\[
\nabla f(x)^T(y-x)\ge 0
\]</span></p>
<p>因为，<span class="math inline">\(x\in X\)</span>，也同样有<span
class="math inline">\(Ax=b\)</span>，<span
class="math inline">\(y\)</span>和<span
class="math inline">\(x\)</span>都是<span
class="math inline">\(Ax=b\)</span>的解，则令<span
class="math inline">\(y-x = v\)</span>，因为<span
class="math inline">\(Av = A(y-x) = Ay - Ax = 0\)</span>，有<span
class="math inline">\(v \in \mathcal{N}(A)\)</span>；</p>
<p>那么上述准则可以写成： <span class="math display">\[
\forall v \in \mathcal{N}(A), ~\nabla f(x)^Tv\ge 0
\]</span></p>
<p>对于subspace，如果有 <span class="math inline">\(v \in
\mathcal{N}(A)\)</span>，则一定有 <span class="math inline">\(-v \in
\mathcal{N}(A)\)</span>，那么可得： <span class="math display">\[
\forall v \in \mathcal{N}(A), ~\nabla f(x)^Tv = 0
\]</span> 即 <span class="math inline">\(\nabla f(x)^T \bot
\mathcal{N}(A)\)</span>；</p>
<p>又因为 <span class="math inline">\(\mathcal{N}(A)^\bot =
\mathcal{R}(A^T)\)</span>，因此有 <span class="math inline">\(\nabla
f(x)^T \in \mathcal{R}(A^T)\)</span>，可以写成：存在 <span
class="math inline">\(\nu \in \mathbb{R}^p\)</span>，使得 <span
class="math display">\[
\nabla f(x)^T + A^Tv = 0
\]</span></p>
<p>第5章学习如何进而求解<span class="math inline">\(x\)</span>；</p>
<h5 id="在非负象限上最小化">在非负象限上最小化</h5>
<p><span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp;f_0(x)\\
  \text{subject to} &amp;x\succeq 0
\end{array}
\]</span></p>
<p>使用先前的准则，<span class="math inline">\(x\succeq 0\)</span>
是最优解，等价于 <span class="math display">\[
\forall y \succeq 0, ~\nabla f(x)^T(y-x)\ge 0
\]</span></p>
<p>如果 <span class="math inline">\(\nabla f(x)^T \prec 0\)</span>，那么
<span class="math inline">\(\nabla f(x)^Ty\)</span> 在 <span
class="math inline">\(y\succeq 0\)</span> 是unbounded
below的，上述条件也不可能成立，因此 <span class="math inline">\(\nabla
f(x)^T \succeq 0\)</span>；</p>
<p>进而，为了使得上述条件成立，需在 <span
class="math inline">\(y=0\)</span> 时成立，即 <span
class="math inline">\(-\nabla f(x)^Tx\succeq 0\)</span>，而 <span
class="math inline">\(x\succeq 0\)</span>，<span
class="math inline">\(\nabla f(x)^Tx \succeq 0\)</span>，则只能是 <span
class="math inline">\(\nabla f(x)^Tx = 0\)</span>，展开写成： <span
class="math display">\[
\sum_{i=1}^n(\nabla f(x)^T)_ix_i = 0
\]</span> 由于该和式的每一项都非正，因而只能是每一项都为0： <span
class="math display">\[
(\nabla f(x)^T)_ix_i = 0, ~i = 1, \dots, n, ~ x\succeq 0, ~\nabla f(x)^T
\succeq 0
\]</span></p>
<p>这个条件称为互补性，即最优解<span
class="math inline">\(x\)</span>非零元素的索引与目标函数在<span
class="math inline">\(x\)</span>处导数的非零元素的索引是互补的；</p>
<h4 id="等价的凸优化问题">等价的凸优化问题</h4>
<p>在先前等价的优化问题中，哪些方式能够保持凸性？</p>
<h5 id="消除引入等式限制">消除/引入等式限制</h5>
<p>多数情况下，消除等式限制让问题更难分析，并且降低算法的效率；例如<span
class="math inline">\(x\)</span>是非常高维的，而消除等式限制破坏了问题的稀疏性(sparsity)；</p>
<blockquote>
<p>笔者至此还不能理解，为什么稀疏性是问题的一个好的性质？</p>
</blockquote>
<h5 id="引入松弛变量">引入松弛变量</h5>
<h5 id="转化为epigraph形式的问题">转化为epigraph形式的问题</h5>
<blockquote>
<p>线性目标函数有很多好处之后会体会到</p>
</blockquote>
<h5 id="在部分变量上最小化">在部分变量上最小化</h5>
<h4 id="quasiconvex-优化">Quasiconvex 优化</h4>
<p>形式上，quasiconvex优化问题凸凸优化问题相同，只是要求目标函数<span
class="math inline">\(f_0\)</span>和不等式限制函数<span
class="math inline">\(f_1, \dots, f_m\)</span>为quasiconvex即可；</p>
<h5 id="局部最优解和最优性条件">局部最优解和最优性条件</h5>
<p>quasiconvex优化问题的局部最优解不等价于全局最优解；记<span
class="math inline">\(X\)</span>为可行集，如果满足： <span
class="math display">\[
\begin{array}{ll}
  x \in X, &amp;\nabla f_0(x)^T(y - x) &gt; 0, ~\forall y \in X
\backslash\{x\}
\end{array}
\]</span></p>
<p>注意： - 这只是最优解的充分条件 - 该条件中，<span
class="math inline">\(\nabla f_0(x) \ne 0\)</span></p>
<h5 id="通过凸可行性问题求解">通过凸可行性问题求解</h5>
<p>前文讲过quasiconvex函数的凸函数族表示，设原问题的最优值为<span
class="math inline">\(p^\star\)</span>，对于如下凸可行性问题：</p>
<p><span class="math display">\[
\begin{array}{lll}
  \text{find} &amp;x\\
  \text{subject to} &amp;\phi_t(x)\le 0\\
  &amp;f_i(x)\le 0, &amp;i=1, \dots, m\\
  &amp;Ax=b
\end{array}
\]</span></p>
<p>如果是可行的，说明<span
class="math inline">\(t\)</span>还有下降空间，即<span
class="math inline">\(p^\star \le
t\)</span>，反之，如果不可行，说明<span class="math inline">\(p^\star
\ge t\)</span>；因此，我们可以通过不断缩小<span
class="math inline">\(t\)</span>并反复检查对应的凸可行性问题是否可行，进而不断逼近<span
class="math inline">\(p^\star\)</span></p>
<p>因而诞生了一种二分法(bisection)来找到<span
class="math inline">\(p^\star\)</span>：先假设<span
class="math inline">\(p^\star \in [l, u]\)</span>，之后令<span
class="math inline">\(t =
\frac{l+u}{2}\)</span>，判断对应的凸可行问题是否可行，若可行则<span
class="math inline">\(p^\star\)</span>在左半区间，否则在右半区间；如此递归下去；
（伪代码见教材P146 Algorithm 4.1）</p>
<h3 id="线性优化问题">4.3 线性优化问题</h3>
<p>一般的<strong>线性规划问题(linear program, LP)</strong> 有如下形式：
<span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp;c^Tx+d\\
  \text{subject to} &amp;Gx\preceq h\\
  &amp;Ax=b
\end{array}
\]</span></p>
<blockquote>
<p>目标函数是否加常数 <span
class="math inline">\(d\)</span>，不影响最优解；</p>
</blockquote>
<p>LP的以下两种特殊情况很常见：</p>
<p>standard form LP: <span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp;c^Tx\\
  \text{subject to} &amp;Ax=b\\
  &amp;x\succeq 0
\end{array}
\]</span></p>
<p>inequality form LP: <span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp;c^Tx\\
  \text{subject to} &amp;Ax\preceq b\\
\end{array}
\]</span></p>
<p>一般形式的LP可以转换成标准LP： <span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp;c^Tx^+-c^Tx^-+d\\
  \text{subject to} &amp;Gx^+-Gx^- + s = h\\
  &amp;Ax^+ - Ax^-=b\\
  &amp; s\succeq 0, ~x^+ \succeq 0, ~x^-\succeq 0
\end{array}
\]</span></p>
<p>其中，可以有 <span class="math inline">\(x^+_i = \min\{0,
x_i\}\)</span>，<span class="math inline">\(x^-_i = \min\{0,
-x_i\}\)</span>；</p>
<h4 id="例子">例子</h4>
<blockquote>
<p>完整的例子见教材，笔者只记录一部分觉得有意思的，且与经济学背景无关的；</p>
</blockquote>
<h5
id="多面体的车比雪夫中心chebyshev-center">多面体的车比雪夫中心(Chebyshev
center)</h5>
<p>找到多面体 <span class="math inline">\(\mathcal{P}\)</span>
中最大的欧几里得球 <span
class="math inline">\(\mathcal{B}\)</span>，球心即该多面体的车比雪夫中心，其中：
<span class="math display">\[
\mathcal{P} = \{x\in\mathbb{R}^n ~|~ a^T_ix \le b_i, i = 1,\dots, m\}
\]</span> <span class="math display">\[
\mathcal{B} = \{x_c + u ~|~ \Vert u\Vert _2 \le r\}
\]</span></p>
<p>问题即在 <span class="math inline">\(\mathcal{B} \subseteq
\mathcal{P}\)</span> 的条件下，找到最大的 <span
class="math inline">\(r\)</span>；</p>
<p>首先，让 <span class="math inline">\(\mathcal{B}\)</span>
在多面体的一个超平面的内侧，即对于 <span
class="math inline">\(u\)</span> 满足 <span class="math inline">\(\Vert
u\Vert _2 \le r\)</span>，有 <span class="math display">\[
a^T_i (x_c + u) \le b_i
\]</span></p>
<p>又因为： <span class="math display">\[
\sup\{a^T_iu ~|~ \Vert u\Vert _2\le r\} = r\Vert a_i\Vert _2
\]</span></p>
<p>那么： <span class="math display">\[
a^T_i x_c + r\Vert a_i\Vert _2 \le b_i
\]</span></p>
<p>问题的形式变为： <span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp;r\\
  \text{subject to} &amp;a^T_i x_c + r\Vert a_i\Vert _2 \le b_i,
&amp;i=1,\dots, m
\end{array}
\]</span> 其中，<span class="math inline">\(r\)</span> 和 <span
class="math inline">\(x_c\)</span> 是变量；</p>
<h5 id="车比雪夫不等式">车比雪夫不等式</h5>
<p>假设 <span class="math inline">\(x\)</span> 是集合 <span
class="math inline">\(\{x_1, \dots, u_n\}\)</span>
上的离散随机变量，使用向量 <span class="math inline">\(p \in
\mathbb{R}^n\)</span> 表示 <span class="math inline">\(x\)</span>
的概率密度，满足 <span class="math inline">\(p\succeq 0\)</span> 且
<span class="math inline">\(\mathbf 1^Tp = 1\)</span>： <span
class="math display">\[
p_i = \mathbf{prob}(x=u_i)
\]</span></p>
<p>假设 <span class="math inline">\(u_i\)</span> 是已知的但是 <span
class="math inline">\(p\)</span> 是未知的；设我们事先可以知道关于 <span
class="math inline">\(x\)</span> 的某些函数 <span
class="math inline">\(f_i\)</span> 的期望的上下界，求出目标函数 <span
class="math inline">\(f_0(x)\)</span> 的期望的上下界是一个LP； <span
class="math display">\[
\mathbf{E} f_i = \sum_{j=1}^n p_j f_i(u_j) = a^T_ip
\]</span></p>
<p>问题表示为： <span class="math display">\[
\begin{array}{lll}
  \text{minimize} &amp;a_0^Tp\\
  \text{subject to} &amp;p\succeq 0, ~\mathbf{1}^Tp = 1\\
  &amp; \alpha_i \le a^T_ip \le \beta_i, &amp; i = 1, \dots, m\\
\end{array}
\]</span></p>
<h4 id="线性分数规划">线性分数规划</h4>
<p>详见教材P151-152</p>
<h3 id="二次规划问题">4.4 二次规划问题</h3>
<p>（凸）<strong>二次规划问题(quadratic program, QP)</strong>
有如下形式：</p>
<p><span class="math display">\[
\begin{array}{lll}
  \text{minimize} &amp;(1/2)x^TPx + q^Tx + r\\
  \text{subject to} &amp;Gx \preceq h\\
  &amp;Ax=b
\end{array}
\]</span></p>
<p>其中，<span class="math inline">\(P\in \mathbf{S}^n_+\)</span>，<span
class="math inline">\(G\in \mathbb{R}^{m\times n}\)</span>，<span
class="math inline">\(A\in \mathbb{R}^{p\times n}\)</span></p>
<p><strong>二次限制的二次规划问题(quadratically constrained quadratic
program, QCQP)</strong> 形式如下： <span class="math display">\[
\begin{array}{lll}
  \text{minimize} &amp;(1/2)x^TP_0x + q_0^Tx + r_0\\
  \text{subject to} &amp;(1/2)x^TP_ix + q_i^Tx + r_i \le 0, &amp;i =
1,\dots, m\\
  &amp;Ax=b
\end{array}
\]</span></p>
<h4 id="例子-1">例子</h4>
<h5 id="最小二乘回归">最小二乘/回归</h5>
<p>无限制的最小二乘函数： <span class="math display">\[
\Vert Ax-b\Vert _2^2 = x^TA^TAx - 2b^TAx + b^Tb
\]</span></p>
<p>该问题的解析解是 <span class="math inline">\(x = A^+b\)</span></p>
<p>如果对 <span class="math inline">\(x\)</span>
添加了简单的上下界限制，则没有解析解： <span class="math display">\[
\begin{array}{lll}
  \text{minimize} &amp;\Vert Ax-b\Vert _2^2\\
  \text{subject to} &amp;l_i \le x_i \le u_i, &amp;i = 1,\dots, m\\
\end{array}
\]</span></p>
<h5 id="方差的上界">方差的上界</h5>
<p>与前文中车比雪夫不等式的先验知识，我们希望知道随机变量 <span
class="math inline">\(f(x)\)</span> 的方差的界，因为 <span
class="math display">\[
\mathbf{Var} f = \mathbf{E} f^2 - (\mathbf{E}f)^2 = \sum_{i=1}^n
f_i^2p_i - \left(\sum_{i=1}^n f_ip_i\right)^2
\]</span></p>
<p>显然 <span class="math inline">\(\mathbf{Var} f\)</span> 是关于 <span
class="math inline">\(p\)</span> 的凹二次函数，我们可以求方差的上界：
<span class="math display">\[
\begin{array}{lll}
  \text{maximize} &amp;\sum_{i=1}^n f_i^2p_i - \left(\sum_{i=1}^n
f_ip_i\right)^2\\
  \text{subject to} &amp;p\succeq 0, ~\mathbf{1}^Tp = 1\\
  &amp; \alpha_i \le a^T_ip \le \beta_i, &amp; i = 1, \dots, m\\
\end{array}
\]</span></p>
<h5 id="随机代价的线性规划">随机代价的线性规划</h5>
<p>考虑一般的线性规划 <span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp;c^Tx\\
  \text{subject to} &amp;Gx\preceq h\\
  &amp;Ax=b
\end{array}
\]</span></p>
<p>除了变量 <span class="math inline">\(x\in \mathbb{R}^n\)</span>
之外，假设 <span class="math inline">\(c\in \mathbb{R}^n\)</span>
是随机变量，<span class="math inline">\(\overline{c}\)</span>
是均值，<span class="math inline">\(\Sigma\)</span> 是协方差矩阵；</p>
<p>给定 <span class="math inline">\(x\in \mathbb{R}^n\)</span>，那么
<span class="math inline">\(\mathbf{E}(c^Tx) =
\overline{c}^Tx\)</span>， <span class="math display">\[
\mathbf{var}(c^Tx) = \mathbf{E}(c^Tx - \mathbf{E}c^Tx)^2 = x^T\Sigma x
\]</span></p>
<p>通常需要在低均值和低方差间做出妥协，引入 <span
class="math inline">\(\gamma \ge 0\)</span> 作为 risk-aversion 参数：
<span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp;\overline c^Tx + \gamma x^T\Sigma x\\
  \text{subject to} &amp;Gx\preceq h\\
  &amp;Ax=b
\end{array}
\]</span></p>
<h4 id="二次锥规划">二次锥规划</h4>
<p>二次锥规划(second-order cone program, SOCP)： <span
class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp;f^Tx\\
  \text{subject to} &amp;\Vert A_ix+b_i\Vert _2 \le c^T_ix+d_i &amp;i, =
1, \dots, m\\
  &amp;Fx=g
\end{array}
\]</span></p>
<p>如果 <span class="math inline">\(c_i = 0, ~i=1, \dots,
m\)</span>，那么SOCP退化为QCQP问题；</p>
<h5 id="鲁棒的线性规划">鲁棒的线性规划</h5>
<p>鲁棒线性规划(robust linear porgram)问题： <span
class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp;c^Tx\\
  \text{subject to} &amp;a_ix \le b_i ~\text{for all} ~a_i\in
\mathcal{E_i}, &amp;i=1,\dots, m \\
\end{array}
\]</span></p>
<p>其中： <span class="math display">\[
\mathcal{E}_i = \{\overline{a_i} + P_i u ~|~ \Vert u\Vert _2 \le 1\}
\]</span></p>
<p>想法还是通过让： <span class="math display">\[
\sup\{a^T_ix ~|~ a_i \in \mathcal{E_i}\} \le b_i
\]</span> 即 <span class="math display">\[
\overline{a}^T_ix + \Vert P^T_ix\Vert _2 \le b_i
\]</span></p>
<p>那么问题转化为SOCP的形式： <span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp;c^Tx\\
  \text{subject to} &amp;\overline{a}^T_ix + \Vert P^T_ix\Vert _2 \le
b_i, &amp;i=1,\dots, m \\
\end{array}
\]</span></p>
<h5 id="随机限制的线性规划">随机限制的线性规划</h5>
<p><span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp;c^Tx\\
  \text{subject to} &amp;\mathbf{prob}(a^T_ix\le b_i) \ge \eta
&amp;i=1,\dots, m \\
\end{array}
\]</span></p>
<p>其中，<span class="math inline">\(\eta \ge 1/2\)</span>，<span
class="math inline">\(a_i \sim \mathcal{N}(\overline{a}_i,
\Sigma_i)\)</span>；</p>
<p>记 <span class="math inline">\(u = a^T_ix \sim \mathcal{N}(\overline
u, \sigma_i)\)</span>，那么不等式限制可以写为： <span
class="math display">\[
\mathbf{prob}(\frac{u-\overline{u}}{\sigma} \le
\frac{b-\overline{u}}{\sigma}) \ge \eta
\]</span></p>
<p>记 <span class="math inline">\(\Phi\)</span>
是标准正态分布的CDF，那么有</p>
<p><span class="math display">\[
\frac{b-\overline{u}}{\sigma} \ge \Phi^{-1}(\eta)
\]</span></p>
<p>将 <span class="math inline">\(\overline{u} =
\overline{a}^T_ix\)</span>，<span class="math inline">\(\sigma =
(x^T\Sigma_i x)^{1/2} = \Vert \Sigma_i^{1/2} x\Vert _2\)</span>
代入，得到SOCP形式的优化问题，(因为 <span class="math inline">\(\eta\ge
1/2\)</span>，有 <span class="math inline">\(\Phi^{-1}(\eta)\ge
0\)</span>，不等式限制函数是凸的)：</p>
<p><span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp;c^Tx\\
  \text{subject to} &amp;\overline{a}^T_ix + \Phi^{-1}(\eta)\Vert
\Sigma_i^{1/2} x\Vert _2 \le b_i &amp;i=1,\dots, m \\
\end{array}
\]</span></p>
<h3 id="几何规划问题">4.5 几何规划问题</h3>
<h4
id="单项式monomials和正向式posynomials">单项式(monomials)和正向式(posynomials)</h4>
<p><strong>单项式(monomial)</strong> 是函数 <span
class="math inline">\(f:\mathbb{R}^n\rightarrow
\mathbb{R}\)</span>，定义域为 <span
class="math inline">\(\operatorname{dom}f = \mathbb{R}^n_{++}\)</span>
定义为： <span class="math display">\[
f(x) = cx_1^{a_1}\dots x_n^{a_n}
\]</span> 其中 <span class="math inline">\(c &gt; 0\)</span>，<span
class="math inline">\(a_i \in \mathbb{R}\)</span>；</p>
<p><strong>正项式(posnomial)</strong> 是单项式的和：$ <span
class="math display">\[
f(x) = \sum_{k=1}^{K} c_k x_1^{a_{1k}}\dots x_n^{a_{nk}}
\]</span></p>
<p>其中 <span class="math inline">\(c_k &gt; 0\)</span>；</p>
<h4 id="几何规划">几何规划</h4>
<p><strong>几何规划(Geometric Problem, GP)</strong>
问题标准形式（正项式形式）如下： <span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp;f_0(x)\\
  \text{subject to} &amp;f_i(x) \le 1, &amp;i = 1, \dots, m\\
  &amp;h_i(x) = 1, &amp;i = 1, \dots, p
\end{array}
\]</span> 其中，<span class="math inline">\(f_0, \dots, f_m\)</span>
是正项式，<span class="math inline">\(h_1, \dots, h_p\)</span>
是单项式；</p>
<p>问题的域是 <span class="math inline">\(\mathcal{D} \in
\mathbb{R}^n_{++}\)</span>；</p>
<h4 id="转化为凸优化形式">转化为凸优化形式</h4>
<p>几何规划通常不是凸优化问题，但是可以通过变量代换，<span
class="math inline">\(y_i = \log
x_i\)</span>，并对问题的目标函数和限制函数两边取 <span
class="math inline">\(\log\)</span>，将问题转化为凸优化问题：</p>
<p><span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp;\tilde f_0(y) = \log\left( \sum_{k=1}^{K_0}
e^{a_{0k}^T y + b_{0k}} \right)\\
  \text{subject to} &amp;\tilde f_i(y) = \log\left( \sum_{k=1}^{K_i}
e^{a_{ik}^T y + b_{ik}} \right) \le 1, &amp;i = 1, \dots, m\\
  &amp;\tilde h_i(y) = g_i^Ty + h_i = 1, &amp;i = 1, \dots, p
\end{array}
\]</span></p>
<p>注意到log-sum-exp
函数是凸函数，上述形式又叫做凸形式的几何规划；因为使用的是 <span
class="math inline">\(\log\)</span>
的变量代换和函数代换，这两种形式的问题是等价的（见前文“等价问题”一节）；</p>
<h4 id="例子-2">例子</h4>
<h5 id="对角缩放的frobenius范数">对角缩放的Frobenius范数</h5>
<p>考虑原来有线性方程：<span class="math inline">\(y =
Mu\)</span>，<span class="math inline">\(M\in \mathbb{R}^{n\times
n}\)</span>，对 <span class="math inline">\(u\)</span> 和 <span
class="math inline">\(y\)</span> 使用相同的等比缩放 <span
class="math inline">\(D\)</span> 之后( <span class="math inline">\(D
=\mathbf{diag}(d), ~d_i \ge 0\)</span> )，得到 <span
class="math inline">\(\tilde{u} = Du, ~\tilde{y} = Dy\)</span>，那么有
<span class="math inline">\(\tilde{y} = DMD^{-1}
\tilde{u}\)</span>；</p>
<p>这里的问题是，最小化矩阵 <span
class="math inline">\(DMD^{-1}\)</span> 的Fronenius范数；</p>
<p><span class="math display">\[
\begin{split}
  \Vert DMD^{-1}\Vert _F^2 &amp;= \mathbf{tr}\left(
(DMD^{-1})^T(DMD^{-1}) \right)\\
  &amp;= \sum_{i,j=1}^{n}(DMD^{-1})^2_{ij}\\
  &amp;= \sum_{i,j=1}^{n}(d_i^2M_{ij}^2)/d_j^2\\
\end{split}
\]</span></p>
<p>这是关于向量 <span class="math inline">\(d\)</span>
的一个正项式；</p>
<h3 id="广义不等式限制">4.6 广义不等式限制</h3>
<p>使用广义不等式限制拓展标准凸优化问题为如下形式： <span
class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp; f_0(x)\\
  \text{subject to} &amp;f_i(x)\preceq_{K_i} 0, &amp;i=1,\dots, m\\
  &amp; Ax=b
\end{array}
\]</span></p>
<p>如下性质仍然成立：</p>
<ul>
<li>可行集，sublevel set，最优集均是凸集；</li>
<li>局部最优即全局最优；</li>
<li>对于可导的 <span
class="math inline">\(f_0\)</span>，最优解化条件仍成立；</li>
</ul>
<h4 id="锥形式问题">锥形式问题</h4>
<p>目标函数、不等式限制函数是线性函数的广义不等式凸优化问题；</p>
<h4 id="半定规划">半定规划</h4>
<p>设 <span class="math inline">\(K\)</span> 即 <span
class="math inline">\(\mathbf{S}_+^k\)</span>，半定规划(semidefinite
program, SDP)的形式如下：</p>
<p><span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp;c^Tx\\
  \text{subject to} &amp;x_1F_1 + \dotsm + x_nF_n + G \preceq 0\\
  &amp;Ax=b   
\end{array}
\]</span></p>
<p>其中， <span class="math inline">\(G, F_1, \dots, F_n \in
\mathbf{S}_+^k\)</span>，<span class="math inline">\(A\in
\mathbb{R}^{p\times n}\)</span>；</p>
<p>若 <span class="math inline">\(G, F_1, \dots, F_n \in
\mathbf{S}_+^k\)</span> 均为对角矩阵，那么该SDP问题退化为LP问题；</p>
<p>类似的，有标准形式的SDP： <span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp;\mathbf{tr}(CX)\\
  \text{subject to} &amp;\mathbf{tr}(A_iX)=b_i, &amp;i=1,\dots, p\\
  &amp;X\succeq 0   
\end{array}
\]</span></p>
<p>有不等式形式的SDP： <span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp;c^Tx\\
  \text{subject to} &amp;x_1A_1 + \dotsm + x_nA_n \preceq B\\  
\end{array}
\]</span></p>
<p>还可以添加更多的LMI（linear matrix
inequality）限制以及线性不等式限制：</p>
<p><span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp;c^Tx\\
  \text{subject to} &amp;F^{(i)}(x) = x_1F_1^{(i)} + \dotsm +
x_nF_n^{(i)} + G^{(i)} \preceq 0, &amp;i=1,\dots, K\\
  &amp;Gx\preceq h, \quad Ax=b   
\end{array}
\]</span></p>
<h4 id="例子-3">例子</h4>
<h5 id="二次锥规划-1">二次锥规划</h5>
<p>前文提到的SOCP问题可以表示为SDP问题：</p>
<p><span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp;c^Tx\\
  \text{subject to} &amp;-(A_ix + b_i, c^T_ix + d_i) \preceq_{K_i} 0
&amp;i=1, \dots, m\\
  &amp;Fx=g
\end{array}
\]</span></p>
<p>其中 <span class="math inline">\(K_i = \{(y, t)\in \mathbb{R}^{n_i+1}
~|~ \Vert y\Vert _2 \le t\}\)</span>；</p>
<h5 id="矩阵范数最小化">矩阵范数最小化</h5>
<p>记 <span class="math inline">\(A(x) = A_0 + x_1A_1 + \dotsm +
x_nAn\)</span>，其中 <span class="math inline">\(A_i\in
\mathbb{R}^{p\times q}\)</span>，考虑如下的无限制问题：</p>
<p><span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp; \lVert A(x) \rVert _2
\end{array}
\]</span></p>
<p>其中，谱范数 <span class="math inline">\(\lVert \rVert _2\)</span>
返回最大特征值，<span class="math inline">\(x\in \mathbb{R}^n\)</span>
是变量；</p>
<p>考虑到 <span class="math inline">\(\lVert A \rVert _2 \le s \iff A^TA
\preceq s^2I\)</span>（<span class="math inline">\(s\ge
0\)</span>），那么上述问题可以表示成一个 <span
class="math inline">\(q\times q\)</span>
的矩阵不等式限制的凸优化问题：</p>
<p><span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp; s\\
  \text{subject to} &amp; A^T(x)A(x) \preceq s^2I
\end{array}
\]</span></p>
<p>我们还可以表示为一个 SDP 问题：</p>
<p><span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp; t\\
  \text{subject to} &amp; \begin{bmatrix}
    tI &amp;A(x) \\ A^T(x) &amp;tI
  \end{bmatrix}\succeq 0
\end{array}
\]</span></p>
<h3 id="向量优化">4.7 向量优化</h3>
<h4 id="基本形式">基本形式</h4>
<p>目标函数 <span class="math inline">\(f_0 : \mathbb{R}^n \rightarrow
\mathbb{R}^q\)</span> 在真锥 <span class="math inline">\(K\in
\mathbb{R}^q\)</span> 下最小化：</p>
<p><span class="math display">\[
\begin{array}{ll}
  \text{minimize (with respect to K)} &amp; f_0(x)\\
  \text{subject to} &amp; f_i(x)\le 0 &amp; i = 1, \dots, m\\
  &amp; h_i(x) = 0 &amp; i = 1, \dots, p
\end{array}
\]</span></p>
<p>如果 <span class="math inline">\(f_0\)</span>
是K-convex的，不等式限制函数是凸的，等式限制函数是仿射的，那么就称为凸向量优化；</p>
<h4 id="最优值与最优解">最优值与最优解</h4>
<p>记可达目标值集合为： <span class="math display">\[
O = \{f_0(x) ~|~ \exists x\in \mathcal{D}, f_i(x)\le 0, i = 1, \dots, m,
~ h_i(x) = 0, i = 1, \dots, p\}
\]</span></p>
<p>点 <span class="math inline">\(x^\star\)</span>
是最优解，当且仅当其是可行的，且 <span class="math inline">\(O\)</span>
中的值都大于等于 <span class="math inline">\(f(x^\star)\)</span>：</p>
<p><span class="math display">\[
O\subseteq f(x^\star) + K
\]</span></p>
<h4 id="pareto-最优">Pareto 最优</h4>
<p>如果 <span class="math inline">\(f_0(x)\)</span>
是可达目标值集合的极小值，那么称 <span class="math inline">\(x\)</span>
和 <span class="math inline">\(f_0(x)\)</span>
为问题的Pareto最优解和Pareto最优值，公式化的： <span
class="math display">\[
(f_0(x) - K) \cap O = \{f_0(x)\}
\]</span></p>
<h4 id="标量化scalaization">标量化(Scalaization)</h4>
<p>利用广义不等式的对偶性，我们可以选择任何一个 <span
class="math inline">\(\lambda \succ_{K*}
0\)</span>，与目标函数点积，得到一般的标量优化问题：</p>
<p><span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp; \lambda^T f_0(x) \\
  \text{subject to} &amp; f_i(x)\le 0, &amp; i = 1, \dots, m \\
  &amp; h_i(x) = 0, &amp; i = 1, \dots, p
\end{array}
\]</span></p>
<p>该问题的最优解就是原问题的一个Pareto 最优解；</p>
<p>对于向量优化问题，枚举所有的 <span class="math inline">\(\lambda
\succ_{K*} 0\)</span> 不能得到所有的
Pareto最优解，但是对于凸向量优化问题，可以对于每一个Pareto最优解，都找到一个
<span class="math inline">\(\lambda \succeq_{k*} 0\)</span>；</p>
<h4 id="多目标优化">多目标优化</h4>
<p>如果在向量优化问题中，有 <span class="math inline">\(K =
\mathbb{R}^q_+\)</span>，目标函数 <span
class="math inline">\(f_0\)</span> 可以拆成一系列标量函数 <span
class="math inline">\(F_1, \dots,
F_q\)</span>，这样的问题又称为。<strong>多目标优化</strong>(<em>multicriterion</em>
or <em>muti-objective</em> opimization problem) ；</p>
<h5 id="trade-off-分析">trade off 分析</h5>
<p>对于多目标优化的多个Pareto最优解，我们常常需要进行trade off；</p>
<p>对多目标优化进行标量化就得到了我们常见的多目标加权和的形式，<span
class="math inline">\(\lambda_i \ge 0\)</span> 就是 <span
class="math inline">\(F_i\)</span> 的权重；</p>
<h2 id="第5章-对偶性">第5章 对偶性</h2>
<h3 id="拉格朗日对偶函数">5.1 拉格朗日对偶函数</h3>
<h4 id="the-lagrange">The Lagrange</h4>
<p>对于标准形式的优化问题： <span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp; f_0(x) \\
  \text{subject to} &amp; f_i(x) \le 0, &amp; i = 1, \dots, m \\
  &amp; h_i(x) = 0, &amp; i = 1, \dots, p
\end{array}
\]</span></p>
<p>其定义域 <span class="math inline">\(\mathcal{D} = \bigcap_{i=0}^m
\operatorname{dom} f_i \cap \bigcap_{i=1}^p \operatorname{dom}
h_i\)</span> 非空；最优值记为 <span
class="math inline">\(p^\star\)</span>；</p>
<p>定义 the Lagrangian 为函数 <span class="math inline">\(L:
\mathbb{R}^n \times \mathbb{R}^m \times\mathbb{R}^p \rightarrow
\mathbb{R}\)</span>：</p>
<p><span class="math display">\[
L(x, \lambda, \nu) = f_0(x) + \sum_{i=1}^m \lambda_i f_i(x) + \sum_{i =
1}^p \nu_i h_i(x)
\]</span></p>
<p><span class="math inline">\(\lambda_i\)</span> 和 <span
class="math inline">\(\nu_i\)</span>
分别是和不等式限制函数和等式限制函数对应的拉格朗日乘子；</p>
<h4 id="拉格朗日对偶函数-1">拉格朗日对偶函数</h4>
<p>对the lagrange在 <span class="math inline">\(x\)</span>
上最小化，得到拉格朗日对偶函数： <span class="math display">\[
g(\lambda, \nu) = \inf_{x\in \mathcal{D}}L(x, \lambda, \nu)
\]</span> 由于 <span class="math inline">\(L\)</span> 是 <span
class="math inline">\((\lambda, \nu)\)</span>
的线性函数，之后在经过求下确界，因此 <span
class="math inline">\(g\)</span> 是 <span
class="math inline">\((\lambda, \nu)\)</span>
的凹函数（无论原问题是否是凸的）；</p>
<h4 id="最优值的下界">最优值的下界</h4>
<p>假设 <span class="math inline">\(\tilde{x}\)</span>
是原问题的任意可行解，那么在 <span class="math inline">\(\lambda \succeq
0\)</span> 时有 <span class="math inline">\(\sum_{i=1}^m \lambda_i
f_i(\tilde x) \le 0\)</span>，<span class="math inline">\(\sum_{i=1}^p
\nu_i h_i(\tilde{x}) = 0\)</span>，即： <span class="math display">\[
f_0(\tilde{x}) \ge L(\tilde{x}, \lambda, \nu) \ge g(\lambda, \nu)
\]</span></p>
<p>对于任意的 <span class="math inline">\(\lambda \succeq 0\)</span>
都成立；</p>
<p>这就意味着，当 <span class="math inline">\(\lambda \succeq 0\)</span>
时，<span class="math inline">\(g(\lambda, \nu)\)</span> 是 <span
class="math inline">\(f\)</span> 的有参下界，其最大值 <span
class="math inline">\(d^\star\)</span> 对于我们估计原问题的最小值 <span
class="math inline">\(p^\star\)</span>很有意义；</p>
<h4
id="从线性估计角度理解拉格朗日对偶函数">从线性估计角度理解拉格朗日对偶函数</h4>
<p>为什么要这样表示 <span class="math inline">\(L\)</span>？又为什么
<span class="math inline">\(\inf L\)</span>
是原问题的有参下界？下面从线性估计的角度进行解释：</p>
<p>原问题可以等价表示为如下的无限制问题：</p>
<p><span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp; f_0(x) + \sum_{i=1}^m I_{-}(f_i(x)) +
\sum_{i=1}^p I_0 (h_i(x))
\end{array}
\]</span></p>
<p>其中： <span class="math display">\[
I_-(x) = \left\{\begin{matrix}
  0 &amp; u \le 0 \\
  \infty &amp; u &gt; 0
\end{matrix}\right .
\]</span> <span class="math display">\[
I_0(x) = \left\{\begin{matrix}
  0 &amp; u = 0 \\
  \infty &amp; \text{otherwise}
\end{matrix}\right .
\]</span></p>
<p>使用 <span class="math inline">\(\lambda_i f_i(x)\)</span>，<span
class="math inline">\(\lambda_i \ge 0\)</span>，估计 <span
class="math inline">\(I_-(f_i(x))\)</span>，用 <span
class="math inline">\(\nu_i h_i(x)\)</span> 估计 <span
class="math inline">\(I_0(h_i(x))\)</span>；问题表示为：</p>
<p><span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp; L(x, \lambda, \nu)
\end{array}
\]</span></p>
<p>这种估计是soft的，虽然很poor，但是对于任意的 <span
class="math inline">\(u\)</span> 满足 <span
class="math inline">\(\lambda_i u \le I_-(u)\)</span> 和 <span
class="math inline">\(\nu_i u \le
I_0(u)\)</span>，这样对目标函数的“低估”也就意味着该问题得到的最优值
<span class="math inline">\(g(\lambda, \nu)\)</span>
是对原问题最优值的下界；</p>
<h4 id="例子-4">例子</h4>
<h5 id="线性限制的最小平方">线性限制的最小平方</h5>
<p>问题形式如下：</p>
<p><span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp; x^Tx\\
  \text{subject to} &amp; Ax=b
\end{array}
\]</span></p>
<p>The lagrange:</p>
<p><span class="math display">\[
L(x, \nu) = x^Tx + \nu^T(Ax-b)
\]</span></p>
<p>这是关于 <span class="math inline">\(x\)</span>
的凸函数，我们容易得到在 <span class="math inline">\(x =
-(1/2)A^T\nu\)</span> 时取得最小值：</p>
<p><span class="math display">\[
g(\nu) = -(1/4)\nu^TAA^T\nu -b^T\nu
\]</span></p>
<p>是关于 <span class="math inline">\(\nu\)</span> 的偶函数，对于任何
<span class="math inline">\(\nu \in \mathbb{R}\)</span>，都满足： <span
class="math inline">\(g(\nu) \le \inf\{x^Tx ~|~ Ax=b\}\)</span></p>
<h5 id="二分two-way-partitioning问题">二分(Two-way
partitioning)问题</h5>
<p>考虑 <span class="math inline">\(n\)</span>
个点，两两之间存在一条无向边，边的权重记录的可类似于两个点彼此互相讨厌的程度；我们希望将
<span class="math inline">\(n\)</span>
个点分成两类，并且让全局的彼此讨厌水平最小，形式化为： <span
class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp; x^TWx \\
  \text{subject to} &amp; x_i ^ 2 = 1\\
\end{array}
\]</span></p>
<p>问题的可行集合是在 <span class="math inline">\(\mathbb{R}^n\)</span>
中的单位立方体的 <span class="math inline">\(2^n\)</span>
个顶点，并不是凸优化问题；如果把条件变为到 <span
class="math inline">\(x_i^2 \le 1\)</span> 则是凸优化问题；</p>
<p>但是这种问题的求解有一个特殊且简单的做法：</p>
<p><span class="math inline">\(x_i ^ 2 = 1, ~i = 1, \dots n\)</span>
<span class="math inline">\(\implies\)</span> <span
class="math inline">\(\sum_{i=1}^n x_i^2 = n\)</span>，即 <span
class="math inline">\(\lVert x \rVert _2^2 = n\)</span></p>
<p>二次型 <span class="math inline">\(x^TAx\)</span> 在 <span
class="math inline">\(\lVert x\rVert _2^2 = 1\)</span> 时的最小值是
<span
class="math inline">\(\lambda_{\min}(W)\)</span>，因此该问题的最小值下界是
<span class="math inline">\(n\lambda_{\min}(W)\)</span>；</p>
<p>让我们用拉格朗日法解决找到这个下界：</p>
<p><span class="math display">\[
\begin{split}
L(x, \nu) &amp;= x^TWx + \sum_{i=1}^n\nu_i(x_i^2 -1) \\
&amp;= x^T(W + \mathrm{diag}(\nu))x - \mathbf{1}^T\nu
\end{split}
\]</span></p>
<p>因此， <span class="math display">\[
g(\nu) = \left\{ \begin{matrix}
  -\mathbf{1}^T\nu, &amp; W + \mathrm{diag}(\nu)\succeq 0\\
  -\infty &amp; W + \mathrm{diag}(\nu)\prec 0
\end{matrix}\right .
\]</span></p>
<p>取 <span class="math inline">\(\nu =
-\mathbf{1}^T\lambda_{\min}(W)\)</span>，得到 <span
class="math inline">\(g(\nu) = n\lambda_{\min}(W)\)</span></p>
<h4 id="拉格朗日对偶函数与共轭函数">拉格朗日对偶函数与共轭函数</h4>
<p>在如下优化问题中，我们来找寻拉格朗日对偶函数和共轭函数的关系： <span
class="math display">\[
\begin{matrix}
  \text{minimize} &amp; f_0(x) \\
  \text{subject to} &amp; Ax \preceq b \\
  &amp; Cx = d
\end{matrix}
\]</span></p>
$$
<span class="math display">\[\begin{split}
  g(\lambda, \nu) &amp;= \inf_{x} \left( f_0(x) + \lambda^T(Ax-b) +
\nu^T(Cx-d) \right) \\
  &amp;= -\sup_{x}((-A^T\lambda-C^T\nu)^Tx -f_0(x)) -b^T\lambda -d^T\nu
\\
  &amp;= -f_0^*(-A^T\lambda-C^T\nu)  -b^T\lambda -d^T\nu

\end{split}\]</span>
<p>$$</p>
<h3 id="拉格朗日对偶问题">5.2 拉格朗日对偶问题</h3>
<p>上述由拉格朗日对偶函数求最大值得到原问题的下界的过程，可以看作求拉格朗日对偶问题，形式化为：
<span class="math display">\[
\begin{array}{ll}
  \text{maximize} &amp; g(\lambda, \nu)\\
  \text{subject to} &amp; \lambda \succeq 0
\end{array}
\]</span></p>
<p>可行域为 <span class="math inline">\(\{(\lambda, \nu) ~|~ \lambda
\succeq 0, g(\lambda, \nu) &gt; -\infty\}\)</span> ；最优解记为 <span
class="math inline">\((\lambda^\star, \nu^\star)\)</span>；</p>
<p>无论原问题怎样，拉格朗日对偶问题是个凸优化问题；</p>
<h4 id="对偶限制显式表示">对偶限制显式表示</h4>
<h5 id="标准形式lp的拉格朗日对偶">标准形式LP的拉格朗日对偶</h5>
<p><span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp; c^Tx \\
  \text{subject to} &amp; Ax = b \\
  &amp; x\succeq 0
\end{array}
\]</span></p>
<p>其对偶函数为： <span class="math display">\[
g(\lambda, \nu) = \left \{
  \begin{matrix}
    -b^T\nu &amp; A^T\nu -\lambda + c = 0\\
    -\infty &amp; \text{otherwise}
  \end{matrix}
\right .
\]</span></p>
<p>严格地说，对应的拉格朗日对偶问题为：</p>
<p><span class="math display">\[
\begin{array}{ll}
  \text{maximize} &amp; g(\lambda, \nu) = \left \{
    \begin{matrix}
      -b^T\nu &amp; A^T\nu -\lambda + c = 0\\
      -\infty &amp; \text{otherwise}
    \end{matrix} \right . \\
  \text{subject to} &amp; \lambda \succeq 0 \\
\end{array}
\]</span></p>
<p>显式拉格朗日函数的定义域限制，该问题可以等价于：</p>
<p><span class="math display">\[
\begin{array}{ll}
  \text{maximize} &amp; -b^T\nu \\
  \text{subject to} &amp; A^T\nu -\lambda + c = 0 \\
  &amp; \lambda \succeq 0
\end{array}
\]</span></p>
<p>进一步地，还可以精简为：</p>
<p><span class="math display">\[
\begin{array}{ll}
  \text{maximize} &amp; -b^T\nu \\
  \text{subject to} &amp; A^T\nu + c \succeq 0 \\
\end{array}
\]</span></p>
<p>是一个不等式形式的LP问题；</p>
<h5 id="不等式形式的lp问题">不等式形式的LP问题</h5>
<p>类似的，我们可以得到，对于不等式形式的LP问题：</p>
<p><span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp; c^Tx \\
  \text{subject to} &amp; A^Tx+b \succeq 0 \\
\end{array}
\]</span></p>
<p>其拉格朗日对偶问题可以等价表示为标准形式的LP问题：</p>
<p><span class="math display">\[
\begin{array}{ll}
  \text{maximize} &amp; -b^T\lambda \\
  \text{subject to} &amp; A^T\lambda + c \succeq 0 \\
  &amp; \lambda \succeq 0
\end{array}
\]</span></p>
<h4 id="弱对偶性">弱对偶性</h4>
<p><span class="math inline">\(p^\star\)</span> 是原问题的最小值，<span
class="math inline">\(d^\star\)</span>
是其拉格朗日对偶问题的最大值，在他们都是有限的条件下，有： <span
class="math display">\[
d^\star \le p^\star
\]</span></p>
<p>将 <span class="math inline">\(p^\star - d^\star\)</span> 原问题的
optimal duality gap；</p>
<h4
id="强对偶性与施莱特约束规范-slaters-constraint-qualification">强对偶性与施莱特约束规范
(Slater's constraint qualification)</h4>
<p>如果 <span class="math inline">\(p^\star =
d^\star\)</span>，则称有强对偶性；对于优化问题的constraints，满足一定的条件可以使得拥有强对偶性，这样的条件就是
constraint qualification；</p>
<p><span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp; f_0(x)\\
  \text{subject to} &amp; f_i(x) \le 0, &amp; i = 1, \dots, m \\
  &amp; Ax = b
\end{array}
\]</span></p>
<p>Slater's condition 是一种简单的 constraint qualification：</p>
<p>存在 <span class="math inline">\(x\in
\operatorname{relint}\mathcal{D}\)</span>，满足 <span
class="math display">\[
f_i(x) &lt; 0,\quad i = 1, \dots, m,\quad Ax = b
\]</span></p>
<p>如果问题是凸的且Slaters' condition满足，则有强对偶性；</p>
<p>(weak form of Slater's condition):
如果不等式限制函数中有仿射函数，那么他们可以不必是严格不等；</p>
<h4 id="例子-5">例子</h4>
<p>P227-229，此处略；</p>
<h4 id="mixed-strategies-for-matrix-games">Mixed strategies for matrix
games</h4>
<p>用强对偶性解释了知道对手的策略对赢得matrix
games没有优势，很有意思；</p>
<h3 id="几何解释">5.3 几何解释</h3>
<h3 id="鞍点解释">5.4 鞍点解释</h3>
<blockquote>
<p>这两节笔者感觉读起来很吃力，并且都是对拉格朗日对偶的各种interpretation，包含
constraint qualification 的证明等等，就先跳过了；</p>
</blockquote>
<h3 id="最优化条件">5.5 最优化条件</h3>
<h4 id="次优证明与停止准则">次优证明与停止准则</h4>
<p>对偶可行点 <span class="math inline">\((\lambda, \nu)\)</span> 提供了
<span class="math inline">\(p^\star &gt; g(\lambda, \nu)\)</span> 的证明
(proof or certificate)：不用求出 <span
class="math inline">\(p^\star\)</span>，我们就可以知道当前的解距离最优解的程度，设
<span class="math inline">\(x\)</span> 是原问题的可行解： <span
class="math display">\[
f_0(x) - p^\star \le f_0(x) - g(\lambda, \nu)
\]</span></p>
<p>我们将 <span class="math inline">\(f_0(x) - g(\lambda, \nu)\)</span>
记作 duality gap；有 <span class="math inline">\(p^\star \in [g(\lambda,
\nu), f_0(x)]\)</span>，<span class="math inline">\(d^\star \in
[g(\lambda, \nu), f_0(x)]\)</span>；</p>
<p>假设算法产生了一系列的 <span class="math inline">\(x^{(k)}\)</span>
和 <span class="math inline">\((\lambda^{(k)},\nu^{(k)})\)</span>，<span
class="math inline">\(k = 1, 2, \dots\)</span>，那么停止准则就是： <span
class="math display">\[
f_0(x^{(k)}) - g(\lambda^{(k)}, \nu^{(k)}) \le \epsilon_\mathrm{abs}
\]</span></p>
<p>也可以使用相对误差 <span
class="math inline">\(\epsilon_\mathrm{rel}\)</span>： <span
class="math display">\[
g(\lambda^{(k)}, \nu^{(k)}) &gt; 0, \quad \frac{f_0(x^{(k)}) -
g(\lambda^{(k)}, \nu^{(k)})}{g(\lambda^{(k)}, \nu^{(k)})} \le
\epsilon_\mathrm{rel}
\]</span> 或者 <span class="math display">\[
f_0(x^{(k)}) &lt; 0, \quad \frac{f_0(x^{(k)}) - g(\lambda^{(k)},
\nu^{(k)})}{-f_0(x^{(k)})} \le \epsilon_\mathrm{rel}
\]</span></p>
<p>上述条件保证了 <span class="math display">\[
\frac{f_0(x^{(k)})-p^\star}{|p^\star|} \le \epsilon_\mathrm{rel}
\]</span></p>
<h4 id="互补松弛-complementary-slackness">互补松弛 (Complementary
slackness)</h4>
<p>假设有强对偶性，记 <span class="math inline">\(x^\star\)</span> 和
<span class="math inline">\((\lambda^\star, \nu^\star)\)</span>
分别是原问题和对偶问题的最优解，则：</p>
<p><span class="math display">\[
\begin{split}
  f_0(x^\star) &amp;= g(\lambda^\star, \nu^\star) \\
  &amp;= \inf_{x}\left( f_0(x^\star) +
\sum_{i=1}^m\lambda^\star_if_i(x^\star) +
\sum_{i=1}^p\nu^\star_ih_i(x^\star)\right)\\
  &amp;\le f_0(x^\star) + \sum_{i=1}^m\lambda^\star_if_i(x^\star) +
\sum_{i=1}^p\nu^\star_ih_i(x^\star) \\
  &amp;\le f_0(x^\star).
\end{split}
\]</span></p>
<blockquote>
<p>第四行不等式，由于 <span class="math inline">\(f_i(x^\star)\le
0\)</span>，<span class="math inline">\(h_i(x^\star) = 0\)</span>，<span
class="math inline">\(\lambda_i^\star \ge 0\)</span> 得到；</p>
</blockquote>
<p>由此，我们可以得到一些有趣的结论（在满足强对偶性条件下）：</p>
<ol type="1">
<li><span class="math inline">\(x^\star\)</span> 是 <span
class="math inline">\(L(x, \lambda, \nu)\)</span> 关于 <span
class="math inline">\(x\)</span> 的一个最优解；</li>
<li>互补松弛条件：<span class="math inline">\(\lambda^\star f_i(x^\star)
= 0, ~i = 1, \dots, m\)</span>，也就是说，要么第 <span
class="math inline">\(i\)</span> 个最优的拉格朗日乘子是0，要么第 <span
class="math inline">\(i\)</span> 个不等式限制是 active 的；</li>
</ol>
<h4 id="kkt-最优条件">KKT 最优条件</h4>
<h5 id="对于非凸优化问题">对于非凸优化问题</h5>
<p>假设 <span class="math inline">\(f_0, \dots, f_m, h_1, \dots,
h_p\)</span> 是可导的，但并不假设其凸性；记 <span
class="math inline">\(x^\star\)</span> 和 <span
class="math inline">\((\lambda^\star, \nu^\star)\)</span>
分别是原问题和对偶问题的最优解，并且duality gap为0（强对偶性）；由于
<span class="math inline">\(x^\star\)</span> 是 <span
class="math inline">\(L(x, \lambda, \nu)\)</span> 关于 <span
class="math inline">\(x\)</span> 的一个最优解，有：</p>
<p><span class="math display">\[
\nabla f_0(x^\star) + \sum_{i=1}^m\lambda^\star_i\nabla f_i(x^\star) +
\sum_{i=1}^p\nu^\star_i\nabla h_i(x^\star) = 0
\]</span></p>
<p>由此，可以得到 <strong>Karush-Kuhn-Tucker (KKT) 条件</strong>：</p>
<p><span class="math display">\[
\begin{array}{rll}
  f_i(x^\star) &amp;\le 0, &amp; i = 1, \dots, m\\
  h_i(x^\star) &amp; = 0, &amp; i = 1, \dots, p\\
  \lambda_i^\star &amp; \ge 0, &amp; i = 1, \dots, m\\
  \lambda_i^\star f_i(x^\star) &amp; = 0, &amp; i = 1, \dots, m\\
  \nabla f_0(x^\star) + \sum_{i=1}^m\lambda^\star_i\nabla f_i(x^\star) +
\sum_{i=1}^p\nu^\star_i\nabla h_i(x^\star) &amp;= 0
\end{array}
\]</span></p>
<p>综上，任何目标函数和限制函数可导的、强对偶的优化问题，其原问题和对偶问题的最优解对满足KKT条件；(KKT条件是这些点作为原问题和对偶问题最优解的必要条件)</p>
<h5 id="对于凸优化问题">对于凸优化问题</h5>
<p>对于满足上述条件(可导、强对偶)的凸优化问题，满足KKT条件是这些点作为原问题和对偶问题的最优解的充要条件；</p>
<p>使用KKT条件，可以为一些凸优化问题求出解析解；很多算法本质上也都是在求解KKT条件；</p>
<h4 id="用对偶问题求解原问题">用对偶问题求解原问题</h4>
<p>由前文内容可知，如果满足强对偶，且对偶问题的最优解 <span
class="math inline">\((\lambda^\star, \nu^\star)\)</span>
存在，那么原问题的最优解同样也是 <span class="math inline">\(L(x,
\lambda^\star, \nu^\star)\)</span> 关于 <span
class="math inline">\(x\)</span> 最小化时的最优解；</p>
<p>当原问题比较难，但是满足强队偶性，且容易求得其对偶问题的最优解时，我们就可以：</p>
<p><span class="math display">\[
\text{minimize} \quad f_0(x) + \sum_{i=1}^m \lambda_i^\star f_i(x) +
\sum_{i = 1}^p \nu_i^\star h_i(x)
\]</span></p>
<p>该问题的解如果在原问题上是可行的，那么必然时原问题的最优解；</p>
<p>例子：最大化交叉熵，问题形式如下：</p>
<p><span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp; f_0(x) = \sum_{i=1}^n x_i\log x_i\\
  \text{subject to} &amp; Ax \preceq b \\
  &amp; \mathbf{1}^Tx = 1
\end{array}
\]</span></p>
<p>首先，<span class="math inline">\(f(x) = x\log x， x\in
\mathbb{R}_{++}\)</span> 的共轭函数如下，有 <span
class="math inline">\(\operatorname{dom}f_0^* =
\mathbb{R}\)</span>：</p>
<p><span class="math display">\[
\begin{aligned}
  f^*(y) &amp;= \sup_{x\in \mathbb{R}_{++}} \{yx - x\log x\} \\
  &amp;= e^{y-1}
\end{aligned}
\]</span></p>
对于 <span class="math inline">\(f_0(x) = \sum_{i=1}^n x_i\log x_i，
x\in \mathbb{R}_{++}^n\)</span>： $$
<span class="math display">\[\begin{aligned}
f^*_0(y) &amp;= \sup_{x\in \mathbb{R}_{++}^n} \left\{y^Tx - \sum_{i=1}^n
x_i\log x_i\right\} \\
&amp;= \sup_{x\in \mathbb{R}_{++}^n} {\sum_{i=1}^n (y_ix_i - x_i \log
x_i)}\\
&amp;= \sum_{i=1}^n \sup_{x\in \mathbb{R}_{++}} \{y_ix_i - x_i\log x_i
\}\\
&amp;= \sum_{i=1}^n e^{y_i - 1}

\end{aligned}\]</span>
<p>$$</p>
<blockquote>
<p>在 <span class="math inline">\(a\)</span> 和 <span
class="math inline">\(b\)</span> 无关（可以同时取到最大值）时 <span
class="math inline">\(\max{a} + \max{b} = \max{(a + b)}\)</span></p>
</blockquote>
<p>使用共轭函数和拉格朗日函数的关系求出：</p>
<p><span class="math display">\[
\begin{aligned}
  g(\lambda, \nu) &amp;= -f_0^*(-A^T\lambda-C^T\nu) - b^T\lambda -d^T\nu
\\
  &amp;= -f_0^*(-A^T\lambda-\nu) - b^T\lambda -\nu \\
  &amp;= - b^T\lambda -\nu  - \sum_{i=1}^n e^{-a_i^T\lambda -\nu -1}\\
  &amp;= - b^T\lambda -\nu - e^{-\nu-1}\sum_{i=1}^n e^{-a_i^T\lambda}
\end{aligned}
\]</span></p>
<p>其中，<span class="math inline">\(a_i\)</span> 是 <span
class="math inline">\(A\)</span> 的第 <span
class="math inline">\(i\)</span> 列，<span class="math inline">\(\lambda
\in \mathbb{R}^n\)</span>，<span class="math inline">\(\nu \in
\mathbb{R}\)</span>；</p>
<p>进而可以写出其拉格朗日对偶问题为：</p>
<p><span class="math display">\[
\begin{array}{ll}
  \text{maximize} &amp; - b^T\lambda -\nu - e^{-\nu-1}\sum_{i=1}^n
e^{-a_i^T\lambda} \\
  \text{subject to} &amp; \lambda \succeq 0
\end{array}
\]</span></p>
<p>拉格朗日对偶问题一定是凸优化问题，且假设其满足弱形式的史莱特条件（这里不等式限制函数全为仿射函数），即存在
<span class="math inline">\(x\succ 0\)</span> 满足 <span
class="math inline">\(Ax\preceq b\)</span> 且 <span
class="math inline">\(\mathbf{1}^Tx =
1\)</span>，那么就有了强对偶性；我们可以从上述凸优化问题中求出 <span
class="math inline">\((\lambda^\star, \nu^\star)\)</span>；</p>
<p>我们可以先固定 <span class="math inline">\(\lambda\)</span>
进行最大化，让关于 <span class="math inline">\(\nu\)</span>
的导数为0，得到：</p>
<p><span class="math display">\[
\nu = \log \sum_{i=1}^n e^{-a_i^T\lambda} - 1
\]</span></p>
<p>问题简化为：</p>
<p><span class="math display">\[
\begin{array}{ll}
  \text{maximize} &amp; - b^T\lambda  - \log\sum_{i=1}^n
e^{-a_i^T\lambda} \\
  \text{subject to} &amp; \lambda \succeq 0
\end{array}
\]</span></p>
<p>这是一个几何规划问题的凸优化形式；</p>
<p>求得 <span class="math inline">\((\lambda^\star, \nu^\star)\)</span>
后，拉格朗日函数在 <span class="math inline">\((\lambda^\star,
\nu^\star)\)</span> 是关于 <span class="math inline">\(x\)</span> 在
<span class="math inline">\(\mathcal{D}\)</span>
上的严格凸函数，可以令关于 <span class="math inline">\(x\)</span>
的导数为0得到 <span class="math inline">\(x^\star\)</span>；如果 <span
class="math inline">\(x^\star\)</span>
是可行的，那么就是最优解；如果不可行，说明最优解不可达；</p>
<p><span class="math display">\[
L(x, \lambda^\star, \nu^\star) = \sum_{i=1}^n x_i \log x_i +
\lambda^{\star T} (Ax-b) + \nu^\star (\mathbf{1}^Tx - 1)
\]</span></p>
<p><span class="math display">\[
x^\star_i = 1/\exp{(a^T_i \lambda^\star + \nu^\star + 1)},\quad i = 1,
\dots, n
\]</span></p>
<h3 id="扰动与敏感性分析">5.6 扰动与敏感性分析</h3>
<h4 id="受扰动的问题">受扰动的问题</h4>
<p><span class="math display">\[
\begin{array}{lll}
  \text{minimize} &amp; f_0(x)\\
  \text{subject to} &amp; f_i(x) \le u_i, &amp; i = 1, \dots, m\\
  &amp; h_i(x) = v_i, &amp; i = 1, \dots, p
\end{array}
\]</span></p>
<p>当 <span class="math inline">\(u = 0, v=0\)</span>
时，就是一般的优化问题；</p>
<p>记扰动问题的最优值为 <span class="math inline">\(p^\star(u,
v)\)</span>；当原问题是凸的时，<span
class="math inline">\(p^\star\)</span> 也是关于 <span
class="math inline">\((u, v)\)</span> 的凸函数；</p>
<h4 id="一个全局不等式">一个全局不等式</h4>
<p>假设有强对偶性，且对偶最优解 <span
class="math inline">\((\lambda^\star, \nu^\star)\)</span>
可以求出（当原问题是凸的，且满足史莱特条件时），有：</p>
<p><span class="math display">\[
p^\star(u, v) \ge p^\star(0, 0) - \lambda^{\star T} u - \nu^{\star T}v
\]</span></p>
<p>证明很简单：</p>
<p><span class="math display">\[
\begin{aligned}
   p^\star(0, 0) = g(\lambda^\star, \nu^\star) &amp; \le  f_0(x) +
\sum_{i=1}^m \lambda^\star_i f_i(x) + \sum_{i=1}^p \nu^\star_i h_i(x)\\
   &amp; \le f_0(x) + \lambda^{\star T} u +\nu^{\star T}v
\end{aligned}
\]</span></p>
<p>因此对于任何可行的 <span class="math inline">\(x\)</span>，都有：
<span class="math display">\[
f_0(x) \ge p^\star(0, 0) - \lambda^{\star T} u - \nu^{\star T}v
\]</span></p>
<p>也就证得全局不等式；</p>
<h5 id="敏感性解释">敏感性解释</h5>
<p><img src="fig-5-10.png" /></p>
<p>该全局不等式给出了扰动最优的一个仿射的下界；</p>
<h4 id="局部敏感性分析">局部敏感性分析</h4>
<p>假设 <span class="math inline">\(p^\star(u, v)\)</span> 在 <span
class="math inline">\(u=0, v=0\)</span> 可导，且有强对偶性，有：</p>
<p><span class="math display">\[
\lambda^\star_i = -\frac{\partial p^\star(0, 0)}{\partial u_i}, \quad
\nu^\star_i = -\frac{\partial p^\star(0, 0)}{\partial v_i}
\]</span></p>
<ul>
<li>微微缩紧(tighten)第 <span class="math inline">\(i\)</span>
个不等式限制 （<span class="math inline">\(u_i\)</span>
是一个很小的负数），<span class="math inline">\(p^\star\)</span>，<span
class="math inline">\(p^\star\)</span> 大约增加 <span
class="math inline">\(-\lambda_i^\star u_i\)</span>；</li>
<li>微微放松(loosen)第 <span class="math inline">\(i\)</span>
个不等式限制 （<span class="math inline">\(u_i\)</span>
是一个很小的正数），<span class="math inline">\(p^\star\)</span>，<span
class="math inline">\(p^\star\)</span> 大约减少 <span
class="math inline">\(\lambda_i^\star u_i\)</span>；</li>
</ul>
<details>
<summary>
证明
</summary>
<p>假设 <span class="math inline">\(p^\star(u, v)\)</span> 在 <span
class="math inline">\(u=0, v=0\)</span> 可导，且有强对偶性，设扰动 <span
class="math inline">\(u = te_i, v = 0\)</span>，其中 <span
class="math inline">\(e_i\)</span> 是仅第 <span
class="math inline">\(i\)</span> 个元素不为0的单位向量，那么</p>
<p><span class="math display">\[
\lim_{t\rightarrow 0} \frac{p^\star(te_i, 0)-p^\star}{t} =
\frac{\partial p^\star(0, 0)}{\partial u_i}
\]</span></p>
<p>其中，当 <span class="math inline">\(t &gt; 0\)</span>
时，由全局不等式可得： <span class="math display">\[
\frac{p^\star(te_i, 0)-p^\star}{t} \ge -\lambda_i
\]</span> 当 <span class="math inline">\(t &lt; 0\)</span>
时，上述不等式反号；</p>
<p>综上，有： <span class="math display">\[
-\lambda^\star_i = \frac{\partial p^\star(0, 0)}{\partial u_i}
\]</span></p>
同理可证关于 <span class="math inline">\(-\nu^\star_i\)</span> 的等式；
</details>
<p>局部敏感性结果让我们得以分析 how active a constraint is at the
optimum <span class="math inline">\(x^\star\)</span>：</p>
<p>如果 <span class="math inline">\(f_i(x^\star) &lt;
0\)</span>，说明我们对第 <span class="math inline">\(i\)</span>
个不等式限制进行微小的紧缩或者放宽都不会影响最优解；</p>
<p>如果 <span class="math inline">\(f_i(x^\star) =
0\)</span>，此时可以查看 <span
class="math inline">\(\lambda_i^\star\)</span> 的值，如果很小，说明对第
<span class="math inline">\(i\)</span>
个不等式限制进行微小的紧缩或者放宽对最优解的影响很小；如果 <span
class="math inline">\(\lambda_i^\star\)</span>
很大，那么扰动对最优解的影响也很大；</p>
<h3 id="例子-6">5.7 例子</h3>
<h4 id="引入变量和等式限制">引入变量和等式限制</h4>
<p>考虑如下无限制优化问题： <span class="math display">\[
\text{minimize} f_0(Ax+b)
\]</span></p>
<p>其拉格朗日对偶函数即 <span
class="math inline">\(p^\star\)</span>，具有强队偶性，但是从该常函数我们不能获得任何有用的信息；</p>
<p>我们把问题改写成：</p>
<p><span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp; f_0(y) \\
  \text{subject to} &amp; Ax+b=y
\end{array}
\]</span></p>
<p>The Lagrangian为： <span class="math display">\[
L(x, y, \nu) = f_0(y) + \nu^T (Ax+b-y)
\]</span></p>
<p>对其在 <span class="math inline">\(x,y\)</span> 上求极小值，先在
<span class="math inline">\(x\)</span> 上求极小值，只有 <span
class="math inline">\(A^T\nu = 0\)</span> 才能使得 <span
class="math inline">\(g(\nu)\)</span> bounded above；</p>
<p><span class="math display">\[
g(\nu) = b^T\nu + \inf_{y}(f_0(y) - \nu^T y) = b^T\nu  - f_0^*(\nu)
\]</span></p>
<p>其中，<span class="math inline">\(f_0^*\)</span> 是 <span
class="math inline">\(f_0\)</span> 的共轭函数；</p>
<p>拉格朗日对偶问题即为： <span class="math display">\[
\begin{array}{ll}
  \text{maximize} &amp; b^T\nu  - f_0^*(\nu) \\
  \text{subject to} &amp; A^T\nu = 0
\end{array}
\]</span></p>
<p>这个形式就有用多了；</p>
<p>引入等式限制的方法还可以用在包含不等式限制的优化问题上： <span
class="math display">\[
\begin{array}{lll}
  \text{minimize} &amp; f_0(A_0x+b_0) \\
  \text{subject to} &amp; f_i(A_ix + b_i) \le 0, &amp; i =1,\dots, m
\end{array}
\]</span></p>
<p>其中 <span class="math inline">\(A_i\in \mathbb{R}^{k_i\times
n}\)</span>，<span class="math inline">\(f_i:
\mathbb{R}^{k_i}\rightarrow\mathbb{R}\)</span> 是凸函数；引入变量 <span
class="math inline">\(y\in \mathbb{R}^{k_i},~i = 0, \dots,
m\)</span>；有： <span class="math display">\[
\begin{array}{lll}
  \text{minimize} &amp; f_0(y_0) \\
  \text{subject to} &amp; f_i(y_i) \le 0, &amp; i =1,\dots, m\\
  &amp; A_ix + b = y_i, &amp; i = 0, \dots, m
\end{array}
\]</span></p>
<p>该问题的The Lagrangian是： <span class="math display">\[
L(x, y_0, \dots, y_m, \lambda, \nu_0, \dots, \nu_m) =
f_0(y_0) + \sum_{i=1}^m \lambda_if_i(y_i) + \sum_{i=0}^m\nu_i^T(A_ix +
b_i - y_i)
\]</span></p>
<p>在对 <span class="math inline">\(x\)</span> 最小化时，可以得到 <span
class="math inline">\(\sum_{i=0}^mA_i^T\nu_i = 0\)</span>；接着对 <span
class="math inline">\(y\)</span> 最小化：</p>
<p><span class="math display">\[
\begin{aligned}
  g(\lambda, \nu_0, \dots, \nu_m) &amp;= \sum_{i=0}^m \nu_i^Tb_i +
\inf_{y_0, \dotsm y_m}\left( f_0(y_0) + \sum_{i=1}^m\lambda_i f_i(y_i) -
\sum_{i=0}^m\nu_i^Ty_i \right)\\
  &amp;= \sum_{i=0}^m \nu_i^Tb_i + \inf_{y_0} (f_0(y_0) - \nu_0^Ty_0) +
\sum_{i=1}^m \lambda_i\inf_{y_i} (f_i(y_i) - (\nu_i/\lambda_i)^Ty_i)\\
  &amp;= \sum_{i=0}^m \nu_i^Tb_i - f_0^*(\nu_0) - \sum_{i=1}^m \lambda_i
f_i^*(\nu_i/\lambda_i)
\end{aligned}
\]</span></p>
<blockquote>
<p>因为在第二行等式中，把 <span class="math inline">\(\lambda_i\)</span>
从 <span class="math inline">\(\inf\)</span>
里面提到了外面，因此需要保证 <span class="math inline">\(\lambda \succeq
0\)</span>；</p>
<p>因为 <span class="math inline">\(y_0, \dots, y_m\)</span>
的变元独立，可以满足一定条件使得 “和的最小值等于最小值的和”</p>
</blockquote>
<p>值得注意的是，在 <span class="math inline">\(\lambda_i =
0\)</span>，<span class="math inline">\(\nu_i \ne 0\)</span> 时，<span
class="math inline">\(\lambda_i f_i^*(\nu_i/\lambda_i)\)</span>
的值是让人迷惑的？要让 <span class="math inline">\(g\)</span> 是bounded
above，则需要 <span class="math inline">\(\lambda_i
f_i^*(\nu_i/\lambda_i) = \infty\)</span>；还是说考虑 <span
class="math inline">\(f_i\)</span> 是凸函数，在其定义域上它是bounded
below的？</p>
<h4 id="转换目标函数">转换目标函数</h4>
<p>在优化问题的等价问题一节中，我们说过，用标量单增函数包裹目标函数，可以得到等价问题，包裹后的问题的对偶问题将非常不一样；</p>
<p>我们考虑最小化norm的问题：</p>
<p><span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp; \lVert Ax=b \rVert
\end{array}
\]</span></p>
<p>将该问题改写为：</p>
<p><span class="math display">\[
\begin{array}{ll}
  \text{minimize} &amp; (1/2)\lVert y \rVert ^2\\
  \text{subject to} &amp; Ax - b = y
\end{array}
\]</span></p>
<p>在套用上一节结论之前，我们首先要求一下 <span
class="math inline">\(f(x) = (1/2)\lVert x \rVert ^2, ~x\in
\mathbb{R}^n\)</span> 的共轭函数：</p>
<details>
<summary>
对偶范数
</summary>
<p>记 <span class="math inline">\(\lVert \cdot \rVert\)</span> 是 <span
class="math inline">\(\mathbb{R}^n\)</span>
上的范数，其对偶范数定义为：</p>
<p><span class="math display">\[
\lVert z \rVert _* = \sup \{ z^Tx ~|~ \lVert x \rVert \le 1\}
\]</span></p>
<p>由该定义可得，对于任意的 <span class="math inline">\(x\)</span> 和
<span class="math inline">\(z\)</span>，有： <span
class="math display">\[
z^Tx \le \lVert x \rVert \lVert z \rVert _*
\]</span></p>
<p>且有 <span class="math inline">\(\lVert x \rVert _{**} =
x\)</span></p>
<p>对于欧几里得范数（向量2范数），由柯西-施瓦茨不等式：</p>
<p><span class="math display">\[
(z^Tx)^2 \le \lVert x \rVert _2^2 \lVert z \rVert _2^2
\]</span></p>
<p>可得欧几里得范数的对偶范数还是它本身；</p>
<p><span class="math inline">\(L_1, L_\infty\)</span>
互为彼此的对偶范数；<span class="math inline">\(L_p, L_q, ~(1/p +
1/q=1)\)</span> 互为彼此的对偶范数；</p>
<p>矩阵的谱范数/矩阵2范数（最大奇异值）的对偶范数是矩阵的核范数（所有奇异值之和）；</p>
</details>
<p><span class="math display">\[
\begin{aligned}
  y^Tx - (1/2)\lVert x \rVert ^2 &amp;\le \lVert y\rVert _* \lVert x
\rVert - (1/2)\lVert x \rVert ^2\\
  &amp;\le (1/2) \lVert y \rVert _*^2
\end{aligned}
\]</span></p>
<blockquote>
<p>第一行不等式来自对偶范数的定义</p>
<p>第二行不等式来自最大化关于 <span class="math inline">\(x\)</span>
的二次函数</p>
<p>两个不等式取等的条件都是 <span class="math inline">\(\lVert x
\rVert\)</span> = <span class="math inline">\(\lVert y\rVert
_*\)</span></p>
</blockquote>
<p>因此，<span class="math inline">\(f^*(y) = (1/2)\lVert y\rVert
_*\)</span>；</p>
<p>这样，拉格朗日对偶问题可以表示为： <span class="math display">\[
\begin{array}{ll}
  \text{maximize} &amp; -(1/2)\lVert \nu \rVert ^2_* + b^T\nu\\
  \text{subject to} &amp; A^T\nu = 0
\end{array}
\]</span></p>
<h4 id="隐式限制">隐式限制</h4>
<p>把部分限制纳入目标函数中，当这些限制不成立时，使得目标函数为无穷大；</p>
<h3 id="theorems-of-alternatives">5.8 Theorems of alternatives</h3>
<p>用拉格朗日对偶问题为不等式系统找到替代 (alternatives)，弱替代 (weak
alternatives) 表明原不等式系统和其替代至多只能有一个可行；强替代 (strong
alternatives) 表明两者有且只有一个是可行的；</p>
<h3 id="广义不等式-1">5.9 广义不等式</h3>
<p>这一节对于不等式限制为广义不等式的优化问题做了上述讨论；</p>
<p><span class="math inline">\(\lambda_i\)</span>
变成了向量，对偶问题中的条件为 <span class="math inline">\(\lambda_i
\succeq_{K_i^*} 0\)</span>；</p>
<p>形式上与标量的情况基本一致(analog)，不再赘述；</p>
]]></content>
      <categories>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>凸优化</tag>
      </tags>
  </entry>
  <entry>
    <title>卷积分解：考虑非线性函数、多层误差累积以及秩选择</title>
    <url>/2023/01/13/Non-linear-Conv-Decomp/</url>
    <content><![CDATA[<img src="/2023/01/13/Non-linear-Conv-Decomp/fig1.png" class="">
<p><strong>论文</strong>：<a
href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Zhang_Efficient_and_Accurate_2015_CVPR_paper.html"><strong>Efficient
and Accurate Approximations of Nonlinear Convolutional
Networks</strong></a></p>
<p><strong>作者</strong>：<em>Xiangyu Zhang, Jianhua Zou, Xiang Ming,
Kaiming He, Jian Sun</em></p>
<p><strong>一作单位</strong>：Microsoft Research</p>
<p><strong>录用情况</strong>：CVPR'2015</p>
<blockquote>
<p>作者从卷积核沿着空间维度和通道维度展开后的低秩性入手，将其分解为更少的卷积核以及一系列1x1卷积；接着，作者考虑了卷积核后的非线性层的印象，设计了一个更加复杂的优化目标，并给出了优化算法；作者还考虑了多层误差累积现象，将优化目标改为“反对称”的；对于每一层的秩的选择，作者将其视为一个组合问题，用贪心法求解；在这一系列的策略下，SPPnet在字符分类精度损失0.7%的情况下获得了4.5倍加速比；</p>
</blockquote>
<span id="more"></span>
<h2 id="方法">方法</h2>
<h3 id="响应的低秩估计">响应的低秩估计</h3>
<p>卷积层的输入时低秩的，卷积核是低秩的，那么输出（响应）也应到是低秩的；我们向来都是利用这种低秩性，来减小计算复杂度；</p>
<p>设一个卷积核的尺寸为 <span class="math inline">\(k\times k\times
c\)</span>，一个输入表示为 <span class="math inline">\(\mathbf x \in
\mathbb{R}^{k^2c+1}\)</span>（补充了1，用于与偏置项相乘），输出响应
<span class="math inline">\(\mathbf y\in \mathbb{R}^d\)</span>
由下式计算出：</p>
<p><span class="math display">\[
\mathbf y = W\mathbf x
\]</span></p>
<p>其中，<span class="math inline">\(W \in
\mathbb{R}^{d\times(k^2c+1)}\)</span>，<span
class="math inline">\(d\)</span>
是卷积核的个数，每个卷积核沿着空间和通道维度展平并append上了偏置项，该式的复杂度是
<span class="math inline">\(O(dk^2c)\)</span>；</p>
<p>如果向量 <span class="math inline">\(\mathbf{y}\)</span>
可在低秩空间中表示，记 <span
class="math inline">\(\overline{\mathbf{y}}\)</span> 是其均值，存在
<span class="math inline">\(M \in \mathbb{R}^{d\times d}\)</span>，<span
class="math inline">\(\operatorname{rank}M = d&#39; &lt;
d\)</span>，使得： <span class="math display">\[
\mathbf{y} = M(\mathbf{y} - \overline{\mathbf{y}}) +
\overline{\mathbf{y}}
\]</span></p>
<blockquote>
<p>笔者暂时不能清晰地说出这条引理为什么是对的；</p>
</blockquote>
<p>将上式展开，得到：</p>
<p><span class="math display">\[
\mathbf{y} = MW\mathbf{x} + \mathbf{b}
\]</span></p>
<p>其中 <span class="math inline">\(\mathbf{b} = \overline{\mathbf{y}} -
M \overline{\mathbf{y}}\)</span>；</p>
<p>对于低秩矩阵 <span class="math inline">\(M\)</span>
我们可以将其分解为 <span class="math inline">\(P,
Q\in\mathbb{R}^{d\times d&#39;}\)</span>，即 <span
class="math inline">\(M=PQ^T\)</span>，记 <span
class="math inline">\(W&#39; = Q^TW\)</span>，那么有</p>
<p><span class="math display">\[
\mathbf{y} = PW&#39;\mathbf{x} + \mathbf{b}
\]</span></p>
<p>该式的复杂度是 <span class="math inline">\(O(d&#39;k^2c +
dd&#39;)\)</span>，由于通常有 <span class="math inline">\(dd&#39;\ll
d&#39;k^2c\)</span>，那么可以将理论复杂度减少到原来的约 <span
class="math inline">\(d&#39;/d\)</span>；</p>
<p>在实现中，<span class="math inline">\(W&#39;\)</span> 相当于 <span
class="math inline">\(d&#39;\)</span> 个 <span
class="math inline">\(k\times k \times c\)</span> 卷积核，<span
class="math inline">\(P\)</span> 相当于 <span
class="math inline">\(d\)</span> 个 <span class="math inline">\(1\times
1\times d&#39;;\)</span> 个卷积核，如图1所示；</p>
<p>对 <span class="math inline">\(M\)</span>
的低秩分解可以用多种方法实现，奇异值分解，主成分分析，都是如下目标函数的解：</p>
<p><span class="math display">\[
\begin{split}
  \min_{M} &amp;\sum_i\Vert (\mathbf{y}_i - \overline{\mathbf{y}}) -
M(\mathbf{y}_i - \overline{\mathbf{y}})\Vert_2^2 \\
  \text{s.t.}~ &amp;\operatorname{rank}M \le d&#39;
\end{split}
\]</span></p>
<h3 id="考虑非线性层">考虑非线性层</h3>
<p>以ReLU作为非线性函数为例，对于 <span
class="math inline">\(\mathbf{y}_i\)</span>
的非负部分的准确估计很重要，而对于其负数部分有再大的估计误差也不会继续传播；因此，在理想情况下，最小化重建损失问题应该记为：</p>
<p><span class="math display">\[
\begin{split}
  \min_{M,\mathbf{b}} &amp;\sum_i \lVert r(\mathbf y_i) -
r(M\mathbf{y}_i + \mathbf{b}_i)  \rVert _2^2\\
  \text{s.t.}~ &amp;\operatorname{rank}M\le d&#39;
\end{split}
\]</span></p>
<p>由于低秩约束以及非线性函数的存在，该问题并不好解，作者将问题relax为了如下形式：</p>
<p><span class="math display">\[
\begin{split}
  \min_{M,\mathbf{b}, \{\mathbf{z}_i\}} &amp;\sum_i \lVert
r(\mathbf{y}_i) - r(\mathbf{z}_i)  \rVert _2^2 + \lambda \lVert \mathbf
z_i - (M\mathbf{y}_i + \mathbf{b}_i) \rVert _2^2\\
  \text{s.t.}~ &amp;\operatorname{rank}M\le d&#39;
\end{split}
\]</span></p>
<p>该问题在 <span
class="math inline">\(\lambda\rightarrow\infty\)</span>
时，逼近前一个问题；对于该问题，可以交替优化 <span
class="math inline">\(\{\mathbf{z}_i\}\)</span> 和 <span
class="math inline">\(M,\mathbf{b}\)</span>；</p>
<p>具体的优化方法见原论文，此处不再赘述；</p>
<h3 id="考虑多层误差累积">考虑多层误差累积</h3>
<p>使用逐层优化的方式，我们提前保存下标准网络各层的输入；记在估计的网络中当前层的输入为
<span
class="math inline">\(\hat{\mathbf{x}}\)</span>，在精确的网络中当前层的输出为
<span class="math inline">\(\mathbf{x}\)</span>，优化问题可写为：</p>
<p><span class="math display">\[
\begin{split}
  \min_{M,\mathbf{b}, \{\mathbf{z}_i\}} &amp;\sum_i \lVert
r(W\mathbf{x}_i) - r(\mathbf{z}_i)  \rVert _2^2 + \lambda \lVert \mathbf
z_i - (MW\hat{\mathbf{x}}_i + \mathbf{b}_i) \rVert _2^2\\
  \text{s.t.}~ &amp;\operatorname{rank}M\le d&#39;
\end{split}
\]</span></p>
<p>相比于都使用 <span class="math inline">\(\hat{\mathbf{x}}\)</span>
或者都使用 <span
class="math inline">\(\mathbf{x}\)</span>，这种设计更合理；</p>
<h3 id="各层秩的选择">各层秩的选择</h3>
<p>每一层的 <span class="math inline">\(d&#39;\)</span>
应该是多少最合适？这里要兼顾精度与速度；在速度上，前文我们得到本文的分解方法在将每层的计算复杂度缩小到原来的
<span class="math inline">\(d&#39;/d\)</span>，即第 <span
class="math inline">\(l\)</span> 层原来的复杂度为 <span
class="math inline">\(C_l\)</span>，期望压缩后整个网络的复杂度为 <span
class="math inline">\(C\)</span>，要对整个网络有加速，至少要满足 <span
class="math inline">\(\sum_l \frac{d&#39;}{d}C_l \le C\)</span>；</p>
<p>作者还通过实验发现，仅压缩一层，精度水平与PCA能量水平大约是线性关系，因此，可以用各层保存的能量（保留的特征值的和）的积来指示整个网络的精度水平，因此，优化问题为：</p>
<p><span class="math display">\[
\begin{split}
  \max_{\{d_l&#39;\}}~ &amp;\mathcal{E} = \prod_l
\sum_{a=1}^{d_l&#39;}\sigma_{l,a}\\
  \text{s.t.}~ &amp;\sum_l \frac{d&#39;}{d}C_l \le C
\end{split}
\]</span></p>
<p>使用如下的贪心方法优化：</p>
<ul>
<li>初始化集合 <span
class="math inline">\(\{\sigma_{l,a}\}\)</span>，初始化 <span
class="math inline">\(d_l&#39;=d_l\)</span>；</li>
<li>每次从集合中剔除一个 <span class="math inline">\(\sigma_{l,
d_l&#39;}\)</span>，即将第 <span class="math inline">\(l\)</span>
层去除特征值最小的对应的特征向量方向，这将复杂度降低了 <span
class="math inline">\(\Delta C =
\frac{1}{d_l}C_l\)</span>，也将目标函数减少为原来的 <span
class="math inline">\(\Delta \mathcal{E}/\mathcal{E} =
\sigma_{l,d&#39;}/\sum_{a=1}^{d_l&#39;}\sigma_{l,a}\)</span>，希望对前者的减少大于对后者的减少，因此遍历所有层，找到使得
<span class="math inline">\(\frac{\Delta
\mathcal{E}/\mathcal{E}}{\Delta}\)</span> 最小的那个 <span
class="math inline">\(\sigma_{l, d_l&#39;}\)</span> 并剔除；</li>
<li>直到达到期望的复杂度 <span class="math inline">\(C\)</span>
为止；</li>
</ul>
<h2 id="实验">实验</h2>
<p>通过逐层压缩实验，证明了非线性压缩在Conv2-7层上都对线性压缩有更低的分类误差；</p>
<p>通过整个网络的实验，证明了非对称的优化目标比对称的优化目标有更低的分类误差；相同加速比下，使用rank
selection相比于手工设置的一组rank，也能降低分类误差；</p>
<p>得益于非对称目标函数的设计，可以在对某一层压缩时使用多种方法，得到的结果会作为下一层压缩时的输入，而下一层重建目标仍然是精确网络上的输出响应；</p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>模型压缩</tag>
        <tag>卷积分解</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习课堂笔记</title>
    <url>/2022/09/30/ML-Class/</url>
    <content><![CDATA[<blockquote>
<p>本文记录在本学期《机器学习及其应用课》上一些印象比较深刻的知识点；大部分在课堂PPT或者相关书籍（周志华《机器学习》，李航《统计学习方法》）上很容易理解的点就不在此赘述了；</p>
<p>笔者水平有限，如果有错误清多多指正；</p>
</blockquote>
<span id="more"></span>
<h2 id="模型的评估与选择">模型的评估与选择</h2>
<h3 id="泛化误差的分解">泛化误差的分解</h3>
泛化误差是模型对位置数据预测的误差： $$
<span class="math display">\[\begin{aligned}
R_\text{exp}(\hat f)&amp;=E_P[L(Y, \hat f(X))]\\
&amp;=\int_{\mathcal{X}\times\mathcal{Y}}L(y, \hat f(x))P(x, y)dxdy

\end{aligned}\]</span>
<p>$$</p>
<p>首先要明确的一点是，泛化误差<span
class="math inline">\(\ne\)</span>某个测试集误差，后者只是前者的一次估计，前者是后者所在分布的期望，测试集的选择、推理过程的随机性都会得到不同的结果。应当使用统计学中的假设检验方法比较两个模型的好坏。</p>
<blockquote>
<p>但是笔者目前看到几乎所有的论文都是，在测试集甚至验证集上，有更好的性能，就SOTA了？</p>
</blockquote>
<p>泛化误差是一个理论上的误差，如果能描述清楚其上下界，则对实验有一定的指导意义：泛化误差的下界决定了在一个任务上最理想能刷到多少点，是收到数据集的限制的；泛化误差的上界反映了模型本身是否容易学习，一般认为，泛化误差上界越小的模型越好，然而对于简单的线性模型，我们也许可以给出上界，但是对于现在的各种复杂的非线性的、深层的模型，给出泛化误差上界仍然是很难的问题。</p>
<p>下面是对泛化误差的分解：</p>
<p>记测试样本为<span class="math inline">\(x\)</span>，<span
class="math inline">\(y_D\)</span>与<span
class="math inline">\(y\)</span>分别为样本在数据集中的标记和样本真实标记，<span
class="math inline">\(f(x;D)\)</span>为训练集<span
class="math inline">\(D\)</span>上学习的模型对于<span
class="math inline">\(x\)</span>的预测标签（以下简记为<span
class="math inline">\(f\)</span>），定义对于测试样本中预测的<strong>方差</strong>为<span
class="math inline">\(var(x)=E_D[(f(x;D)-\overline
f(x))^2]\)</span>（以下简记为<span
class="math inline">\(E[(f-Ef)^2]\)</span>），定义噪声为<span
class="math inline">\(\epsilon^2=E_D[(y_D-y)^2]\)</span>，定义偏差为<span
class="math inline">\(bias^2(x)=(\hat f(x)- y)^2\)</span>;</p>
<blockquote>
<p><span class="math inline">\(y_D\)</span>和<span
class="math inline">\(y\)</span>的区别，是数据标注时引入的误差，由标注员本身认知的偏置和偶然的错误组成；<span
class="math inline">\(f\)</span>和<span
class="math inline">\(Ef\)</span>的区别，即由于训练集变动等原因，导致模型对同一个样本的预测不总是相同；</p>
<p>下面的<span class="math inline">\(L\)</span>直接使用均方误差：</p>
</blockquote>
<p><span class="math display">\[
\begin{aligned}
E(f;D) &amp;= E[(f(x;D)-y_D)^2]\\
&amp;= E[(f - Ef + Ef - y_D)^2]\\
&amp;= E[(f-Ef)^2]+E[(Ef-y_D)^2]+2E[f\cdot Ef + y_DEf - y_Df - E_f^2]\\
&amp;= E[(f-Ef)^2]+E[(E_f-y + y-y_D)^2]\\
&amp;= E[(f-Ef)^2]+E[(E_f-y)^2]+E[(y-y_D)^2+E[(E_f-y)(y-y_D)]\\
&amp;= var(x) + bias^2(x) + \epsilon^2
\end{aligned}
\]</span></p>
<blockquote>
<p>上述推导用到的知识有：</p>
<ul>
<li><p><span class="math inline">\(E(aX+bY)=aEX+bEY\)</span>，其中<span
class="math inline">\(a, b\)</span>是常数，<span
class="math inline">\(X,Y\)</span>是随机变量；特殊地<span
class="math inline">\(E(a)=a\)</span>；</p></li>
<li><p><span
class="math inline">\(E(XY)=E(X)E(Y)\)</span>，当且仅当<span
class="math inline">\(X,Y\)</span>是相互独立的随机变量；</p></li>
</ul>
<p>需要的假设有：</p>
<ul>
<li><p>噪声期望为0，即<span
class="math inline">\(E(y_D-y)=0\)</span>，这是假设了数据集的标注没有系统误差而都是随机误差，比较合理；则可以推出<span
class="math inline">\(\text{Var}[(y_D-y)]=E[(y_D-y)]^2=\epsilon^2\)</span>；</p></li>
<li><p>偏差与噪声无关；</p></li>
</ul>
</blockquote>
<h3 id="vc维">VC维</h3>
<blockquote>
<p>以下定义来自<a
href="https://www.jiqizhixin.com/graph/technologies/e766aa0d-af15-480a-9ce9-b6357442330e">机器之心
- VC维度</a></p>
</blockquote>
<p>VC维是衡量可以通过统计<strong>分类</strong>算法学习的函数空间的容量的度量。定义为算法可以破碎(shatter)的最大点集的基数。shatter的含义为，对于一个假设空间<span
class="math inline">\(\mathcal H\)</span>（函数的集合），如果存在<span
class="math inline">\(m\)</span>个数据样本能够被假设空间中的函数按照所有可能的<span
class="math inline">\(2^n\)</span>种方式分开（每个样本有两种类别可能），则称<span
class="math inline">\(\mathcal H\)</span>能够把<span
class="math inline">\(m\)</span>个数据样本破碎开。</p>
<p>e.g.
线性模型的VC维为3，因为3个点线性可分而4个点的xor情况线性不可分。</p>
<h3 id="泛化误差上界">泛化误差上界</h3>
<p>在推导特定问题的泛化误差上界之前，我们定性地描述泛化误差上界：</p>
<ul>
<li>样本容量越大，泛化误差上界趋于0；</li>
<li>假设空间容量越大，泛化误差上界越大；</li>
</ul>
<p>考虑二分类问题，样本容量为<span
class="math inline">\(N\)</span>，训练数据集<span
class="math inline">\(T\)</span>是从联合概率分布<span
class="math inline">\(P(X,Y)\)</span>独立同分布产生的，假设空间是函数的有限集合<span
class="math inline">\(\mathcal F = \{f_1, \cdots,
f_d\}\)</span>，损失函数为0-1损失，则期望风险（<span
class="math inline">\(L(Y, F(X))\)</span>期望值）和经验风险（<span
class="math inline">\(L(Y, F(X))\)</span>样本均值）分别为： <span
class="math display">\[
R(f)=E[L(Y,f(X))]
\]</span></p>
<p><span class="math display">\[
\hat R(f) = \frac 1 N \sum_{i=1}^N L(y_i, f(x_i))
\]</span></p>
<p>经验风险最小化函数为 <span class="math display">\[
f_N = \arg\min_{f\in\mathcal F}\hat R(f)
\]</span> 我们关心<span class="math inline">\(f_N\)</span>的泛化能力：
<span class="math display">\[
R(f_N)=E[L(Y,f_N(X))]
\]</span> 对于任意一个<span
class="math inline">\(f\)</span>，至少以概率<span
class="math inline">\(1-\delta, ~
0&lt;\delta&lt;1\)</span>，以下不等式成立： <span
class="math display">\[
R(f) \le \hat R(f) + \sqrt{\frac {1}{2N} (\log d+\log\frac 1 \delta)}
\]</span></p>
<blockquote>
<p>具体证明见： 李航《统计学习方法》。</p>
</blockquote>
<h2 id="向量矩阵张量的导数">向量、矩阵、张量的导数</h2>
<p>理解矢量导数，最朴素的方式，就是把矢量计算拆成多个并行的标量计算；</p>
<p>以下内容来自<a
href="http://cs231n.stanford.edu/vecDerivs.pdf">cs231n的参考资料</a>，用粗体字母表示向量，用大写字母表示矩阵，一般的小写字母表示标量；</p>
<h3 id="向量对向量求导">向量对向量求导</h3>
<p>设<span class="math inline">\(\mathbf{y}\)</span>是长度为<span
class="math inline">\(C\)</span>列向量，<span
class="math inline">\(W\)</span>是<span class="math inline">\(C\times
D\)</span>矩阵，<span class="math inline">\(\mathbf
x\)</span>是长度为<span class="math inline">\(D\)</span>列向量，有<span
class="math inline">\(\mathbf y = W\mathbf
x\)</span>，简单的排列组合知识告诉我们，<span
class="math inline">\(\frac{\partial\mathbf y}{\partial \mathbf
x}\)</span>有<span class="math inline">\(C\times
D\)</span>项（二维矩阵）；不妨以<span class="math inline">\(\mathbf
y\)</span>的第3个元素对<span class="math inline">\(\mathbf
x\)</span>的第7个元素为例： <span class="math display">\[
\begin{aligned}
\frac{\partial y_3}{\partial x_7} &amp;= \frac{\partial}{\partial
x_7}(W_{3,1}x_1+\cdots+W_{3,7}x_7+\cdots+W_{3,D}x_D)\\
&amp;=W_{3,7}
\end{aligned}
\]</span> 推广到其他位置，不难得出结论，<span
class="math inline">\(\frac{\partial\mathbf y}{\partial \mathbf
x}\)</span>恰好就是<span class="math inline">\(W\)</span></p>
<blockquote>
<p>如果<span class="math inline">\(\mathbf y\)</span>与<span
class="math inline">\(\mathbf x\)</span>都是行向量且<span
class="math inline">\(\mathbf y = \mathbf x W\)</span>，同样有<span
class="math inline">\(\frac{\partial\mathbf y}{\partial \mathbf
x}=W\)</span></p>
</blockquote>
<h3 id="向量对矩阵求导">向量对矩阵求导</h3>
<p>与上一小节条件相同，直观地看，<span
class="math inline">\(\frac{\partial\mathbf y}{\partial
W}\)</span>应该是一个<span class="math inline">\(C\times C\times
D\)</span>的张量，设<span class="math inline">\(W=[\mathbf w_1, \cdots,
\mathbf w_D]\)</span>，<span class="math inline">\(\mathbf y = W\mathbf
x\)</span>也可以理解为<span class="math inline">\(\mathbf
y\)</span>是<span class="math inline">\(W\)</span>各列以<span
class="math inline">\(\mathbf x\)</span>中对应元素为权重的加权和，即
<span class="math display">\[
\mathbf y = x_1\mathbf w_1 + \cdots + x_D \mathbf w_D
\]</span> 特殊的，我们可以得到 <span class="math display">\[
\frac{\partial y_3}{\partial W_{3,7}} = \frac{\partial}{\partial
W_{3,7}}(x_1W_{3,1} +\cdots + x_7W_{3,7} + \cdots + x_DW_{3,D}) = x_7
\]</span></p>
<p><span class="math display">\[
\frac{\partial y_3}{\partial W_{2,7}} = \frac{\partial}{\partial
W_{2,7}}(x_1W_{3,1} +\cdots + x_7W_{3,7} + \cdots + x_DW_{3,D}) = 0
\]</span></p>
<p>推广后，我们发现，对于<span
class="math inline">\(\frac{\partial\mathbf y_i}{\partial
W_{j,k}}\)</span>，仅仅在<span
class="math inline">\(i=j\)</span>时不为零，且<span
class="math display">\[\frac{\partial\mathbf y_i}{\partial
W_{i,k}}=x_k\]</span>，这样，我们可以将结果的3D张量表示成一个更紧凑的2D矩阵，形状为<span
class="math inline">\(C\times D\)</span>，（<span
class="math inline">\(C\)</span>个<span class="math inline">\(\mathbf
x\)</span>拼接）： <span class="math display">\[
\frac{\partial\mathbf y}{\partial W} = \mathbf x \mathbf 1^T
\]</span></p>
<blockquote>
<p>当然，结果还可以继续“紧凑”成向量<span class="math inline">\(\mathbf
x\)</span>，这里我们有意让<span
class="math inline">\(\frac{\partial}{\partial W}\)</span>的形状与<span
class="math inline">\(W\)</span>相同，是利于理解反向传播过程中参数的更新，当然，利用numpy/pytorch的广播机制，只需要保存一个向量即可；</p>
</blockquote>
<h3 id="矩阵对矩阵求导">矩阵对矩阵求导</h3>
<p>将先前定义的<span class="math inline">\(\mathbf y\)</span>和<span
class="math inline">\(\mathbf x\)</span>分别扩展成矩阵<span
class="math inline">\(Y\in \mathbb R^{C\times N}\)</span>，<span
class="math inline">\(X\in \mathbb R^{D\times N}\)</span>，<span
class="math inline">\(Y=WX\)</span>；这种扩展，可以理解为使用了<span
class="math inline">\(N\)</span>个数据，对于<span
class="math inline">\(i = 1, \cdots, N\)</span>，有<span
class="math inline">\(\mathbf y_i = W \mathbf x_i\)</span>；</p>
<p>使用先前的结论，有 <span class="math display">\[
\frac{\partial Y_{.,i}}{\partial X_{.,i}} = \frac{\partial \mathbf
y_i}{\partial \mathbf x_i} = \frac{\partial}{\partial \mathbf
x_i}W\mathbf x_i = W
\]</span></p>
<p><span class="math display">\[
\frac{\partial Y_{.,i}}{\partial X_{.,j}} = \frac{\partial \mathbf
y_i}{\partial \mathbf x_j} = \frac{\partial}{\partial \mathbf
x_j}W\mathbf x_i = 0
\]</span></p>
<p>将矩阵乘法展开成标量乘积之和形式，还可以得到<span
class="math inline">\(\frac{\partial Y_{a,b}}{\partial
X_{c,d}}\)</span>在<span class="math inline">\(a\ne
c\)</span>时也为0，因此紧凑地表示<span
class="math inline">\(\frac{\partial Y}{\partial
X}\)</span>，可以将4D张量缩减为2D矩阵，也就是<span
class="math inline">\(W\)</span>；</p>
<p>类似的，<span class="math inline">\(\frac{\partial Y}{\partial W} =
X\)</span>；</p>
<blockquote>
<p>这个结果，在形式上，又与标量求导相同，只不过我们需要清楚，这些都是“紧凑”后的形式；</p>
</blockquote>
<h2 id="线性模型">线性模型</h2>
<h3 id="线性判别分析">线性判别分析</h3>
<p>核心思想：将高维空间的样本投影到一条直线上，使得同类样本尽可能近，异类样本尽可能远；</p>
<h4 id="如何表示投影">如何表示投影</h4>
<p>如何表示投影？在<a
href="https://ashun989.github.io/2022/08/10/linear-algebra/">线性代数要点总结</a>的“正交投影”一节，给出了将<span
class="math inline">\(\mathbb{R}^n\)</span>中的向量<span
class="math inline">\(y\)</span>投影到子空间<span
class="math inline">\(W\)</span>后的数学表示，设<span
class="math inline">\(U=[u_1,\cdots,
u_p]\)</span>为该子空间的单位正交基，则 <span class="math display">\[
\text{proj}_Wy=UU^Ty
\]</span> 在西瓜书上的图解是一个最简单的情况，原空间是<span
class="math inline">\(\mathbb
R^2\)</span>，投影子空间是其中的一条直线（1维子空间），其单位正交基应该只有一个向量，即该直线的单位向量<span
class="math inline">\(\omega\)</span>，按照上述公式，投影后的点坐标应该是<span
class="math inline">\(\omega\omega^Tx\)</span>，为什么后续却适用了<span
class="math inline">\(\omega^Tx\)</span>这一表示呢？</p>
<p><img src="image-20221027141501670.png" /></p>
<p>直观地看，在上图中，<span
class="math inline">\(\omega\omega^Tx\)</span>仍然保持了与<span
class="math inline">\(x\)</span>相同的形状，而<span
class="math inline">\(\omega^Tx\)</span>是一个标量；向量空间<span
class="math inline">\(\mathbb R^n\)</span>的子空间<span
class="math inline">\(W\)</span>有<span
class="math inline">\(p\)</span>个线性无关的基，即子空间是<span
class="math inline">\(p\)</span>维的，但每个基的元素个数为<span
class="math inline">\(n\)</span>，即子空间的域仍然是<span
class="math inline">\(p\)</span>，即在线代中介绍的投影运算，在投影前后不改变域，而在这里介绍的“投影”，希望得到的是与该子空间同构的<span
class="math inline">\(\mathbb
R^p\)</span>上的结果，而线性判别分析的限制更强，即<span
class="math inline">\(p=1\)</span>；</p>
<p>回想投影公式的推导过程，<span
class="math inline">\(U^Ty\)</span>是<span
class="math inline">\(p\)</span>个正交的单位向量与<span
class="math inline">\(y\)</span>的点积结果，表示<span
class="math inline">\(y\)</span>投影到子空间各个方向上的长度是多少，之后再左乘<span
class="math inline">\(U\)</span>是为了得到投影点在<span
class="math inline">\(\mathbb{R}^n\)</span>中的坐标；</p>
<p>现在我们只需要知道在1维子空间的唯一基方向的长度<span
class="math inline">\(y=\omega^Tx\)</span>，用于分类，更准确的说，我们只要得到不同的<span
class="math inline">\(x\)</span>，投影后向相对长度，因此<span
class="math inline">\(\omega\)</span>不必要是一个方向向量，不同的<span
class="math inline">\(\omega\)</span>只会使得所有投影结果乘上不同的比例因子罢了；</p>
<h4 id="如何设计目标函数">如何设计目标函数</h4>
<p>首先定义一些接下来会用到的符号，总样本集合<span
class="math inline">\(X\)</span>包含<span
class="math inline">\(N\)</span>个<span
class="math inline">\(d\)</span>维样本<span
class="math inline">\(x\)</span>，即<span
class="math inline">\(X\)</span>是<span class="math inline">\(d\times
N\)</span>的矩阵，记类别数为<span
class="math inline">\(K=2\)</span>，第<span
class="math inline">\(i\)</span>类样本的集合为<span
class="math inline">\(X_i\)</span>，基数为<span
class="math inline">\(N_i\)</span>，均值为<span
class="math inline">\(m_i\)</span>，协方差为<span
class="math inline">\(\Sigma_i\)</span>；投影后的均值为<span
class="math inline">\(\omega m_i\)</span>，协方差矩阵<span
class="math inline">\(\omega^T\Sigma\omega\)</span>；（这里投影后的协方差矩阵是1x1的，因此可以视为一个标量）</p>
<blockquote>
<p>有关协方差和投影后协方差矩阵的形式，可见<a
href="https://ashun989.github.io/2022/08/10/linear-algebra/">线性代数要点总结</a>的主成分分析一节；</p>
</blockquote>
<p>希望类间距离越大，可以写成样本中心距离越大；希望类内距离越小，可以写成协方差越小，则目标函数（最大化）为：
<span class="math display">\[
\begin{aligned}
J &amp;= \frac{||\omega^Tm_0 - \omega^T
m_1||_2^2}{\omega^T\Sigma_0\omega + \omega^T\Sigma_1\omega}\\
&amp;= \frac{\omega^T(m_0 - m_1)(m_0 -
m_1)^T\omega}{\omega^T(\Sigma_0+\Sigma_1)\omega}\\
\end{aligned}
\]</span> 把分子分母中做个变量代换，定义：</p>
<p>类间散度矩阵为<span class="math inline">\(S_b\)</span>，<span
class="math inline">\(d\times d\)</span> <span class="math display">\[
S_b = (m_0-m_1)(m_0-m_1)^T
\]</span> 分母上 <span class="math display">\[
\begin{aligned}
\Sigma_0 +\Sigma_1 &amp;= \frac{1}{N_0-1}(X_0-m_0\mathbf
1)(X_0-m_0\mathbf 1)^T + \frac{1}{N_1-1}(X_1-m_1\mathbf
1)(X_1-m_1\mathbf 1)^T\\
&amp;= \frac{1}{N_0-1}\sum_{x\in X_0}(x-m_0)(x-m_0)^T +
\frac{1}{N_1-1}\sum_{x\in X_1}(x-m_1)(x-m_1)^T
\end{aligned}
\]</span> 而在定义类内散度矩阵<span
class="math inline">\(S_w\)</span>，<span class="math inline">\(d\times
d\)</span>时， 抛去了归一化项： <span class="math display">\[
S_w = \sum_{x\in X_0}(x-m_0)(x-m_0)^T + \sum_{x\in X_1}(x-m_1)(x-m_1)^T
\]</span> 上述目标函数重写为： <span class="math display">\[
J = \frac{\omega^TS_b\omega}{\omega^TS_w\omega}
\]</span></p>
<blockquote>
<p>抛去了归一化项会有什么影响吗？直观地，正负样本的数量不同时，会有差距，不做归一化会使得<span
class="math inline">\(S_w\)</span>偏向样本多得那一方。可是求平均同样会放大样本较少类别中离群点的影响。对于类别不均衡问题，LDA算法本身没有考虑，但是后续可以通过难样本挖掘等采样策略来训练等等；</p>
</blockquote>
<h4 id="如何优化">如何优化</h4>
<p>如何求解使<span class="math inline">\(J\)</span>最大的<span
class="math inline">\(\omega\)</span>？分子分母都是关于<span
class="math inline">\(\omega\)</span>的二次型；由于我们只需要<span
class="math inline">\(\omega\)</span>的方向，因此可以让分母为非零常数<span
class="math inline">\(c\)</span>，分子最大化；</p>
<blockquote>
<p>为了便于理解，在上图中原始空间为<span class="math inline">\(\mathbb
{R}^2\)</span>的情况下，记<span class="math inline">\(\omega =
(\omega_1, \omega_2)\)</span>，<span
class="math inline">\(J\)</span>也就是这样的一个目标函数： <span
class="math display">\[
J =
\frac{a\omega_1^2+b\omega_2^2+c\omega_1\omega_2}{d\omega_1^2+e\omega_2^2+f\omega_1\omega_2}
\]</span></p>
</blockquote>
<p><span class="math display">\[
\begin{matrix}
    \min_w &amp;-\omega S_b\omega\\
\text{s.t.}&amp;\omega^TS_w\omega = c
\end{matrix}
\]</span></p>
<p>对应的拉格朗日函数为： <span class="math display">\[
L(\omega, \lambda) = \omega^T S_b\omega - \lambda(\omega^TS_w\omega - c)
\]</span> 对<span
class="math inline">\(\omega\)</span>求导并令导数为0，得到 <span
class="math display">\[
S_b\omega  = \lambda S_w\omega
\]</span> LDA假设样本的协方差矩阵是满秩的，因此有<span
class="math inline">\(S_w\)</span>非奇异，则 <span
class="math display">\[
S_w^{-1}S_b\omega = \lambda \omega
\]</span></p>
<blockquote>
<p>在Fisher判别分析中，没有协方差矩阵满秩的假设，因此<span
class="math inline">\(S_w^{-1}\)</span>可以使用求伪逆的方法；</p>
</blockquote>
<p>转化为求解<span
class="math inline">\(S_\omega^{-1}S_b\)</span>的特征向量的问题；但是，又观察到<span
class="math inline">\((m_0-m_1)^T\omega\)</span>是标量，则<span
class="math inline">\(S_b\omega\)</span>始终与<span
class="math inline">\((m_0-m_1)\)</span>同方向： <span
class="math display">\[
S_b\omega = (m_0-m_1)(m_0-m_1)^T\omega = \lambda&#39;(m_0-m_1)
\]</span> 因此 <span class="math display">\[
\omega = \frac{\lambda}{\lambda&#39;}S_w^{-1}(m_0-m_1)
\]</span> 比例因子<span class="math inline">\(\lambda /
\lambda&#39;\)</span>可以忽略，因为我们只在意方向；</p>
<h4 id="如何分类">如何分类</h4>
<p>对于二分类问题，在投影到1维直线上之后，我们需要确定一个阈值来在推理时区分正负类别，最简单的，可以使用投影后的样本中心的均值作为阈值；</p>
<h4 id="如何扩展到多分类">如何扩展到多分类</h4>
<p>观察上述过程，扩展到多分类，仍要满足类内距离小，那么<span
class="math inline">\(S_w\)</span>的定义只要由两类协方差矩阵之和扩展成多类协方差矩阵之和即可；
<span class="math display">\[
S_w = \sum_{i=1}^{K}\sum_{x\in X_i}(x-m_i)(x-m_i)^T
\]</span></p>
<p>仍要满足类间距离大，那么<span
class="math inline">\(S_b\)</span>的定义该如何扩展？</p>
<p>先定义一个全局散度矩阵<span class="math inline">\(S_t\)</span>，<span
class="math inline">\(m\)</span>表示所有样本的均值： <span
class="math display">\[
S_t = \sum_{i=1}^{K}\sum_{x\in X_i}(x-m)(x-m)^T
\]</span> 令<span class="math inline">\(S_b = S_t -
S_w\)</span>，则有类间样本均值：</p>
<blockquote>
<p><span class="math display">\[
\begin{aligned}
(x-m)(x-m)^T - (x-m_i)(x-m_i)^T
&amp;=(x-m)(x^T-m^T) - (x-m_i)(x^T-m_i^T)\\
&amp;=-mx^T-xm^T+mm^T+m_ix^T+xm_i^T-m_im_i^T\\
\sum_{x\in X_i}(-mx^T-xm^T-mm^T+m_ix^T+xm_i^T-m_im_i^T) &amp;=
N_i(mm^T-m_i^Tm_i^T)+(m_i-m)N_im_i^T + N_im_i(m_i^T-m^T)\\
&amp;= N_i(mm^T + m_i^Tm_i^T - mm_i^T - m_im^T)\\
&amp;= N_i(m_i - m)(m_i - m)^T
\end{aligned}
\]</span></p>
</blockquote>
<p><span class="math display">\[
S_b = \sum_{i=1}^{K}N_i(m_i-m)(m_i-m)^T
\]</span></p>
<p>扩展到多分类的另一个问题在于，投影到1维直线上已经不太合适了，<span
class="math inline">\(d\times 1\)</span>的投影向量<span
class="math inline">\(\omega\)</span>扩展为<span
class="math inline">\(d\times (K-1)\)</span>的投影矩阵<span
class="math inline">\(W\)</span>，但是这样一来，目标函数的分子分母都不再是标量而是矩阵，可以改成求矩阵的迹</p>
<blockquote>
<p>协方差矩阵的迹是总方差</p>
</blockquote>
<p><span class="math display">\[
\max_{W}\frac{\text{tr}(W^TS_bW)}{\text{tr}(W^tS_wW)}
\]</span></p>
<p>上述问题的解与之前有相同形式（笔者目前还不能给出推导过程）： <span
class="math display">\[
S_bW = \lambda S_w W
\]</span> 问题同样转化为求<span
class="math inline">\(S_w^{-1}S_b\)</span>的前<span
class="math inline">\(d&#39;\)</span>个最大广义非零特征值对应的特征向量，<span
class="math inline">\(d&#39; \le (K - 1)\)</span>；</p>
<p>在推理时，设<span
class="math inline">\(y=Wx\)</span>是测试样本在投影空间的表示，计算<span
class="math inline">\(y\)</span>到哪个投影后的类别中心<span
class="math inline">\(Wm_i\)</span>最近，决定了<span
class="math inline">\(x\)</span>属于哪个类别；</p>
<blockquote>
<p>为什么<span class="math inline">\(K\)</span>分类投影到<span
class="math inline">\(K-1\)</span>的超平面？还是别的维度也可以？</p>
<p>不存在什么限制要求必须投影到$K-1维超平面</p>
</blockquote>
<h4 id="扩展到降维">扩展到降维</h4>
<p>上面在多分类的扩展中，我们看到了LDA可以作为监督降维技术，并且这种投影过程使用了类别信息；</p>
<h3 id="逻辑斯蒂回归对数几率回归">逻辑斯蒂回归/对数几率回归</h3>
<p>一句话描述逻辑斯蒂回归，就是在线性回归层之后使用了sigmoid函数，并设置阈值完成二分类预测，相比于直接用线性模型+阈值做分类，能够一定程度减少离群点的影响；</p>
<p>如何优化逻辑斯蒂回归模型的参数？我想起来在吴恩达老师的机器学习入门课中，给了一个新手友好的解释：</p>
<blockquote>
<p>线性回归模型 + MSE损失，和逻辑回归模型 +
BCE损失，求导后损失函数对<span
class="math inline">\(\theta\)</span>的梯度形式均为Error * Feature</p>
</blockquote>
<p>逻辑回归模型为什么引入CE？又为什么不用MSE？吴恩达老师只是简单提到了最大似然估计和非凸函数，没有给当时还是新手的我讲太多；下面笔者记录一下，是怎样得到BCE损失函数的？</p>
<h4 id="从统计意义解释">从统计意义解释</h4>
<p>决策边界：使得模型输出=分类阈值的所有输入的集合，设输入特征是<span
class="math inline">\(\mathbb R^d\)</span>中的向量，则决策边界是<span
class="math inline">\(d-1\)</span>维的超平面；对于逻辑回归，其决策边界是线性的；</p>
<blockquote>
<p>如果阈值为0.5，那么超平面即<span
class="math inline">\(wx+b=0\)</span>，将偏置项"吸入"，也就是<span
class="math inline">\(wx=0\)</span></p>
</blockquote>
<p>使用sigmoid激活之后，模型的输出可以视为一种后验概率： <span
class="math display">\[
\begin{aligned}
P(Y=1|X) &amp;= \frac{e^{wx}}{1+e^{wx}} &amp;= p\\
P(Y=0|X) &amp;= \frac{1}{1+e^{wx}} &amp;=  1 - p\\
\end{aligned}
\]</span>
定义事件的几率为发生与不发生的比率，对数几率即几率取对数，那么逻辑斯蒂回归模型的对数几率，即为线性函数的输出：
<span class="math display">\[
\log\frac{P(Y=1|X)}{P(Y=0|X)} = wx
\]</span>
也就是说，虽然披着“分类”的外皮，但是我们还是在做一个回归任务，只不过，回归的目标是“对数几率”；</p>
<p>设输入数据为<span class="math inline">\(\{x_i,
y_i\}_{i=1}^m\)</span>，<span
class="math inline">\(y_i\)</span>为0或1，则对数似然函数为 <span
class="math display">\[
l(w) = \sum_{i=1}^m\log p(y_i|x_i; w)
\]</span> 按照似然函数的定义，其中： <span class="math display">\[
p(y_i|x_i;w) = p_i^{y_i}(1-p_i)^{1-y_i}
\]</span> 则 <span class="math display">\[
\begin{aligned}
l(w) &amp;= \sum_{i=1}^{m}[y_i\log p_i + (1-y_i)\log(1-p_i)]\\
&amp;= \sum_{i=1}^{m}[y_i\log\frac{p_i}{1-p_i} + \log(1-p_i)]\\
&amp;= \sum_{i=1}^{m}[y_i(wx_i)-\log(1+e^{wx})]
\end{aligned}
\]</span> 上式的第一行也就是常见的BCE损失的表现形式，求<span
class="math inline">\(\frac{\partial l}{\partial
w}\)</span>即我们熟悉的Error * Feature形式；</p>
<h4 id="扩展到多分类">扩展到多分类</h4>
<p>同样的，写出对数似然函数，仍然能够得到CE损失的形式：</p>
<p><img src="image-20221027212853259.png" /></p>
<p><img src="image-20221027212912620.png" /></p>
<p><a
href="https://ashun989.github.io/2022/08/30/Stable-Calculations/#more">这里</a>有一份softmax+CE
loss的求导过程；</p>
]]></content>
  </entry>
  <entry>
    <title>目标检测论文阅读列表</title>
    <url>/2022/10/20/Object-Detection-Papers/</url>
    <content><![CDATA[<blockquote>
<p>目标检测与语义分割是类似的任务</p>
</blockquote>
<span id="more"></span>
<h5 id="r-cnn">R-CNN</h5>
<p>深度学习在目标检测任务上开创性的工作，预训练+微调的早期尝试。</p>
<p>多阶段算法：提取候选框--&gt;剪裁变形侯送入分类器--&gt;对每个候选区域进行分类（SVM对每个类进行评分）--&gt;后处理（NMS+边界框回归）</p>
<p>在ILSVRC2012上进行预训练，在VOC和ILSVRC2013上微调，微调时数据集有回归框，使用提议候选区的算法产生正负样本，并且进行了一定的变形，这样微调的目的是让分类器学会区分背景、能够识别扭曲的图像（特征提取网络只能接受固定尺寸的输入，在检测时每个候选框）。</p>
<p>训练SVM时，使用了难样本挖掘技术使其快速收敛。</p>
<p>作者还在文中做出了很多讨论，比如如何定义正例、负例、无用样例，为什么微调和训练SVM时的正例和负例不一样，为什么不用微调后的多分类head而是用SVM做分类，如何对候选框进行形变，如何利用CNN的特征做<span
class="math inline">\(x, y, w,
h\)</span>的线性回归，做回归时候如何选择训练对，训练class-specific的回归器等。</p>
<p>正如后来的论文指出的，这篇论文的问题是，对于每个候选框，都要做特征提取和分类（其中很多重复计算），并且使用SVM的多分类还是一对一的，Selective
Search的候选框提议算法也占用了较多时间，导致检测效率很低。</p>
<h5 id="sppnet">SPPnet</h5>
<p>本文提出的空间金字塔池化层：让池化窗口大小与输入图像尺寸成正比，得到恒定的窗口数量，并设计多种空间尺度的池化，解决了CNN能接受多种尺寸的输入而全连接层只能接受定长输入的gap，增强了模型对驶入尺寸的鲁棒性，并使得多尺寸训练和多尺寸测试成为了可能，再次增强了性能。为了充分利用GPU的并行性，多尺度训练是一epoch换一个尺度。测试时，作者也对full-view,
crop-view, multi-scale + full/crop
view做了很多实验，在ImageNet2012上，multi-scale +
2full的测试配置获得了最好的结果。</p>
<p>对于目标检测任务，作者改进了R-CNN中一个候选区计算一次特征的情况，而是在全图的特征图上，找到候选区映射的特征窗口，并且使用全局金字塔模块得到定长的特征后送入后续分类头，这大大提高了R-CNN的效率，而多尺度的特征提取技巧等，又提升了性能。</p>
<p>后来的论文认为，SPPnet在微调时固定了卷积层参数，只微调全连接层，且训练SVM时使用固定的特征，这限制了其性能，尤其是当使用更深的网络时。</p>
<h5 id="fast-r-cnn">Fast R-CNN</h5>
<p>本文在R-CNN和SPPnet上进一步改进，将之前bbox相对位置回归，预测类别分布概率，以及特征提取都放进了一个网络里，用了一个multi-task
loss完成分类和回归任务。本文提出了一种RoI池化方法，这是SPP的一种特殊情况，每个RoI记录<span
class="math inline">\((n, r, c, h,
w)\)</span>，并使用与输入图片RoI尺寸成正比的窗口大小，进行最大池化，得到固定长度的特征用于全连接层。</p>
<p>对于预训练网络，要修改其最后一个池化层为RoI池化层，修改其分类头为<span
class="math inline">\(K+1\)</span>分类头与回归头，修改其输入为<span
class="math inline">\(N\)</span>张图片的输入batch和<span
class="math inline">\(R\)</span>个感兴趣区域的输出batch。在微调阶段，由于一张图片可能有上千张候选区，作者介绍了其从候选区中采样RoI正负例的方法，层级式的采样保证每张图采样同等数量的RoI。在检测阶段，对于每个RoI，都会输出一个类别后验概率（各个类别概率和为1，相比于之前每个类预测一个概率，这引入了类别之前竞争），以及各个类别的<span
class="math inline">\((t_x, t_y, t_w, t_h)\)</span></p>
<p>作者还探索了诸如使用截断SVD压缩全连接层，在微调时冻结多少参数，多任务vs单任务vs分段训练，暴力多尺度（将输入图像缩放到多个尺寸，增加了开销）vs单尺度的训练推理（希望网络自己有近似的尺度不变性）；softmax
vs SVM；候选区算法的配置等；</p>
<p>后续论文指出，使用ss或者edgebox等候选区提议算法仍然是主要的瓶颈所在。</p>
<p><strong>Faster RCNN</strong></p>
<p>本文希望使用全卷机神经网络预测提议区域，即RPN，并且这个网络与用于目标检测的网络应当共享特征提取的参数。对于RPN，接受任意尺寸的图像作为输入，使用CNN提取特征，在最后一层特征图上使用<span
class="math inline">\(n\times n\)</span>的滑动窗口，映射到一个<span
class="math inline">\(d\)</span>维向量，作为这个窗口的语义信息，此外，为了获得多尺寸的提案，预先定义3种尺度和3种长宽比共9种anchor，更一般地，记为<span
class="math inline">\(k\)</span>种anchor，则滑动窗口的<span
class="math inline">\(d\)</span>维向量接着会用于预测<span
class="math inline">\(2k\)</span>的objectness score和<span
class="math inline">\(4k\)</span>个相对坐标；</p>
<blockquote>
<p>先前R-CNN，Fast
R-CNN都是用全图的特征来预测bbox，而这里每个滑动窗口的特征图只用了<span
class="math inline">\(n\times n\)</span>，而且其<span
class="math inline">\(k\)</span>个anchor的预测又不共享参数，旨在获得丰富的形状。</p>
</blockquote>
<p>并不是所有的anchor都会用于训练RPN，使用IoU来选择正负例，且仍然存在的负例个数大于正例的问题，使用了采样方法控制反向传播的正负例为1:1；</p>
<p>上述训练的RPN只是区域提议网络，剩下的检测网络的预训练、微调可以与Fast
R-CNN类似，然而让这两者共享特征层不那么容易，因为不清楚不断迭代的候选区送入不断迭代的检测网络，最后能不能收敛到好的结果？因此文中给出了一种方法。</p>
<h5 id="mask-r-cnn">Mask R-CNN</h5>
<p>同时完成目标检测和实例分割。在Faster R-CNN基础上，修改了原来RoI
Pooling为更精确的RoI Align，为检测网络添加了一个分支，用于预测<span
class="math inline">\(K\)</span>个类别的mask。</p>
<p><a href="https://zhuanlan.zhihu.com/p/73138740">知乎 - RoI
Pooling和RoI Align</a></p>
<h5 id="yolov1">YOLOv1</h5>
<p>单阶段目标检测，即不需要滑动窗口、候选区的步骤，直接对全局进行编码，类似一个回归器预测目标框和类别概率，优点在于速度更快，更少地将背景错判为物体，但是定位精度不够好，尤其是小物体。</p>
<p>方法上，将图像分成<span class="math inline">\(S\times
S\)</span>的网格，每个网格预测<span
class="math inline">\(B\)</span>个物体框，包含objectness分数和<span
class="math inline">\(x, y, w, h\)</span>，每个网格预测<span
class="math inline">\(C\)</span>个类别的得分；推理时，将每个物体框的置信度与所在网格的C类别置信度相乘，可以得到哪些框最贴合哪一类物体；</p>
<p>具体的，预训练一个ConvNet，之后添加了额外的随机初始化的卷积层和全连接层，映射到一个<span
class="math inline">\(S\times S \times
(5B+C)\)</span>的输出。使用均方差损失，但是由于大量的bbox应该都是无物体的背景，没有回归损失项，因此在损失函数上，给包含物体的boxes赋予更大的权重；考虑到<span
class="math inline">\(w,h\)</span>的预测误差对于小boxes更严重，因此预测<span
class="math inline">\(\sqrt w\)</span>与<span
class="math inline">\(\sqrt h\)</span>（在<span
class="math inline">\(\sqrt x\)</span>对于较大的<span
class="math inline">\(x\)</span>有较小的梯度）；</p>
<p>除了定位不准，YOLOv1还有一个问题就是，召回率较低；</p>
<blockquote>
<p>【存疑】：在训练时，每个回归框负责预测哪个物体，是由其与gt框的IoU决定的，根据笔者的理解，在训练迭代过程中，这种绑定关系可能会发生改变？也就是每一次前向过程之前都要计算这种绑定关系？那么在推理时没有了gt框，就直接用前向输出的结果作为置信度，选择最高的那个作为一个box绑定的一个object，但是论文中也提到推理时会出现重复检测？</p>
<p>需要阅读代码；</p>
</blockquote>
<h5 id="yolov2">YOLOv2</h5>
<p>相比于YOLOv1，使用了如下trick获得更好的结果：</p>
<ul>
<li>使用BN，提高了mAP；</li>
<li>使用两次微调，一次是为了适应高分辨率，一次是适应检测任务，提高了mAP；</li>
<li>将YOLOv1中，预测<span class="math inline">\(S\times S \times
B\)</span>个回归框的全连接层换成Faster
R-CNN中预测大量anchor的全卷积FPN，并将分辨率改为416x416使得最后一层特征图为奇数边长，从而有位于中心的anchor，提高了mAP；</li>
<li>RPN中不在手工地预先定义anchor初始尺寸，而是使用训练集中的聚类结果，聚类的类别数、距离度量在文中有讨论；</li>
<li>在R-CNN系列中预测anchor相对于滑动窗口中心的偏移量，这里YOLOv1中预测anchor相对于网格的相对偏移，并且使用sigmoid约束偏移不超过1个像素（特征图上的）</li>
<li>相比于利用多尺度特征图，YOLOv1直接在最后一个<span
class="math inline">\(13\times
13\)</span>的特征图上进行检测，并在通道维度拼接了一个passthrough层，由<span
class="math inline">\(26\times
26\)</span>剪裁成4份得到；（笔者不清楚，这样做不会破坏空间位置关系吗？）</li>
<li>使用多尺度训练；</li>
</ul>
<p>论文还设计了层次的分类，在结合多个数据集的情况下做检测，即YOLO9000；</p>
<h5 id="ssd">SSD</h5>
<p>也是单阶段目标检测算法，即消除了候选框提议和特征图上重采样的过程。SSD的核心是在多层特征图上使用小型卷积来预测一组固定的默认bbox类别分数和偏移量，用另一些卷积核预测宽高比；</p>
<h5 id="dssd">DSSD</h5>
<h5 id="fpn">FPN</h5>
<p>空间尺度不变性对于识别、检测任务都是十分重要的。先前的方法使用过多尺度的输入图像，这种方法可以得到多个分辨率的富语义特征图，但显著增加了训练/推理时的时间成本；还有的方法使用特征提取网络中不同阶段得到不同分辨率的特征图组成特征金字塔，但是由于不同阶段的特征之间存在语义gap，如浅层的low-level特征会损害识别精度；如何根据单个输入图像构造一个特征金字塔（不显著增加成本），并且在各层都有丰富的语义，各层特征都能用于目标检测，是本文探索的目标。</p>
<p>FPN模块将高级的语义特征图上采样，并使用1x1卷积压缩特征维度后，与较低级别的特征图逐元素相加，从而得到新的一组特征金字塔，在各个分辨率下都有不错的语义表示。接着，作者改进了RPN，让滑动窗口在融合后的特征金字塔上产生anchor；作者还改进了Fast
R-CNN，为了利用多尺度特征，对于越小的RoI区域，映射到越精细的特征图上。</p>
<p>实验部分证明了FPN效果很好。</p>
<h5 id="ohem">OHEM</h5>
<h5 id="r-fcn">R-FCN</h5>
<h5 id="retinanet">RetinaNet</h5>
<p>单阶段目标检测方法，作者观察到之前的单阶段方法比不过两阶段方法的一个原因是，在单阶段的密集检测过程中，前景和背景的样本十分不均衡。在多阶段方法中，这一点可以通过设计对候选区的划分、采样策略解决；作者修改了交叉熵损失函数，得到Focal
loss，抑制了well-claassified
examples的损失，防止大量的负样本在训练期间压倒检测器。</p>
<p>RetinaNet使用了FPN做特征融合，相比于FPN论文中的方式，做了一些改动：其输出的特征图为<span
class="math inline">\(P_3\)</span>到<span
class="math inline">\(P_7\)</span>，均为256维，其中<span
class="math inline">\(P_3\)</span>到<span
class="math inline">\(P_5\)</span>是按照FPN的方式将ResNet特征图的<span
class="math inline">\(C_3\)</span>到<span
class="math inline">\(C_5\)</span>进行融合，而<span
class="math inline">\(P_6\)</span>是在<span
class="math inline">\(C_5\)</span>上应用步长2的3x3卷积得到的，<span
class="math inline">\(P_7\)</span>是在<span
class="math inline">\(P_6\)</span>上应用步长2的3x3卷积+ReLU得到的；希望这两个level能够用于预测超大物体；</p>
<p>RetinaNet的Head仍然是一个anchor-based，密集预测的全卷积网络，可以简单地类比为多分类的RPN；他有两个subnet，分别用于分类和回归；分类网络使用多层3x3卷积+ReLU，后跟一个映射到<span
class="math inline">\(KA\)</span>输出维度的卷积和sigmoid激活层，即对每个位置，预测<span
class="math inline">\(A\)</span>个anchor的<span
class="math inline">\(K\)</span>个类别得分；其中，与Faster
R-CNN类似的，<span class="math inline">\(A=3\times
3\)</span>；回归网络与分类网络有着类似的结构，除了最后的卷积层输出维度为<span
class="math inline">\(4A\)</span>，这样的设置与先前的很多工作不同：多个类别使用相同的回归器，作者实验发现在这里这样做能够节省参数并获得同样好的结果；</p>
<p>与RPN相比，这种Head设计更重，且分类子网和回归子网没有共享参数；</p>
<p>Head在多个level的特征图上是共享的；</p>
<blockquote>
<p>一个细节问题：为什么共享head不能使用共享BN？</p>
<p>在训练时，BN的running_mean和running_var是不断积累的，而各个feature的数据并不是一批地送入共享head中，而是for循环多次送入，这就意味着，在训练时，各个head统计的均值和方差是不同的；但是再推理时，BN中的均值和方差固定，即对于所有head使用相同的均值方差，这就造成了训练和推理时的不一致问题。</p>
<p><a href="https://zhuanlan.zhihu.com/p/406651553">知乎 -
共享Head为什么不是用BN层</a>解释了为什么要共享head，以及不能共享BN的原因；<a
href="https://www.bilibili.com/video/BV1bB4y1F7xr/">bilibili -
共享Head中为什么不能用BN</a>也给出了很好的解释；<a
href="https://github.com/open-mmlab/mmdetection/blob/master/mmdet/models/dense_heads/retina_sepbn_head.py">mmdetection</a>中有一个共享Head但是不共享BN的实现；</p>
</blockquote>
<p>为了证明Focal
Loss是好用的，作者对于使用IoU策略划分出的正负例anchor，没有使用任何启发式的或者硬负挖掘来对anchors进行采样；损失归一化时，使用assigned的anchors的数量（正例的数量）而非所有anchors的数量来归一化，因为在FocalLoss的作用下，那些容易的负例的损失可以忽略不计；</p>
<p>一个看似不起眼但是很重要的trick是，在初始化分类网络的最后一层时，bias不能初始化为全0，让每个anchor都以一定置信度称为前景，不然开始时过大的loss很容易发散；</p>
<p>YOLOv3</p>
<p>YOLOv4</p>
<p>YOLOv5</p>
<p>YOLOv6</p>
<p>YOLOv7</p>
]]></content>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title>Non-local网络回顾</title>
    <url>/2022/09/21/Non-local/</url>
    <content><![CDATA[<img src="/2022/09/21/Non-local/image-20220921200450948.png" class="">
<p><strong>论文</strong>：Non-local Neural Networks</p>
<p><strong>作者</strong>：<em>Xiaolong Wang, Ross Girshick, Abhinav
Gupta, Kaiming He</em></p>
<p><strong>录用情况</strong>：CVPR'2018</p>
<p><strong>第一作者单位</strong>：Carnegie Mellon University，Facebook
AI Research</p>
<p>本文提出的Non-Local是捕获长距离信息的，这篇文章主要在视频分类任务上做实验（Kinetics,
Charades），但在静态图像方面，也在COCO上进行了实例分割、检测和姿态估计的实验。</p>
<p>笔者认为，Non-Local在计算attention的方法上有所扩展，并在计算attention之前使用池化方法减小计算量之外，与self-attention没有什么不同的。</p>
<blockquote>
<p>当然，在当时的情况下，Self-Attention还在NLP领域内玩，而且作者提出他们的insight来自于一种经典的图像去噪算法<a
href="https://en.wikipedia.org/wiki/Non-local_means">Non-local
mean</a>。</p>
</blockquote>
<span id="more"></span>
<h2 id="整体形式">整体形式</h2>
<p>与Non-local mean一样的，作者希望将如下公式设计到网络结构中去： <span
class="math display">\[
\mathbf{y}_i=\frac{1}{\mathcal{C}(\mathbf x)}\sum_{\forall j}f(\mathbf
x_i, \mathbf x_j)g(\mathbf x_j)
\]</span> 其中，<span class="math inline">\(\mathbf
x\)</span>是输入信号，<span class="math inline">\(\mathbf
y\)</span>是与之形状相同的输出信号，<span
class="math inline">\(i,j\)</span>是空间位置上的索引，<span
class="math inline">\(f\)</span>是度量相似度的函数，<span
class="math inline">\(g\)</span>是一种从源空间到度量空间的映射，<span
class="math inline">\(\mathcal C(\mathbf
x)\)</span>是归一化项。其含义可以理解为，对于每个位置上的输入信号，都与空间上其他位置的信号计算一个相似度，作为权值，计算在全局上的加权平均作为输出。</p>
<p>相比之下，卷积操作，一个像素只能与其邻域范围内的像素直接相关；而全连接（即矩阵乘法）也是一种全局的加权，但是其权重是在训练集中学习到的，而非Non-local这样输入适应的，并且全连接层限制了输入的形状。</p>
<h2 id="细节设计">细节设计</h2>
<p>作者令<span class="math inline">\(g(x)=W_g\mathbf
x_j\)</span>，没有做过多的讨论。对于<span
class="math inline">\(f\)</span>和<span class="math inline">\(\mathcal
C\)</span>，作者给出了四种形式：</p>
<ul>
<li><strong>Gaussian</strong>：<span class="math inline">\(f(\mathbf
x_i, \mathbf x_j)=e^{\mathbf x_i^T \mathbf x_j}\)</span>，<span
class="math inline">\(\mathcal C(\mathbf x)=\sum_{\forall j}f(\mathbf
x_i, \mathbf
x_j)\)</span>，在实现的时候，就是完成点积之后添加一个Softmax层；</li>
<li><strong>Embedded Gaussian</strong>：<span
class="math inline">\(f(\mathbf x_i, \mathbf x_j)=e^{\theta(\mathbf
x_i)^T \phi(\mathbf x_j)}\)</span>，<span class="math inline">\(\mathcal
C(\mathbf x)=\sum_{\forall j}f(\mathbf x_i, \mathbf
x_j)\)</span>，其中<span class="math inline">\(\theta\)</span>和<span
class="math inline">\(\phi\)</span>也都是线性映射；这种情况就是没有除以<span
class="math inline">\(\sqrt d\)</span>的self attention了；</li>
<li><strong>Dot product</strong>：<span class="math inline">\(f(\mathbf
x_i, \mathbf x_j)=\theta(\mathbf x_i)^T \phi(\mathbf
x_j)\)</span>，<span class="math inline">\(\mathcal
C(x)=N\)</span>，其中<span
class="math inline">\(N\)</span>是空间位置的数量，对于2D，就是HxW，对于3D，就是TxHxW，这里使用常数作为归一化是为了简单；</li>
<li><strong>Concatenation</strong>：<span
class="math inline">\(f(\mathbf x_i, \mathbf x_j)=\text{ReLu}(\mathbf
w_f^T[\theta(\mathbf x_i), \phi(\mathbf x_j)])\)</span>，<span
class="math inline">\(\mathcal C(x)=N\)</span>；</li>
</ul>
<p>同样的，作者引入了残差连接，即希望学习的<span
class="math inline">\(\mathbf
y\)</span>是一个残差，整个Non-local块表示为： <span
class="math display">\[
z_i=W_z\mathbf y_i+\mathbf x_i
\]</span>
按照bottleneck的设计，同样是为了减少参数量，度量时的通道数减少为输入输出的一半；此外，还可以引入池化操作进一步减少计算量，用Embedding
Gaussian中的式子表示，为<span
class="math inline">\(y_i=\frac{1}{\mathcal C(\hat{\mathbf x})}
f(\mathbf x_i, \hat{\mathbf x}_j)g(\hat{\mathbf
x}_j)\)</span>，其中<span
class="math inline">\(\hat{\mathbf{x}}_j\)</span>是<span
class="math inline">\(\mathbf x_j\)</span>下采样的结果。</p>
<h2 id="实验">实验</h2>
<p>原作的实验关注了视频理解任务，因为该论文现在不算新了，这里就不再赘述。在MMSegmentation里有一个把Non-local用在decoder
head中的分割模型，可以<a
href="https://github.com/open-mmlab/mmsegmentation/blob/master/configs/_base_/models/nonlocal_r50-d8.py">参考</a>。</p>
]]></content>
  </entry>
  <entry>
    <title>SCNN：一种卷积核的稀疏化方法以及CPU上的高效稀疏矩阵乘法</title>
    <url>/2023/02/01/SCNN/</url>
    <content><![CDATA[<img src="/2023/02/01/SCNN/fig1.png" class="">
<p><strong>论文</strong>：<a
href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Liu_Sparse_Convolutional_Neural_2015_CVPR_paper.pdf"><strong>Sparse
Convolutional Neural Networks</strong></a></p>
<p><strong>作者</strong>：<em>Baoyuan Liu, Min Wang, Hassan Foroosh,
Marshall F. Tappen, Marianna Penksy</em></p>
<p><strong>一作单位</strong>：Computational Imaging Lab, Computer
Science, University of Central Florida, Orlando, FL, USA</p>
<p><strong>录用情况</strong>：CVPR'2015</p>
<blockquote>
<p>本文将卷积核两次分解，之后对第二次分解得到的“权重”矩阵进行稀疏化，对于ILSVRC2012数据集上训练的一个5层卷积层的网络，经过微调，在1%的性能损失下，获得了90%的稀疏率；尽管稀疏的算法在高度并行的GPU设备上并不吃香，但是对无GPU设备的加速仍有其意义，因此本文设计了在CPU上高效的稀疏矩阵乘法，获得了实质的推理加速；</p>
</blockquote>
<span id="more"></span>
<h2 id="方法">方法</h2>
<h3 id="线性分解稀疏化">线性分解+稀疏化</h3>
<p>记输入 <span class="math inline">\(I\in \mathbb{R}^{h\times w \times
m}\)</span>，原来的卷积核为 <span class="math inline">\(K \in
\mathbb{R}^{s\times s \times m \times n}\)</span>，输出为 <span
class="math inline">\(O \in \mathbb{R}^{(h-s+1)\times(h-s+1)\times
n}\)</span> （stride=1,
padding=0），作者将标准卷积表示为如下形式，计算复杂度为 <span
class="math inline">\(O(s^2mn(h-s+1)^2)\)</span></p>
<p><span class="math display">\[
O(y, x, j) = \sum_{i=1}^m\sum_{u,v=1}^s K(u,v,i,j)I(y+u-1, x+v-1,i)
\]</span></p>
<p>使用的第一种变换是对原卷积核 <span class="math inline">\(K\)</span>
在通道维度上进行分解，得到 <span class="math inline">\(P\in
\mathbb{R}^{m\times m}\)</span> 和 <span class="math inline">\(R \in
\mathbb{R}^{s\times s \times m \times n}\)</span>；这样有 <span
class="math inline">\(O \approx R*J\)</span>；</p>
<p><img src="equ2.png" /></p>
<p>第二种变换是继续将 <span class="math inline">\(R\)</span>
进行线性分解，为第 <span class="math inline">\(i\)</span> 个通道的 <span
class="math inline">\(R(\cdot,\cdot, i, \cdot)\)</span> 找到 <span
class="math inline">\(q_i\)</span> 个基 <span
class="math inline">\(\mathcal{Q}_i\in \mathbb{R}^{s\times s\times
q_i}\)</span>，与权重 <span class="math inline">\(\mathcal{S}_i\in
\mathbb{R}^{q_i\times n}\)</span> 的线性组合得到 <span
class="math inline">\(R(\cdot,\cdot, i, \cdot)\)</span></p>
<p>用 <span class="math inline">\(\mathcal{T}_i \in
\mathbb{R}^{(h-s+1)\times (h-s+1)\times q_i}\)</span>
表示变换后的输入的第 <span class="math inline">\(i\)</span> 的通道与第
<span class="math inline">\(i\)</span> 组基进行卷积得到的 <span
class="math inline">\(q_i\)</span> 个中间结果：</p>
<p><img src="equ3.png" /></p>
<p>将此中间结果再与权重进行线性组合得到输出：</p>
<p><span class="math display">\[
O(y, x, j) \approx \sum_{i=1}^m \sum_{k=1}^{q_i} \mathcal{S}_i(k, j)
\mathcal{T}_i(y, x, k)
\]</span></p>
<p>从 <span class="math inline">\(I\)</span> 到 <span
class="math inline">\(J\)</span> 的变换需要 <span
class="math inline">\(O(m^2hw)\)</span> 的计算；</p>
<p>从 <span class="math inline">\(J\)</span> 到 <span
class="math inline">\(\mathcal{T}\)</span> 需要 <span
class="math inline">\(s^2(h-s+1)^2\sum_{i=1}^mq_i\)</span> 的计算；</p>
<p>最后与权重线性组合的过程可以用单个 <span
class="math inline">\((h-s+1)^2\times\sum_{i=1}^m q_i\)</span> (变形
<span class="math inline">\(\mathcal{T}\)</span>) 和 <span
class="math inline">\(\sum_{i=1}^m q_i \times n\)</span> (变形 <span
class="math inline">\(\mathcal{S}\)</span>)
矩阵乘法得到，而我们希望权重矩阵是稀疏的（<span
class="math inline">\(\gamma\)</span>
是密集度，即非零元素占比）并使用稀疏的矩阵乘法，这样最后的稀疏矩阵乘法计算复杂度为
<span class="math inline">\(O(\gamma
n(\sum_{i=1}^mq_i)(h-s+1)^2)\)</span>；</p>
<p>综上，笔者得到总体的计算复杂度是： <span class="math display">\[
(\gamma n + s^2)\left( \sum_{i=1}^m q_i \right ) (h-s+1)^2 + m^2hw
\]</span></p>
<p>而原文中给的复杂度，将 <span class="math inline">\(i\)</span>
上限错误写成了 <span class="math inline">\(n\)</span>；在 <span
class="math inline">\(q_i &lt; s^2\)</span> 的条件下，<span
class="math inline">\(\sum_{i=1}^m q_i &lt;
ms^2\)</span>，即我们给出的这个复杂度比原文的复杂度更“紧”；而原文的写法也更好与原卷积的计算复杂度进行比较；</p>
<p><img src="equ6.png" /></p>
<p>不过原文对 <span class="math inline">\(i\)</span>
的上限错误也导致其接下来的另一个小错误：与原复杂度 <span
class="math inline">\(mns^2(h-s+1)^2\)</span>
对比，希望现在的复杂度优势项是 <span class="math inline">\(\gamma
mns^2(h-s+1)\)</span>（<span class="math inline">\(\gamma\)</span>
决定了加速比），另外两项就要相比于优势项可忽略，即 <span
class="math inline">\(\sum_{i=1}^m q_i = m\overline{q} &lt; \gamma
mn\)</span> （原文写的是 <span class="math inline">\(\overline{q}
&lt;\gamma m\)</span>），还有 <span class="math inline">\(m \ll \gamma
ns^2\)</span>；</p>
<p>综上，要想在理论上对算法复杂度有改进，需要满足：</p>
<p><span class="math display">\[
\left\{
  \begin{matrix}
    q_i &lt; s^2\\
    \overline{q} &lt; \gamma n\\
    m &lt; \gamma ns^2
  \end{matrix}
\right.
\]</span></p>
<blockquote>
<p>如果了解论文<a href="https://arxiv.org/abs/1405.3866">Speeding up
Convolutional Neural Networks with Low Rank
Expansions</a>，就会发现这里使用的kernel
basis线性组合的方法与该论文的scheme1分解方法基本一致，这里在之前添加了channel
basis，没有把kernel
basis进一步做成可分离的，并且对于每个通道使用不同数量的kernel
basis；</p>
</blockquote>
<h3 id="参数学习">参数学习</h3>
<p>接下来的问题就是，如何学习 <span class="math inline">\(P\)</span>,
<span class="math inline">\(Q_i\)</span>, <span
class="math inline">\(S_i\)</span> (<span class="math inline">\(i = 1,
\dots, m\)</span>)，并且让 <span class="math inline">\(S_i\)</span>
是稀疏的？</p>
<p>总体步骤分为初始化和微调，在初始化阶段主要获得合适的 <span
class="math inline">\(P\)</span>, <span
class="math inline">\(Q_i\)</span>, <span
class="math inline">\(S_i\)</span>，使得重建误差很小，但是此时的稀疏性并不好；在微调阶段使用分类损失+稀疏正则项，对
<span class="math inline">\(S_i\)</span> 进行稀疏化；</p>
<h4 id="初始化">初始化</h4>
<p>对 <span class="math inline">\(K\)</span> 在 通道方向分解得到 <span
class="math inline">\(R\)</span>, <span
class="math inline">\(P\)</span>；对 <span
class="math inline">\(R(\cdot, \cdot, i, \cdot)\)</span>
在个数方向上分解得到 <span class="math inline">\(S_i\)</span>, <span
class="math inline">\(Q_i\)</span>；这两种高维张量的分解都可以通过reshape转化为矩阵分解问题，因而有三种初始化方法：</p>
<ul>
<li>使用字典学习算法（本文作者指出该优化是非凸的且在精度和稀疏度上有中和）；</li>
<li>使用主成分分析；</li>
<li>初始化 <span class="math inline">\(P\)</span> 和 <span
class="math inline">\(Q_i\)</span> 为单位标准基：那么令 <span
class="math inline">\(R=K\)</span> 即可以使得 <span
class="math inline">\(K(u,v,i,j) = \sum_{k=1}^m R(u, v, k,
j)P(k,i)\)</span>；不过，<span class="math inline">\(S_i\)</span> 与
<span class="math inline">\(R(\cdot, \cdot, i, \cdot)\)</span>
的维度不一致，如果 <span class="math inline">\(q_i \ge
n\)</span>，可以通过构造在初始化时使得 <span
class="math inline">\(R(u,v,i,j) = \sum_{k=1}^{q_i}
S_i(k,j)Q_i(u,v,k)\)</span>，反之若 <span class="math inline">\(q_i &lt;
n\)</span>，则不能；</li>
</ul>
<h4 id="微调">微调</h4>
<p>微调阶段作者主要希望在保持性能的前提下增加 <span
class="math inline">\(S_i\)</span> 的稀疏度，设计目标函数如下： <img
src="equ7.png" /></p>
<h3 id="稀疏化与低秩化的区别">稀疏化与低秩化的区别</h3>
<p>设 <span class="math inline">\(M \in \mathbb{R}^{m\times
n}\)</span>，<span class="math inline">\(S \in \mathbb{R}^{m\times
n}\)</span>，<span class="math inline">\(P\in \mathbb{R}^{n \times
n}\)</span>，有矩阵分解 <span class="math inline">\(M=SP\)</span>；对
<span class="math inline">\(S\)</span> 进行稀疏化，相当于增多 <span
class="math inline">\(S\)</span> 的零元素个数；对 <span
class="math inline">\(S\)</span> 进行低秩化，相当于增加 <span
class="math inline">\(S\)</span>
的零列（可以由其他列线性组合得到的列）；</p>
<p>从性能上看，稀疏化比起低秩化损失更小；从速度上看，低秩化是结构性的，稀疏化是非结构性的，前者更便于使用现有的算子高效实现达到实际的加速目的；</p>
<h3 id="稀疏的矩阵乘法">稀疏的矩阵乘法</h3>
<p>作者利用推理时模型的如下两个事实：</p>
<ul>
<li>与卷积核相关的稀疏矩阵不再改变；</li>
<li>输入不是稀疏的；</li>
</ul>
<p>在OpenBLAS上实现了一种密集卷积与稀疏卷积的算法：</p>
<p>对于每一个子块的乘法，<span class="math inline">\(C=A\times
B\)</span>，其中 <span class="math inline">\(A\in \mathbb{R}^{8\times
k}\)</span>，<span class="math inline">\(B\in \mathbb{R}^{k\times
8}\)</span>，8是当时作者使用的AVX指令集最大可以并行计算的float的数量；在密集乘法中，<span
class="math inline">\(C\)</span> 的一列可以看作是 <span
class="math inline">\(A\)</span> 的一列与 <span
class="math inline">\(B\)</span> 的对应列的加权和；假设 <span
class="math inline">\(A\)</span> 是密集的，<span
class="math inline">\(B\)</span> 是稀疏的，作者通过寻找 <span
class="math inline">\(B\)</span> 中非零元素的位置，按照 <span
class="math inline">\(B\)</span>
的行列顺序，重新生成了乘法计算的代码，如下图所示：</p>
<p><img src="fig3.png" /></p>
<h3 id="线性层的稀疏化">线性层的稀疏化</h3>
<p>可以将线性层权重分解为密集矩阵与稀疏矩阵的乘法，并选择稀疏化维度更大的那个矩阵，为了使得密集矩阵更小；</p>
<h2 id="实验">实验</h2>
<p>在对分类模型稀疏化时，作者首先拿到已训练好的权重，之后按照前文方法修改卷积计算，进行初始化+微调，并应用阈值函数来获得稳定的稀疏；在收敛之后，去除稀疏限制（但保留阈值函数）再微调一阵以达到最佳精度；</p>
<p>作者在实验中比较了微调后卷积核与原来卷积核的相似度，发现从conv1到conv3，这种相似度从0.85下降到0.34，而精度没有太大损失，说明没有必要逐个卷积核地恢复，基于分类损失的全局微调很有效；</p>
<p>作者还比较了对基的三种初始化方式，发现他们在性能上没有显著差异，但PCA的初始化，微调后得到的稀疏度最高；接着作者发现，稀疏度与主成分有很强的相关性，越重要的成分越容易稀疏；</p>
<p>更多实验详见原文。</p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>模型压缩</tag>
        <tag>卷积分解</tag>
        <tag>稀疏化</tag>
      </tags>
  </entry>
  <entry>
    <title>语义分割论文阅读列表</title>
    <url>/2022/10/19/Semantic-Segmantation-Papers/</url>
    <content><![CDATA[<p>本文记录笔者已经阅读/准备阅读的语义分割论文，并进行简单介绍，附相关链接。</p>
<span id="more"></span>
<h5 id="maskformer">MaskFormer</h5>
<h5 id="segmentor">Segmentor</h5>
<p><a href="https://arxiv.org/pdf/2105.05633.pdf">Segmentor: Transformer
for Semantic Segmentation</a></p>
<p><a href="https://github.com/rstrudel/segmenter">原代码仓库</a>，<a
href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/segmenter_mask_head.py">MMSeg
- MaskFormer Decoder</a></p>
<blockquote>
<p>原代码的依赖好像不是很强，将来不想用mmseg的时候可以试着用这个。</p>
</blockquote>
<p>一种purely transformer
based方法：encoder使用ViT或DeiT的结构（使用了数据增强和正则化的trick，在ImageNet上的top1-acc比原来提升了2%），解码器设计了一种Mask
Transformer的结构：该模块以patch-encodings <span
class="math inline">\(\mathbf{z_L}\in \mathbb R^{N\times D}\)</span>
和<span class="math inline">\(K\)</span>个类别的embeddings <span
class="math inline">\(\mathbf{cls}\in \mathbb R^{K\times
D}\)</span>作为输入，经过<span
class="math inline">\(M\)</span>层Transformer之后，将K类嵌入编码分别与<span
class="math inline">\(N\)</span>个patch做标量积， 得到<span
class="math inline">\(N\)</span>组<span
class="math inline">\(K\)</span>类掩码，最后进行LN+reshape+上采样得到pixel-wise
class score <span class="math inline">\(s \in \mathbb{R}^{H\times
W\times K}\)</span>；如果想做实例分割，可以将class embedding换成object
embedding。</p>
<p>在消融实验部分，作者一个比较重要的观察时，使用更小的patch
size，一方面会提高计算量（降低推理速度，但参数量是不变的），但另一方面也会显著提升分割任务的精度。</p>
<p>在ADE20K上，部分模型性能如下：</p>
<table>
<thead>
<tr class="header">
<th>Model</th>
<th>Patch Size</th>
<th>Params</th>
<th>Im/sec</th>
<th>mIoU(ss)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Seg-Ti</td>
<td>16</td>
<td>6M</td>
<td>396</td>
<td>39.03</td>
</tr>
<tr class="even">
<td>Seg-L</td>
<td>16</td>
<td>307M</td>
<td>33</td>
<td>50.71</td>
</tr>
</tbody>
</table>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>语义分割</tag>
      </tags>
  </entry>
  <entry>
    <title>稳定的数值计算</title>
    <url>/2022/08/30/Stable-Calculations/</url>
    <content><![CDATA[<p>本文将介绍在实现softmax和交叉熵损失计算时需要注意的一些数学问题，并且回顾相关函数的定义与性质；代码来源于<a
href="https://mp.weixin.qq.com/s/5qYbxSe6X5KZo1ISOE4r8A">极市平台 -
编写高效的Pytorch代码技巧</a>；</p>
<span id="more"></span>
<h2 id="部分技巧">部分技巧</h2>
<p>计算机中的浮点数运算通常存在两个问题：精度损失和算术溢出，本质上都是浮点计算单元的位数有限导致的。在训练模型过程中令人讨厌的问题之一就是梯度/loss出现<code>Nan</code>或者<code>Inf</code>，当算法本身在数学上没有错误时，就要考虑是不是出现了不稳定的数值计算，尤其警惕那些特别小或者特别大的数值；下面记录两个避免数值问题的技巧：</p>
<h3 id="softmax">softmax</h3>
<p>不稳定的写法如下，<span
class="math inline">\(e^x\)</span>很容易发生上溢：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">unstable_softmax</span>(<span class="params">logits</span>):</span><br><span class="line">    exp = torch.exp(logits)</span><br><span class="line">    <span class="keyword">return</span> exp / torch.<span class="built_in">sum</span>(exp)</span><br></pre></td></tr></table></figure>
<p>可以将分子分母同时除以一个常数，相当于对输入数据减去一个常量，经验上，这个常量取输入的最大值即可：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">logits</span>):</span><br><span class="line">    exp = torch.exp(logits - torch.<span class="built_in">max</span>(logits))</span><br><span class="line">    <span class="keyword">return</span> exp / torch.<span class="built_in">sum</span>(exp)</span><br></pre></td></tr></table></figure>
<h3 id="cross-entropy">cross entropy</h3>
<blockquote>
<p>计算交叉熵损失通常在对预测得分完成softmax之后，但这并不意味着它的实现与softmax解耦了，事实上，下面的改进正是利用了这种耦合性</p>
</blockquote>
<p>不稳定的写法如下，<span class="math inline">\(\log
x\)</span>很容易发生下溢：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">unstable_softmax_cross_entropy</span>(<span class="params">labels, logits</span>):</span><br><span class="line">    logits = torch.log(softmax(logits))</span><br><span class="line">    <span class="keyword">return</span> -torch.<span class="built_in">sum</span>(labels * logits)</span><br></pre></td></tr></table></figure>
<p>可以这样重写： <span class="math display">\[
\log p_i =
\log(\frac{e^{x_i}}{\sum_je^{x_j}})=\log(\frac{e^{x_i-m}}{\sum_je^{x_j-m}})=(x_i-m)
- \text{LSE}(\mathbf{x}-\mathbf{1}m)
\]</span> 其中<span class="math inline">\(m=\max_jx_j\)</span>；<span
class="math inline">\(\text{LSE}\)</span>是LogSoftMax；</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">softmax_cross_entropy</span>(<span class="params">labels, logits, dim=-<span class="number">1</span></span>):</span><br><span class="line">    scaled_logits = logits - torch.<span class="built_in">max</span>(logits)</span><br><span class="line">    normalized_logits = scaled_logits - torch.logsumexp(scaled_logits, dim)</span><br><span class="line">    <span class="keyword">return</span> -torch.<span class="built_in">sum</span>(labels * normalized_logits)</span><br></pre></td></tr></table></figure>
<h2 id="补充的数学知识">补充的数学知识</h2>
<h3 id="softmax-1">softmax</h3>
<p>通常使用这个函数将输出转化为一组在<span
class="math inline">\((0,1)\)</span>范围内的，和为1的输出，有着概率意义；</p>
<p>softmax是这样的函数：<span
class="math inline">\(\sigma:\mathbb{R}^K\rightarrow
(0,1)^K\)</span>，其中<span class="math inline">\(K&gt;1\)</span> <span
class="math display">\[
\sigma(\mathbf{z})_i=\frac{e^{z_i}}{\sum_{j=1}^Ke^{z_j}}
\]</span> 除了以<span
class="math inline">\(e\)</span>为底，任何一个<span
class="math inline">\(b&gt;0\)</span>可以作为底，如果<span
class="math inline">\(0&lt;b&lt;1\)</span>，则原来更小的值将获得更大的输出；如果<span
class="math inline">\(b&gt;1\)</span>，则原来更大的值将获得更小的输出；令<span
class="math inline">\(b=e^\beta\)</span>，其中<span
class="math inline">\(\beta\)</span>是实数，则还有： <span
class="math display">\[
\sigma(\mathbf{z})_i=\frac{e^{\beta z_i}}{\sum_{j=1}^Ke^{\beta z_j}}
\]</span> softmax是平滑版的argmax，当<span
class="math inline">\(\beta\rightarrow
\infty\)</span>时，softmax收敛到argmax；一般地，平滑即连续可导，这样的性质在神经网络中非常重要；如果<span
class="math inline">\(z_1,\dots,z_n\)</span>两两不相等，有： <span
class="math display">\[
\arg\max(z_1,\dots,z_n)=(y_1,\dots,y_n)=(0,\dots,0,1,0,\dots,0)
\]</span> 其中<span
class="math inline">\(\mathbf{z}\)</span>的一个微小的改变都可能让argmax的输出发生跳变；</p>
<blockquote>
<p>argmax不可导，但是max(x1,x2)在x1!=x2时是可导的，后文也会介绍max的平滑近似——LSE</p>
</blockquote>
<p>类似的，当<span class="math inline">\(\beta\rightarrow
-\infty\)</span>时，argmax收敛到argmin；</p>
<h3 id="cross-entropy的梯度推导">cross entropy的梯度推导</h3>
<p>softmax+交叉熵耦合在一起的另一个原因是，其导数的形式非常简洁：</p>
<p>假设真值为第<span class="math inline">\(i\)</span>个类别，<span
class="math inline">\(\mathbf{q}\)</span>是真实分布，<span
class="math inline">\(\mathbf{p}=\text{softmax}(\mathbf{s})\)</span>是估计分布，则损失函数<span
class="math inline">\(L=-\sum_jq_j\log p_j=-\log p_i\)</span>，其中<span
class="math inline">\(p_i =
\frac{\exp(s_i)}{\sum_j\exp(s_j)}\)</span>；要求出<span
class="math inline">\(\frac{\partial L}{\partial
\mathbf{s}}\)</span>，其中<span
class="math inline">\(\mathbf{s}=(1,\dots,j,\dots,d)\)</span>，分为两种情况：</p>
<p>下面用<span class="math inline">\(\sum\)</span>代替<span
class="math inline">\(s\)</span>，注意<span
class="math inline">\(\frac{\partial L}{\partial
p_i}=-\frac{1}{p_i}=-\frac{\sum{}}{\exp(s_i)}\)</span></p>
<ol type="1">
<li><p><span class="math inline">\(j = i\)</span>，此时<span
class="math inline">\(p_i\)</span>的分子也是<span
class="math inline">\(s_j\)</span>的函数： <span class="math display">\[
\frac{\partial
{p_i}}{\partial{s_j}}=\frac{\exp(s_i)\sum{}-\exp(2s_i)}{\sum{}^2}
\]</span> 则 <span class="math display">\[
\begin{aligned}
\frac{\partial L}{\partial s_j} &amp;=
-\frac{\sum{}}{\exp(s_i)}\frac{\exp(s_i)\sum{}-\exp(2s_i)}{\sum{}^2}\\
&amp;=p_i-1\\
&amp;=p_j-q_j
\end{aligned}
\]</span></p></li>
<li><p><span class="math inline">\(j\ne i\)</span>，此时<span
class="math inline">\(p_i\)</span>的分子与<span
class="math inline">\(s_j\)</span>无关： <span class="math display">\[
\frac{\partial {p_i}}{\partial{s_j}}=\frac{-\exp(s_i+s_j)}{\sum{}^2}
\]</span> 则 <span class="math display">\[
\begin{aligned}
\frac{\partial L}{\partial s_j} &amp;=
-\frac{\sum{}}{\exp(s_i)}\frac{-\exp(s_i+s_j)}{\sum{}^2}\\
&amp;=p_j\\
&amp;=p_j-q_j
\end{aligned}
\]</span></p></li>
</ol>
<p>综上所述，<span class="math inline">\(\frac{\partial L}{\partial
\mathbf{s}}=\mathbf{p}-\mathbf{q}\)</span></p>
<h3 id="logsoftexp">LogSoftExp</h3>
<p>函数的形式为： <span class="math display">\[
\text{LSE}(\mathbf{x})=\log(\sum_je^{x_j})
\]</span> 该函数是max函数的平滑版本，是凸函数，假设<span
class="math inline">\(\mathbf{x}\in\mathbb{R}^n\)</span>，<span
class="math inline">\(m=\max_j x_j\)</span>，则 <span
class="math display">\[
e^m\le \sum_j^ne^{x_j}\le ne^m
\]</span> 两边同时取对数，则有： <span class="math display">\[
m\le \text{LSE}(\mathbf{x}) \le \log(n) + m
\]</span>
在使用计算机计算LSE时同样也有数值问题，而解决方法同样是，对于等式右边，减去再加上<span
class="math inline">\(m = \log e^m\)</span></p>
]]></content>
      <categories>
        <category>基础知识</category>
      </categories>
  </entry>
  <entry>
    <title>《Attention is not all you need》阅读笔记</title>
    <url>/2022/08/29/SA-inductive-bias/</url>
    <content><![CDATA[<p>本文是<a
href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=AjX6hisAAAAJ&amp;citation_for_view=AjX6hisAAAAJ:5nxA0vEk-isC">Attention
is not all you need: Pure attention loses rank doubly exponentially with
depth</a>的阅读笔记，主要贡献为：</p>
<ul>
<li>提出了一种路径分解(path decomposition)的方式来理解自注意力网络(SA or
SANs)；</li>
<li>从理论和实验上证明了如果没有skip
connections和MLP，纯SANs将随着深度的增加，将以双倍指数的速度退化到一个秩为1的矩阵（重复的token），称为秩坍缩(rank
collapse)，也可以说SA具有很强的token uniformity的偏执假设;</li>
<li>结合skip-connection的有效性和路径分解的解释，可以将Transformer结构理解为浅层网络的集成；</li>
</ul>
<span id="more"></span>
<blockquote>
<p>这篇文章的公式很多，查了查作者是数学科班出身，笔者能力有限，文中还有很多推导和表述不能理解；阅读过程中我也参考了一位大佬很详细的解读：<a
href="https://zhuanlan.zhihu.com/p/356956903">知乎 - 迷途小书僮</a></p>
</blockquote>
<p>首先，需要对后续公式的记号做个说明：</p>
<ul>
<li><p>加粗的字母表示列向量（小写）或者矩阵（大写），<span
class="math inline">\(\mathbf{1}\)</span>表示一个全为1的列向量；</p></li>
<li><p><span class="math inline">\([H]=(1,\dots,H)\)</span>；</p></li>
<li><p>作者定义了一种<span
class="math inline">\(l_1,l_\infty\)</span>混合的矩阵范数： <span
class="math display">\[
||X||_{1,\infty}=\sqrt{||X||_1||X||_\infty}
\]</span> 该范数不满足三角不等式，但是是正定的，齐次的；</p></li>
</ul>
<h2 id="理论部分">理论部分</h2>
<h3 id="路径分解">路径分解</h3>
<p>记输入为<span class="math inline">\(n\times d_{in}\)</span>矩阵<span
class="math inline">\(X\)</span>，有<span
class="math inline">\(L\)</span>层self-attention层，每层有<span
class="math inline">\(H\)</span>个头，输出序列的长度为<span
class="math inline">\(m\)</span>，第<span
class="math inline">\(h\)</span>个SA的输出可以记为：</p>
<p><img src="image-20220829182555652.png" /></p>
<p>其中，<span class="math inline">\(W_{V,h}\)</span>是一个<span
class="math inline">\(n\times d_v\)</span>的Value变换区镇，<span
class="math inline">\(P_h\)</span>是一个<span
class="math inline">\(n\times n\)</span>的行随机矩阵：</p>
<p><img src="image-20220829182917579.png" /></p>
<p>其中<span class="math inline">\(W_{K,h}\)</span>和<span
class="math inline">\(W_{Q,h}\)</span>都是<span
class="math inline">\(d_{in}\times d_{qk}\)</span>的变换矩阵，<span
class="math inline">\(W_{QK,h}=W_{Q,h}W_{K,h}^T\)</span></p>
<blockquote>
<p><span
class="math inline">\(P_h\)</span>是使用softmax归一化处理的Attention
Map，但是为什么称row-stochastic不太清楚；</p>
</blockquote>
<p><span
class="math inline">\(P_h\)</span>在展开后为什么少了两项？这里需要注意softmax的两个性质：</p>
<ol type="1">
<li><p>平移不变性 <span class="math display">\[
\sigma(x+C)_i =
\frac{e^{x_i}e^C}{\sum_je^{x_j}e^C}=\frac{e^{x_i}}{\sum_je^{x_j}}=\sigma({x})_i
\]</span> 因此，展开后与<span
class="math inline">\(X\)</span>无关的常数项<span
class="math inline">\(\mathbf{1b_{Q,h}^T
b_{K,h}1^T}\)</span>可以省略；</p></li>
<li><p>这里softmax是逐行归一化的，因此对于一个由相同的列向量组成的矩阵（每行都是同一个值），softmax对每行的权重都是平均分，整体上不改变结果；因此类似于右乘了一个<span
class="math inline">\(\mathbf{1}^T\)</span>的项，都可以在softmax内省略；</p></li>
</ol>
<p>接下来将多个头合并，给出一个多头注意力模块的简洁的表达式（不包含skip-connection）：</p>
<p><img src="image-20220829184907635.png" /></p>
<p>其中，<span
class="math inline">\([\text{SA}_1(X),\dotsm,\text{SA}_H(X)]\)</span>是<span
class="math inline">\(n\times Hd_v\)</span>的，<span
class="math inline">\([W_{O,1}^T,\dots,W_{O,H}^T]^T\)</span>是<span
class="math inline">\(Hd_v\times
d_h\)</span>的，偏移项写得好像有点问题（？不过不重要），之后使用矩阵的分块乘法法则得到第二行的求和式，其中<span
class="math inline">\(W_h=W_{V,h}W_{O,h}^T\)</span></p>
<blockquote>
<p>偏移项可以通过改写<span class="math inline">\(X\)</span>和<span
class="math inline">\(W\)</span>而消除，因此在后面的推导中不再使用偏移项</p>
</blockquote>
<p>当多个SA堆叠时，我们便可以通过如下的递推式：</p>
<p><img src="image-20220829191051128.png" /></p>
<p>得到一个第<span class="math inline">\(L\)</span>层输出<span
class="math inline">\(X^{L}\)</span>关于输入<span
class="math inline">\(X\)</span>的表达式</p>
<p><img src="image-20220829191440509.png" /></p>
<p>对于这个式子，可以看成是从输入到第L层的一个有向图，每一层有<span
class="math inline">\(H\)</span>条路；</p>
<p><img src="image-20220829192520446.png" /></p>
<p><span class="math inline">\(P_{path}\)</span>与输入有关，<span
class="math inline">\(W_{path}\)</span>与输入无关；</p>
<p><img src="image-20220829192613228.png" /></p>
<p>如图，整个SAN被分解为了若干条路径的组合；</p>
<h3 id="san收敛到rank-1矩阵">SAN收敛到rank-1矩阵</h3>
<h4 id="单头注意力的情况">单头注意力的情况</h4>
<p>假设要证明的收敛目标是<span
class="math inline">\(1\mathbf{x}^T\)</span>，这是一个所有行都相同的rank-1矩阵，那么只要证明第<span
class="math inline">\(h\)</span>层的输出<span
class="math inline">\(X\)</span>，与<span
class="math inline">\(1\mathbf{x}^T\)</span>的距离有一个上界，并且这个上界随着<span
class="math inline">\(h\)</span>的增加越来越小；</p>
<p>作者给出了如下目标函数，残差（期望与观测的差）：</p>
<p><img src="image-20220829193554480.png" /></p>
<p>在<span class="math inline">\(H=1\)</span>，<span
class="math inline">\(L\)</span>层，且满足</p>
<p><img src="image-20220829193800445.png" /></p>
<p>的条件下，作者证明（在附录中给出）：</p>
<p><img src="image-20220829193830091.png" /></p>
<p>那么只要<span
class="math inline">\(4\beta&lt;\sqrt{d_{qk}}\)</span>，不等式右边就会随着<span
class="math inline">\(L\)</span>的增加快速收敛到0；作者的实验证明这是一个很宽松的条件；</p>
<blockquote>
<p>附录中有简略的证明，<a
href="https://zhuanlan.zhihu.com/p/356956903">知乎 -
迷途小书僮</a>对大部分的推导过程给出了详细的阐述，我就不再赘述了</p>
</blockquote>
<h4 id="多头注意力的情况">多头注意力的情况</h4>
<p>推广到有<span
class="math inline">\(H\)</span>个头，得到残差的上界：</p>
<p><img src="image-20220829194442073.png" /></p>
<p>那么只要<span class="math inline">\(4\beta
H&lt;\sqrt{d_{qk}}\)</span>，不等式右边就会随着<span
class="math inline">\(L\)</span>的增加快速收敛到0；</p>
<h3 id="抵消秩崩塌的方式">抵消秩崩塌的方式</h3>
<h4 id="skip-connections很有用">Skip connections很有用</h4>
<p>我们可以将选择Skip-connections的路径记为<span
class="math inline">\(h=0\)</span>，同时有<span
class="math inline">\(P_0=I\)</span>，<span
class="math inline">\(W_0=I\)</span>，则之前的式子可以重新写成：</p>
<p><img src="image-20220829200030809.png" /></p>
<p>作者在附录中同样给出了在这个式子下的res(X)的上限，但是这个上限非常大，Skip-connections增加了路径分布的多样性（路径参数组合的多样性）：因为skip-connection算一条路径长度为0的路，因此对于<span
class="math inline">\(L\)</span>层SA，长度为<span
class="math inline">\(l\)</span>的路径数量为： <span
class="math display">\[
\mathcal{|P_l|}=
\left(
\begin{array}{c}
L\\l
\end{array}
\right)
H^l
\]</span></p>
<p>相反地，还可以给出一个目标函数的下界，说明加入Skip-connections之后SANs结果不会收敛到rank-1矩阵：
<span class="math display">\[
||res(X^L)||\ge||res(X)||
\]</span> 当对于所有层都有<span
class="math inline">\(W_v^l=0\)</span>时，上式取等；即使在<span
class="math inline">\(L\rightarrow \infty\)</span>，<span
class="math inline">\(\beta\)</span>很小时，上式也成立；</p>
<blockquote>
<p>原文里有一句 for any parametrization that renders the contribution of
the SAN layers orthogonal to the input，我不太理解</p>
</blockquote>
<p>有此也可以得出结论，SANs是浅层网络的集成，只不过在各个组件之间不是完全解耦的，因为一个head可能出现在多个path中；</p>
<h4 id="mlp有用">MLP有用</h4>
<p>且不看严谨的证明，在weight和bias都是随机初始化的时候，并且输入的token也各不相同的前提下，也能直观地感觉到，反复使用MLP能够避免收敛到rank-1矩阵；</p>
<p>作者同样给出了加入MLP后，res(X)的上界：</p>
<p><img src="image-20220829201840986.png" /></p>
<p>主要是通过让收敛变慢来避免秩坍缩；</p>
<h4 id="layernorm没用">LayerNorm没用</h4>
<p>这里的无用是针对解决秩坍缩问题而言的，因为应用LN后，SA(X)仍然能改写成与原来相同的形式，之后也就可以得到同样形式的上界：</p>
<p><img src="image-20220829202101795.png" /></p>
<p><img src="image-20220829202137282.png" /></p>
<h2 id="实验部分">实验部分</h2>
<h3 id="导致秩坍缩的验证">导致秩坍缩的验证</h3>
<p>作者首先在几个常见的Transformer结构上进行了实验，BERT，Albert，XLNet，挥着了相对残差<span
class="math inline">\(||res(SAN(X^l))||_{1,\infty}/||SAN(X^l)||_{1，\infty}\)</span>随着层数增加的变化，使用在维基百科上的传记摘录的的32个样本训练模型，<span
class="math inline">\(d_{in}=128\)</span></p>
<p><img src="image-20220829202527114.png" /></p>
<blockquote>
<p>主要还是skip-connections对于避免秩坍缩的作用更明显</p>
</blockquote>
<h3 id="避免秩坍缩的验证">避免秩坍缩的验证</h3>
<p>之后，作者训练单层Transformer来学习两对圆弧序列，训练时时teacher
forcing的，那么如果在完全自回归的推理时，两条弧线收敛到同一点，而不是延续训练的轨迹，则认为发生了秩坍缩：</p>
<p><img src="image-20220829203753205.png" /></p>
<p>实验结果表明：</p>
<ul>
<li>skip-connections和MLP有效避免了秩坍缩；</li>
<li>当维度增大时，对于<span
class="math inline">\(4\beta&lt;\sqrt{d_{qk}}\)</span>的条件，看似是不等式右边增加使得收敛条件更宽，实际上<span
class="math inline">\(\beta\)</span>作为<span
class="math inline">\(|||W^l_{QK}||_1||W_V^l||_{1,\infty}\)</span>的上界也在增加，使得收敛条件更紧；</li>
</ul>
<h3 id="路径有效性">路径有效性</h3>
<p>下面的实验试图验证，随着路径长度的增加，即使所涉及的非线性操作数增加（可以理解为路径数量），路径有效性也在降低：</p>
<p>作者分别在序列记忆、学习排序和凸包预测任务上训练Transformer模型；</p>
<p>在上文中我们提到，路径之间不是完全解耦的，如何找到某一个路径的显示表示，如何衡量其单独的贡献？</p>
<p>对于前一个问题，没办法确定一个训练好的Transformer具体是哪些路径的组合，作者就从全体路径中，按照给定的长度随机采样一个路径集合的子集，用该子集中路径的贡献的归一化总和（平均贡献？）；</p>
<p>衡量任意一条给定路径序列<span class="math inline">\(h_1,\dots,h_L \in
[H\cup0]^L\)</span>，使用下面式子的结果作为该路径的输出： <span
class="math display">\[
(P_{h_L}^L\dots P_{h_1}^1)X(W_{h_1}^1\dots W_{h_L}^L)
\]</span> <img src="image-20220829210557076.png" /></p>
<p>记忆和排序任务都验证了路径越长效果越差的假设，凸包任务有明显的类别不平衡问题，上述假设体现不明显，但是随着路径的增加，模型预测结果准确率的方差在变大；</p>
<h2 id="小结">小结</h2>
<p>至此位置，论文理解只有一半，剩下一半几乎都是附录中的证明，这篇文章的理论性很充足，一般人学不来啊；</p>
<p>之前总是讲CNN的归纳偏置是局部性和平移不变性，这篇文章让我们知道了Transformer的token一致性的归纳偏置，以及纯SA竟然这么拉（如果证明严谨无误并且没有比秩坍缩更严重的问题的话）？那么是不是靠着MLP+skip
connection+别的什么结构会有更好的结果？</p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>SegFormer论文阅读</title>
    <url>/2022/09/07/SegFormer/</url>
    <content><![CDATA[<p>标题：《<strong>SegFormer: Simple and Efficient Design for Semantic
Segmentation with Transformers</strong>》</p>
<p>作者：<em>Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose
M. Alvarez, Ping Luo</em></p>
<p>录用情况：NeurIPS'2021</p>
<p>开源代码：在MMSegmentation中已有实现</p>
<p>这篇文章提出了一个简单高效的语义分割模型，使用了多尺度的Transformer结构做为Encoder，和使用MLP实现的简单的Decoder；在Transformer-Based中，作者没有使用显式的位置编码，而使用CNN建模位置信息；并且还使用了一些策略将attention计算的复杂度从<span
class="math inline">\(O(N^2)\)</span>降低到<span
class="math inline">\(O(N^2/R)\)</span>；</p>
<img src="/2022/09/07/SegFormer/image-20220907185126016.png" class="" title="Figure1">
<blockquote>
<p>本文的很多方法都不是原创，但是在作者的组合、改进下，取得了性能与参数量平衡的结果；</p>
</blockquote>
<span id="more"></span>
<h2 id="模型结构">模型结构</h2>
<h3 id="整体">整体</h3>
<p><img src="image-20220907185427672.png" /></p>
<h3 id="overlapped-patch-merging">Overlapped Patch Merging</h3>
<p>将SA的输出重新reshape为<span class="math inline">\(B\times H \times W
\times C\)</span>，之后让patch与patch之间有overlapped的思想在T2T
ViT中就出现了，在实现时可以统一用一个带有<code>stride</code>的卷积操作实现；</p>
<h3 id="efficient-self-attention">Efficient Self-Attention</h3>
<p>作者借用了Pyramid Vision Transformer中的策略，将key,
value矩阵应用如下的映射，从而将attention计算的复杂度从<span
class="math inline">\(O(N^2)\)</span>降低到<span
class="math inline">\(O(N^2/R)\)</span>； <span class="math display">\[
\begin{aligned}
\hat{K}&amp;=\text{Reshape}(\frac{N}{R},C\cdot R)(K)\\
K&amp;=\text{Linear}(C\cdot R, C)(\hat{K})
\end{aligned}
\]</span>
在实现时，对key，value使用步长为R的RxR卷积，即同时完成了上述两步；</p>
<h3 id="mix-ffn">Mix FFN</h3>
<p>借鉴Conditional Positional Encodings for Vision
Transformers中的经验，作者将原来Transformer中使用MLP的FFN模块改为了使用3x3卷积的模块，不再使用显式得PE：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fc1 = Conv2d(</span><br><span class="line">    in_channels=in_channels,</span><br><span class="line">    out_channels=feedforward_channels,</span><br><span class="line">    kernel_size=<span class="number">1</span>,</span><br><span class="line">    stride=<span class="number">1</span>,</span><br><span class="line">    bias=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 3x3 depth wise conv to provide positional encode information</span></span><br><span class="line">pe_conv = Conv2d(</span><br><span class="line">    in_channels=feedforward_channels,</span><br><span class="line">    out_channels=feedforward_channels,</span><br><span class="line">    kernel_size=<span class="number">3</span>,</span><br><span class="line">    stride=<span class="number">1</span>,</span><br><span class="line">    padding=(<span class="number">3</span> - <span class="number">1</span>) // <span class="number">2</span>,</span><br><span class="line">    bias=<span class="literal">True</span>,</span><br><span class="line">    groups=feedforward_channels)</span><br><span class="line">fc2 = Conv2d(</span><br><span class="line">    in_channels=feedforward_channels,</span><br><span class="line">    out_channels=in_channels,</span><br><span class="line">    kernel_size=<span class="number">1</span>,</span><br><span class="line">    stride=<span class="number">1</span>,</span><br><span class="line">    bias=<span class="literal">True</span>)</span><br><span class="line">drop = nn.Dropout(ffn_drop)</span><br><span class="line">layers = [fc1, pe_conv, self.activate, drop, fc2, drop]</span><br></pre></td></tr></table></figure>
<h3 id="all-mlp-decoder">All-MLP Decoder</h3>
<p>作者称，他敢使用这么简单的一个Decoder的原因是，他分析到之前设计的Encoder已经拥有了足够大的感受野，不再需要设计像ASPP那样heavy的结构，如下图所示：</p>
<p><img src="image-20220907192316099.png" /></p>
<p>MLP结构为：</p>
<p><img src="image-20220907192434172.png" /></p>
<h2 id="实验结果">实验结果</h2>
<p>作者修改4个Stage中各种参数（见论文附录），得到了6种配置，在3个数据集上的效果如下：</p>
<p><img src="image-20220907192703069.png" /></p>
<p>与先前模型的对比：</p>
<p><img src="image-20220907192900552.png" /></p>
<p>更多实验见原文；</p>
<h2 id="小结">小结</h2>
<p>作者提出了一种简单的语义分割方法，并且得到了一个3.7M的小模型；抛开作者讲的故事，单看整体的模型结构，像是Transformer结构与CNN的穿插使用；另外，作者在消融实验中似乎没有比较使用原始self
attention和efficient self
attention的性能与成本差距，或者说，在之前的论文中已经做了相似的实验，这需要我继续阅读相关论文；</p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>语义分割</tag>
      </tags>
  </entry>
  <entry>
    <title>RepVGG论文与代码阅读</title>
    <url>/2022/10/03/RepVGG/</url>
    <content><![CDATA[<img src="/2022/10/03/RepVGG/image-20221003122122530.png" class="">
<p><strong>论文</strong>：<a
href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ding_RepVGG_Making_VGG-Style_ConvNets_Great_Again_CVPR_2021_paper.pdf">RepVGG:
Making VGG-style ConvNets Great Again</a></p>
<p><strong>作者</strong>：<em>Xiaohan Ding</em>, <em>Xiangyu Zhang</em>,
<em>Ningning Ma</em>, <em>Jungong Han</em>, <em>Guiguang Ding</em>,
<em>Jian Sun</em></p>
<p><strong>录用情况</strong>：CVPR'2021</p>
<p><strong>第一作者单位</strong>：Beijing National Research Center for
Information Science and Technology (BNRist); School of Software,
Tsinghua University, Beijing, China</p>
<blockquote>
<p>本文借助重参数化技巧，提出了一个在训练时多分支，而在推理时为类似于VGG的直筒形网络（3x3卷积+ReLU堆叠成的基本块），命名为RepVGG。这是第一次一个简单的模型在ImageNet上获得了80%以上的top1准确率，并且直筒形状的结构让RepVGG有着很高的计算密度与推理速度，相比于EfficientNet和RegNet有更好的性能与速度的折中。<a
href="https://zhuanlan.zhihu.com/p/344324470">作者本人在知乎上的稿子</a>。<a
href="https://github.com/DingXiaoH/RepVGG">代码仓库</a>。</p>
</blockquote>
<span id="more"></span>
<h2 id="方法">方法</h2>
<h3 id="为什么需要多分支的网络">为什么需要多分支的网络</h3>
<p>我们不否认直筒型的、深层的网络理论上具有足够的学习能力，但是很难训练；从GoogLeNet、ResNet起，大家就尝到了多分支网络的甜头，对于shortcut连接，一些普遍的解释为：</p>
<ul>
<li>获得了多种浅层模型的集成的效果，具体地说，<span
class="math inline">\(n\)</span>个有shortcut的block，相当于<span
class="math inline">\(2^n\)</span>个网络的集成；</li>
<li>在非线性层存在的前提下，复杂的连接，增加了模型的非线性；</li>
</ul>
<p>然而，这种多分支结构也引入了缺陷：</p>
<ul>
<li>多分支使得显存消耗增加；</li>
<li>复杂的模型降低了并行度；</li>
</ul>
<p>作者指出，尽管一些模型具有较低的FLOPs，但是因为并行度不够等原因，计算密度低，模型在推理时的速度慢。</p>
<h3 id="模型重参数化">模型重参数化</h3>
<p>本文的重参数化，侧重于“使用先前的模型结构的参数，构造新结构的参数，使得模型结构修改前后，对于数据流的作用是等价的”，即找到一种参数的代数变换，使得<span
class="math inline">\(x+g(x)+f(x)=h(x)\)</span>；</p>
<p>作者在文中强调了这个工作与DiractNet的区别：DiractNet的重参数化是用得到的参数计算另一个数学表达式来优化。笔者后续也会更新一些别的重参数化的论文阅读。</p>
<h3 id="winograd-convolution">Winograd Convolution</h3>
<p>这是一种对于步长为1的3x3卷积的加速算法，对于<span
class="math inline">\(F(2\times 2, 3\times
3)\)</span>（表示输出尺寸为2x2，卷积核尺寸为3x3的一卷积运算），考虑每一个输出的每一个位置来自一对长度为9的向量的内积，标准卷积方法需要2x2x3x3=36次乘法计算，而winograd方法通过观察到im2col之后的矩阵存在大量重复元素，设计算法将乘法次数减少到16次。这个<a
href="https://www.slideshare.net/embeddedvision/even-faster-cnns-exploring-the-new-class-of-winograd-algorithms-a-presentation-from-arm?from_action=save">slide</a>清楚地讲述了在<span
class="math inline">\(F(2,3)\)</span>上的算法，又如何扩展到<span
class="math inline">\(F(2\times 2, 3\times
3)\)</span>，进而再扩展到更大尺寸，更多通道上的3x3步长1卷积。</p>
<p>作者整个模型都使用3x3,stride=1的卷积，旨在利用上这种加速算法。</p>
<h3 id="训练时块结构与重参数化方法">训练时块结构与重参数化方法</h3>
<p><img src="image-20221003132412029.png" /></p>
<p>作者为每个块设计了两条额外分支，在每个块的最后（图中没有画出来）的是ReLU非线性层，不参与后续的重参数化。在推理前，目的时将3个分支的参数合并到单分支的3x3卷积中去。</p>
<p><img src="image-20221003132802917.png" /></p>
<p>上图主要展示了，如何将1x1卷积和原始输入构造为具有等价输出的3x3卷积，下面的公式化描述介绍了如何“吸BN”。</p>
<p><img src="image-20221003133459134.png" /></p>
<p>在实现时，注意的第一个细节是，将1x1卷积转化为3x3卷积时，除了给卷积核pad
0之外，图像的padding也要减去<span class="math inline">\(\lfloor
\frac{\text{kernel size}}{2}\rfloor\)</span>，这才使得结果等价。</p>
<p>另一个细节是，由于作者穿插设置了部分块使用分组卷积（后面的整体架构一节中会讲到），对于3x3卷积和1x1卷积的分支，在重参数化时没有什么需要对此特殊处理的，对与将恒等映射改写为3x3卷积的情况，则需要注意分组卷积的组数。</p>
<blockquote>
<p>分组卷积，就是将<span
class="math inline">\(C_1\)</span>维的输入tensor和<span
class="math inline">\(C_2/g\)</span>个<span
class="math inline">\(C_1\)</span>维卷积核分成<span
class="math inline">\(g\)</span>个<span
class="math inline">\(C_1/g\)</span>维的卷积，得到的<span
class="math inline">\(g\)</span>个<span
class="math inline">\(C_2/g\)</span>的tensor在channel维度拼接到一起。因此，对于恒等映射等价的第<span
class="math inline">\(i\)</span>个卷积核，应该只在<span
class="math inline">\(i ~\text{mod}~
g\)</span>维的3x3的矩阵的中心为1。</p>
</blockquote>
<p>对于一个<code>RepVGGBlock</code>，作者定义了<code>switch_to_deploy</code>方法，在训练后，推理前，遍历网络并对所有<code>RepVGGBlock</code>调用该方法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">switch_to_deploy</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">hasattr</span>(self, <span class="string">&#x27;rbr_reparam&#x27;</span>):</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="comment"># 获得与3个分支的作用等价的1个卷积核以及偏差</span></span><br><span class="line">    kernel, bias = self.get_equivalent_kernel_bias()</span><br><span class="line">    <span class="comment"># 构造新的3x3卷积，各种参数配置保持不变，但使用计算好的参数</span></span><br><span class="line">    self.rbr_reparam = nn.Conv2d(in_channels=self.rbr_dense.conv.in_channels, out_channels=self.rbr_dense.conv.out_channels,</span><br><span class="line">                                 kernel_size=self.rbr_dense.conv.kernel_size, stride=self.rbr_dense.conv.stride,</span><br><span class="line">                                 padding=self.rbr_dense.conv.padding, dilation=self.rbr_dense.conv.dilation, groups=self.rbr_dense.conv.groups, bias=<span class="literal">True</span>)</span><br><span class="line">    self.rbr_reparam.weight.data = kernel</span><br><span class="line">    self.rbr_reparam.bias.data = bias</span><br><span class="line">    <span class="comment"># 删除训练时模型中的分支结构</span></span><br><span class="line">    self.__delattr__(<span class="string">&#x27;rbr_dense&#x27;</span>)</span><br><span class="line">    self.__delattr__(<span class="string">&#x27;rbr_1x1&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">hasattr</span>(self, <span class="string">&#x27;rbr_identity&#x27;</span>):</span><br><span class="line">        self.__delattr__(<span class="string">&#x27;rbr_identity&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">hasattr</span>(self, <span class="string">&#x27;id_tensor&#x27;</span>):</span><br><span class="line">            self.__delattr__(<span class="string">&#x27;id_tensor&#x27;</span>)</span><br><span class="line">            self.deploy = <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_equivalent_kernel_bias</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;三合一</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    kernel3x3, bias3x3 = self._fuse_bn_tensor(self.rbr_dense)</span><br><span class="line">    kernel1x1, bias1x1 = self._fuse_bn_tensor(self.rbr_1x1)</span><br><span class="line">    kernelid, biasid = self._fuse_bn_tensor(self.rbr_identity)</span><br><span class="line">    <span class="keyword">return</span> kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_pad_1x1_to_3x3_tensor</span>(<span class="params">self, kernel1x1</span>):</span><br><span class="line">    <span class="keyword">if</span> kernel1x1 <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.nn.functional.pad(kernel1x1, [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_fuse_bn_tensor</span>(<span class="params">self, branch</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;吸BN</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> branch <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(branch, nn.Sequential):</span><br><span class="line">        <span class="comment"># 3x3或1x1的conv_bn分支</span></span><br><span class="line">        kernel = branch.conv.weight</span><br><span class="line">        running_mean = branch.bn.running_mean</span><br><span class="line">        running_var = branch.bn.running_var</span><br><span class="line">        gamma = branch.bn.weight</span><br><span class="line">        beta = branch.bn.bias</span><br><span class="line">        eps = branch.bn.eps</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 恒等映射分支（只有一个BN层）</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">isinstance</span>(branch, nn.BatchNorm2d)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(self, <span class="string">&#x27;id_tensor&#x27;</span>):</span><br><span class="line">            input_dim = self.in_channels // self.groups</span><br><span class="line">            kernel_value = np.zeros((self.in_channels, input_dim, <span class="number">3</span>, <span class="number">3</span>), dtype=np.float32)  <span class="comment"># 构造的3x3卷积核</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.in_channels):</span><br><span class="line">                kernel_value[i, i % input_dim, <span class="number">1</span>, <span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">                self.id_tensor = torch.from_numpy(kernel_value).to(branch.weight.device)</span><br><span class="line">                kernel = self.id_tensor</span><br><span class="line">                running_mean = branch.running_mean</span><br><span class="line">                running_var = branch.running_var</span><br><span class="line">                gamma = branch.weight</span><br><span class="line">                beta = branch.bias</span><br><span class="line">                eps = branch.eps</span><br><span class="line">                std = (running_var + eps).sqrt()</span><br><span class="line">     t = (gamma / std).reshape(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">     <span class="keyword">return</span> kernel * t, beta - running_mean * gamma / std</span><br></pre></td></tr></table></figure>
<h3 id="整体架构">整体架构</h3>
<p>作者配置架构的原则就两个：简单的根据经验设置+减少参数；</p>
<p><img src="image-20221003142015913.png" /></p>
<p>上表展示了不同分辨率的特征图有几个block，以及每个block的通道数，其中<span
class="math inline">\(a, b\)</span>是缩放因子，有<span
class="math inline">\(a&lt;b\)</span>，stage1个stage5都只有一一个block的原因是为了减少参数量。</p>
<p>可选的，可以穿插地将部分block的groups设置为一个全局常数，比如2或者4。相邻的block都使用分组卷积，可能会导致通道内信息交换受限。</p>
<p>最新的代码中，作者添加了RepVGGplus，使用了更深的层数，辅助头，以及在非线性层之后使用<a
href="https://arxiv.org/pdf/1709.01507.pdf">SEBlock</a>。</p>
<h2 id="实验">实验</h2>
<p>这一部分作者想强调的点在于，他们使用了Pytorch官方教程中简单的数据增强，随心的模型配置，就让推理时的模型获得了相比于同样训练设置下参数相近的ResNet，
EfficientNet,
ResNeXt等更快的推理速度和更好的性能（尽管该模型往往参数量更大，FLOPs更大，但速度却最快）。</p>
<p>在消融实验中，作者不仅证明了3个并行分支，一个不能少，并且，通过将ResNet中的Block替换为RepVGG
Block，证明了性能的优越并不是靠在训练时堆参数得到的。</p>
<p>此外作者还刷了刷Cityscapes的点。</p>
<p>实验结果的细节见论文，并且在仓库中，作者更新了最新的模型配置与性能（在ImageNet上最高刷到了84.06%）。</p>
<h2 id="总结">总结</h2>
<p>本文提出的结构重参数化技巧让VGG风格的模型“再次伟大”，本质上还是利用数学上的等价转换将训练好的复杂模型简化。除了卷积，其他算子（如MLP）有没有响应的简化方法？该模型虽然速度快，适用于满载荷的业务场景，但是较高的FLOPs也将带来更多功耗，这在各种终端中也是至关重要的。</p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>结构重参数化</tag>
      </tags>
  </entry>
  <entry>
    <title>Swin Transformer 系列论文与代码阅读</title>
    <url>/2022/09/01/Swin-Transformer/</url>
    <content><![CDATA[<p>Swin Transformer是一种基于位移窗口的多尺度Vision
Transformer结构，通过在窗口而非全局上计算自注意力，将与图像分辨率呈平方复杂度的MSA减少到了线性复杂度；窗口位移的技巧又使得窗口之间发生连接，从而随着网络深度的增加，使得每一个窗口的感受野不断增大；类似于CNN层级结构的网络设计让其与各种下游任务能够很好的集成，而Transformer捕获长程信息的特点更让其在dense的任务上有了相比于之前要好得多的表现！本文将介绍我个人对Swin
Transformer模型结构的理解。</p>
<span id="more"></span>
<blockquote>
<p>patch和token是同一个事物的不同时期表现形式，说patch时，一般指在patch
embedding之前，那个<span class="math inline">\(ph\times pw \times
3\)</span>的张量，说道token时，通常就是指在embedding之后，每一个特征图上，那些<span
class="math inline">\(1\times d\)</span>的张量，在最初<span
class="math inline">\(d=ph\times pw \times
3\)</span>，后面会随着网络加深而改变；</p>
</blockquote>
<h2 id="方法">方法</h2>
<h3 id="整体结构">整体结构</h3>
<p><img src="image-20220901165324925.png" /></p>
<p>上图分别给出了Swin-T作为backbone的那部分结构和一个Transformer
Blocks的结构：</p>
<h4 id="patch-partition和linear-embedding">Patch Partition和Linear
Embedding</h4>
<p>这两部分与ViT中的相同，先将图像划分成<span
class="math inline">\(\frac H4\times\frac W4\times3\)</span>个<span
class="math inline">\(4\times4\)</span>的patch，即图中的<span
class="math inline">\(\frac H4\times\frac W4 \times
48\)</span>，然后再使用线性变换将每个patch的维度从48提升到<span
class="math inline">\(C=96\)</span>；</p>
<p>在代码中为<code>PatchEmbed</code>模块，只用了一个<span
class="math inline">\(k=4,s=4\)</span>的卷积和一些通道顺序交换的操作就完成了这两步：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">// init</span><br><span class="line">self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)</span><br><span class="line"></span><br><span class="line">// forward</span><br><span class="line">B, C, H, W = x.shape</span><br><span class="line">x = self.proj(x).flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># B Ph*Pw C</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="swin-transformer-block">Swin Transformer Block</h4>
<p>该模块的作用是patch与patch之间的注意力，由W-MSA与SW-MSA与若干MLP级联组成，W-MSA的作用是计算窗口内部各个token的注意力，而SW-MSA还能通过窗口的移位，实现了（原本）跨窗口的token之间的注意力计算；记每个窗口包含<span
class="math inline">\(M\times M\)</span>个token，默认情况下<span
class="math inline">\(M=7\)</span>；</p>
<blockquote>
<p>之前有一个token-to-token Transformer的工作是使用了soft
partition，即token/patch之间的划分是互相重叠的，尽管token是window的一个组成单元，但是能不能让window划分的时候，也有一些overlap，获得跨window的信息呢？——造成一些冗余，增加了<span
class="math inline">\(M\)</span>的大小，增加了计算复杂度</p>
</blockquote>
<p>我们在后文讨论该模块设计的细节，现在只要先知道该模块不改变输入张量的维度；</p>
<h4 id="patch-merging">Patch Merging</h4>
<p>这个模块，是使得Swin
Transformer具有CNN式的多尺度特征图的关键，也是可以直接被用于各种密集的下游任务，如语义分割、目标检测；</p>
<p>做法很简单，每次把在空间上相邻的<span class="math inline">\(2\times
2\)</span>个领域在通道维度上拼接起来，使得特征图长宽各减半，而通道数变为原来的4倍；再用线性变换将通道数降至原来的2倍；</p>
<p>代码中的<code>PatchMerging</code>模块，实现时使用类似跨行采样的方式：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        x: B, H*W, C</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">    H, W = self.input_resolution</span><br><span class="line">    B, L, C = x.shape</span><br><span class="line">    <span class="keyword">assert</span> L == H * W, <span class="string">&quot;input feature has wrong size&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> H % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">and</span> W % <span class="number">2</span> == <span class="number">0</span>, <span class="string">f&quot;x size (<span class="subst">&#123;H&#125;</span>*<span class="subst">&#123;W&#125;</span>) are not even.&quot;</span></span><br><span class="line"></span><br><span class="line">    x = x.view(B, H, W, C)</span><br><span class="line"></span><br><span class="line">    x0 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">    x1 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">    x2 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">    x3 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">    x = torch.cat([x0, x1, x2, x3], -<span class="number">1</span>)  <span class="comment"># B H/2 W/2 4*C</span></span><br><span class="line">    x = x.view(B, -<span class="number">1</span>, <span class="number">4</span> * C)  <span class="comment"># B H/2*W/2 4*C</span></span><br><span class="line"></span><br><span class="line">    x = self.norm(x)</span><br><span class="line">    x = self.reduction(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h4 id="后接各种头">后接各种头</h4>
<p>在4个Stage过后，得到的是一个特征图：</p>
<p>如果是分类任务，在这里没有使用额外的cls
token，而是使用所有token的平均（实现时用全局平均池化）得到<span
class="math inline">\(1\times 8C\)</span>，之后再映射到<span
class="math inline">\(1\times 1000\)</span>得到类别分数；</p>
<p>后边还可以界用于语义分割和目标检测的各种head；</p>
<h3 id="w-msa与sw-msa">W-MSA与SW-MSA</h3>
<p>这两个模块体现了本文的精华；</p>
<h4 id="基于窗口计算sa">基于窗口计算SA</h4>
<p>这个设计的主要作用是降低了复杂度，对于W-MSA而言，只需要在送入一般的transformer
block之前调整一下tensor的尺寸，在<code>SwinTransformerBlock</code>中，有</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_windows = x_windows.view(-<span class="number">1</span>, self.window_size * self.window_size, C)  <span class="comment"># nW*B, window_size*window_size, C</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># W-MSA/SW-MSA</span></span><br><span class="line">attn_windows = self.attn(x_windows, mask=self.attn_mask)  <span class="comment"># nW*B, window_size*window_size, C</span></span><br></pre></td></tr></table></figure>
<h4 id="在第二个msa改变窗口划分方式">在第二个MSA改变窗口划分方式</h4>
<p>下面这幅图展示了在W-MSA和SW-MSA中的划分窗口划分方式，其中在红框代表一个进行SA计算的窗口：</p>
<figure>
<img src="image-20220901191216130.png" alt="image-20220901191216130" />
<figcaption aria-hidden="true">image-20220901191216130</figcaption>
</figure>
<p>其实只是完全按照这样的窗口划分进行计算时完全可以的，但是右侧的9个窗口，大小不一，不能放进同一个batch中进行计算，效率会变低，有两种方案：</p>
<ol type="1">
<li>使用padding=&gt;同样会降低效率</li>
<li>使用循环移位+mask，具体方案如下图所示：</li>
</ol>
<p><img src="image-20220901191732740.png" /></p>
<p>在这个<a
href="https://github.com/microsoft/issues/38#issuecomment-823806591">issue</a>下作者也给出了一个解释mask方式的图：</p>
<p><img src="145946639-f9da3139-a793-46aa-ba71-aadaa45706a7.png" /></p>
<ul>
<li>对新划分的窗口进行循环移位，得到一个可以划分为4个相同大小的窗口的图；</li>
<li>除了window0以外，其他window在进行attention时，不同色块之间不能计算attention，可以通过对计算出来的attention
map加上一个mask，其中那些两两不能计算attention的位置，mask值为-100，使得在softmax之后很接近0；</li>
<li>最后在反向循环移位，恢复各个窗口原始位置；</li>
</ul>
<p><span class="math display">\[
A_{ij}=\text{softmax}(Q_{i·}(K_{j.})^T/\sqrt{d_k} + B_{ij}) + Mask_{ij}
\]</span></p>
<p>其中，<span
class="math inline">\(B\)</span>是后文介绍的相对位置编码，<span
class="math inline">\(Mask\)</span>是Attn Mask；记<span
class="math inline">\(\delta_i\)</span>为左图中新window（总数为4的）的第<span
class="math inline">\(i\)</span>个token所在的实际window（总数为9的）的序号（将矩形形排列的token拉直成线性排列之后的编号），则
<span class="math display">\[
Mask_{ij}=-100^{[\delta_i\ne\delta_j]}, \ \ i,j\in[1,M^2]
\]</span></p>
<p>在<code>SwinTransformerBlock</code>中，构建mask的代码如下：</p>
<blockquote>
<p>下面的代码体现出了作者对于向量化的熟练程度，不过我认为反正就在四个block的初始化过程中执行的代码，用最笨的for循环生成mask也不会影响效率的；之后相对位置编码的查询表的生成也是如此；</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># calculate attention mask for SW-MSA</span></span><br><span class="line">H, W = self.input_resolution</span><br><span class="line">img_mask = torch.zeros((<span class="number">1</span>, H, W, <span class="number">1</span>))  <span class="comment"># 1 H W 1</span></span><br><span class="line">h_slices = (<span class="built_in">slice</span>(<span class="number">0</span>, -self.window_size),</span><br><span class="line">            <span class="built_in">slice</span>(-self.window_size, -self.shift_size),</span><br><span class="line">            <span class="built_in">slice</span>(-self.shift_size, <span class="literal">None</span>))</span><br><span class="line">w_slices = (<span class="built_in">slice</span>(<span class="number">0</span>, -self.window_size),</span><br><span class="line">            <span class="built_in">slice</span>(-self.window_size, -self.shift_size),</span><br><span class="line">            <span class="built_in">slice</span>(-self.shift_size, <span class="literal">None</span>))</span><br><span class="line">cnt = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> h <span class="keyword">in</span> h_slices:</span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> w_slices:</span><br><span class="line">        img_mask[:, h, w, :] = cnt</span><br><span class="line">        cnt += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">mask_windows = window_partition(img_mask, self.window_size)  <span class="comment"># nW, window_size, window_size, 1</span></span><br><span class="line">mask_windows = mask_windows.view(-<span class="number">1</span>, self.window_size * self.window_size)</span><br><span class="line">attn_mask = mask_windows.unsqueeze(<span class="number">1</span>) - mask_windows.unsqueeze(<span class="number">2</span>)</span><br><span class="line">attn_mask = attn_mask.masked_fill(attn_mask != <span class="number">0</span>, <span class="built_in">float</span>(-<span class="number">100.0</span>)).masked_fill(attn_mask == <span class="number">0</span>, <span class="built_in">float</span>(<span class="number">0.0</span>))</span><br></pre></td></tr></table></figure>
<p>循环移位的代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># x: B H W C</span></span><br><span class="line">shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(<span class="number">1</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<p>反向循环移位：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">shifted_x = window_reverse(attn_windows, self.window_size, H, W)  <span class="comment"># B H&#x27; W&#x27; C</span></span><br></pre></td></tr></table></figure>
<h4 id="关于复杂度">关于复杂度</h4>
<h3 id="相对位置编码">相对位置编码</h3>
<p>这个不是本文首次提出的，但是再消融实验中，使用相对位置编码对于Swin
Transformer十分有效；这里介绍一下思想与实现；</p>
<p>我们希望，在多个<span class="math inline">\(M \times
M\)</span>的窗口各自计算attention的时候，窗口内query与key相对位置相同时，在attention上添加<strong>同一个</strong>相对位置编码，并且这个编码是<strong>可以学习的</strong>，这就意味着，我们需要建立一个相对位置编码的look-up-table：</p>
<p>问题一：这个table需要多少项？首先在<span
class="math inline">\(M\times
M\)</span>的邻域内，token之间的相对距离可以分解为x,y轴坐标之差，只看一个轴，坐标之差的范围是<span
class="math inline">\([-M+1,M-1]\)</span>，共<span
class="math inline">\(2M-1\)</span>种，则2D的相对位置有<span
class="math inline">\((2M-1)\times(2M-1)\)</span>种；</p>
<p>问题二：最终的相对位移阵是<span class="math inline">\(M^2\times
M^2\)</span>的，该怎么在这个<span class="math inline">\(M^2\times
M^2\)</span>矩阵上分配这些相对位置编码？</p>
<p>下图展示了<span
class="math inline">\(M=2\)</span>时，按照作者的构造索引矩阵的代码的部分执行情况（我认为作者脑回路清奇，难以理解）：</p>
<p><img src="v2-0c99206fb39da67bae3415a650c38742_1440w.png" /></p>
<p>直接看最右边的索引矩阵的结果，记为<span
class="math inline">\(B\)</span>，则满足：<span
class="math inline">\(i-j\)</span>相同的位置使用相同的索引，也正好就是<span
class="math inline">\(3\times 3=9\)</span>种取值；</p>
<p>初始化look up table的代码，注意到不同head使用不同的table</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># define a parameter table of relative position bias</span></span><br><span class="line">self.relative_position_bias_table = nn.Parameter(</span><br><span class="line">    torch.zeros((<span class="number">2</span> * window_size[<span class="number">0</span>] - <span class="number">1</span>) * (<span class="number">2</span> * window_size[<span class="number">1</span>] - <span class="number">1</span>), num_heads))  <span class="comment"># 2*Wh-1 * 2*Ww-1, nH</span></span><br></pre></td></tr></table></figure>
<p>构造索引矩阵的代码，完全可以写得更简单？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># get pair-wise relative position index for each token inside the window</span></span><br><span class="line">coords_h = torch.arange(self.window_size[<span class="number">0</span>])</span><br><span class="line">coords_w = torch.arange(self.window_size[<span class="number">1</span>])</span><br><span class="line">coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  <span class="comment"># 2, Wh, Ww</span></span><br><span class="line">coords_flatten = torch.flatten(coords, <span class="number">1</span>)  <span class="comment"># 2, Wh*Ww</span></span><br><span class="line">relative_coords = coords_flatten[:, :, <span class="literal">None</span>] - coords_flatten[:, <span class="literal">None</span>, :]  <span class="comment"># 2, Wh*Ww, Wh*Ww</span></span><br><span class="line">relative_coords = relative_coords.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).contiguous()  <span class="comment"># Wh*Ww, Wh*Ww, 2</span></span><br><span class="line">relative_coords[:, :, <span class="number">0</span>] += self.window_size[<span class="number">0</span>] - <span class="number">1</span>  <span class="comment"># shift to start from 0</span></span><br><span class="line">relative_coords[:, :, <span class="number">1</span>] += self.window_size[<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">relative_coords[:, :, <span class="number">0</span>] *= <span class="number">2</span> * self.window_size[<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">relative_position_index = relative_coords.<span class="built_in">sum</span>(-<span class="number">1</span>)  <span class="comment"># Wh*Ww, Wh*Ww</span></span><br><span class="line">self.register_buffer(<span class="string">&quot;relative_position_index&quot;</span>, relative_position_index)</span><br></pre></td></tr></table></figure>
<p>在前向传播过程中查表，添加相对位置编码的代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-<span class="number">1</span>)].view(</span><br><span class="line">    self.window_size[<span class="number">0</span>] * self.window_size[<span class="number">1</span>], self.window_size[<span class="number">0</span>] * self.window_size[<span class="number">1</span>], -<span class="number">1</span>)  <span class="comment"># Wh*Ww,Wh*Ww,nH</span></span><br><span class="line">relative_position_bias = relative_position_bias.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>).contiguous()  <span class="comment"># nH, Wh*Ww, Wh*Ww</span></span><br><span class="line">attn = attn + relative_position_bias.unsqueeze(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>当在微调过程中，窗口大小改变时，需要对预训练过程中的相对位置编码进行双三次插值；</p>
<h2 id="实验">实验</h2>
<p>作者在论文中汇报了在ImageNet-1K, COCO,
ADE20K上的图像分类、目标检测、语义分割的结果，效果远好于该论文之前的方法，具体可以见原文；</p>
<p>消融实验部分体现了位移窗口和相对位置编码十分有效；</p>
<h2 id="swin-transformer-v2">Swin Transformer V2</h2>
<p>本文试图解决训练大模型时的三个问题：</p>
<ol type="1">
<li>训练不稳定性（在本文中揭示）；</li>
<li>预训练和微调过程中分辨率的差距，导致的精度下降；</li>
<li>缺少有标签数据；</li>
</ol>
<p>作者训练了一个30亿参数的Swin Transformer
V2模型（看看就行），能够使用高达1536x1536的图像作为输入；</p>
<p>作者使用如下三种解决方案，前两种都在下图中体现：</p>
<figure>
<img src="image-20220903114141714.png" alt="image-20220903114141714" />
<figcaption aria-hidden="true">image-20220903114141714</figcaption>
</figure>
<h3 id="ln后移并使用归一化cos距离">LN后移并使用归一化cos距离</h3>
<p>作者观察到，在V1模型中，随着参数的增加，深层block的输出特征的方差会越来越大，直至不能训练：（下图中圆形点为V1版本，三角形点为V2版本）</p>
<figure>
<img src="image-20220903114100072.png" alt="image-20220903114100072" />
<figcaption aria-hidden="true">image-20220903114100072</figcaption>
</figure>
<p>将LN移动到残差连接之前，并且将受向量长度、幅值影响的点积距离换成余弦距离，并学习归一化参数；</p>
<p>移动<code>LN</code>位置的变化发生在<code>SwinTransformerBlock</code>中，不再赘述；</p>
<p>余弦距离归一化参数，在对数空间内学习：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.logit_scale = nn.Parameter(torch.log(<span class="number">10</span> * torch.ones((num_heads, <span class="number">1</span>, <span class="number">1</span>))), requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>余弦距离计算，其中归一化参数不超过100；</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cosine attention</span></span><br><span class="line">attn = (F.normalize(q, dim=-<span class="number">1</span>) @ F.normalize(k, dim=-<span class="number">1</span>).transpose(-<span class="number">2</span>, -<span class="number">1</span>))</span><br><span class="line">logit_scale = torch.clamp(self.logit_scale, <span class="built_in">max</span>=torch.log(torch.tensor(<span class="number">1.</span> / <span class="number">0.01</span>))).exp()</span><br><span class="line">attn = attn * logit_scale</span><br></pre></td></tr></table></figure>
<h3
id="在对数空间中建立连续的相对位置编码模型">在对数空间中建立连续的相对位置编码模型</h3>
<p>基于查表的思想的相对位置坐标的思想没有变，只是生成相对位置的模型使用MLP，这也意味着在<code>forward</code>中原来每次查表的过程，现在都要多一次对如下模型的训练/推理：
<span class="math display">\[
B(\Delta x, \Delta y) = \mathcal{G}(\Delta x, \Delta y)
\]</span>
相比于插值方法，这种方法连续性更好；同时考虑到微调部分相比于的分辨率差距通常能够达到成倍的增长，在线性空间中建模，外推比较大，转化到对数空间中会减小外推比：
<span class="math display">\[
B_x = \text{sign}(x)\cdot\log(1+|\Delta x|)\\
B_y = \text{sign}(y)\cdot\log(1+|\Delta y|)
\]</span> 这是模型<span class="math inline">\(\mathcal{G}\)</span>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.cpb_mlp = nn.Sequential(nn.Linear(<span class="number">2</span>, <span class="number">512</span>, bias=<span class="literal">True</span>),</span><br><span class="line">                             nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">                             nn.Linear(<span class="number">512</span>, num_heads, bias=<span class="literal">False</span>))</span><br></pre></td></tr></table></figure>
<p>这是构造模型输入的代码，对于<span class="math inline">\(M\times
M\)</span>的当前窗口，有<span
class="math inline">\((2M-1)\times(2M-1)\)</span>相对位置坐标，适当归一化后，构造部分的代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># get relative_coords_table</span></span><br><span class="line">relative_coords_h = torch.arange(-(window_size[<span class="number">0</span>] - <span class="number">1</span>), window_size[<span class="number">0</span>], dtype=torch.float32)</span><br><span class="line">relative_coords_w = torch.arange(-(window_size[<span class="number">1</span>] - <span class="number">1</span>), window_size[<span class="number">1</span>], dtype=torch.float32)</span><br><span class="line">relative_coords_table = torch.stack(</span><br><span class="line">    torch.meshgrid([relative_coords_h,</span><br><span class="line">                    relative_coords_w])).permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).contiguous().unsqueeze(<span class="number">0</span>)  <span class="comment"># 1, 2*Wh-1, 2*Ww-1, 2</span></span><br><span class="line"><span class="keyword">if</span> pretrained_window_size[<span class="number">0</span>] &gt; <span class="number">0</span>:</span><br><span class="line">    relative_coords_table[:, :, :, <span class="number">0</span>] /= (pretrained_window_size[<span class="number">0</span>] - <span class="number">1</span>)</span><br><span class="line">    relative_coords_table[:, :, :, <span class="number">1</span>] /= (pretrained_window_size[<span class="number">1</span>] - <span class="number">1</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    relative_coords_table[:, :, :, <span class="number">0</span>] /= (window_size[<span class="number">0</span>] - <span class="number">1</span>)</span><br><span class="line">    relative_coords_table[:, :, :, <span class="number">1</span>] /= (window_size[<span class="number">1</span>] - <span class="number">1</span>)</span><br><span class="line">relative_coords_table *= <span class="number">8</span>  <span class="comment"># normalize to -8, 8</span></span><br><span class="line">relative_coords_table = torch.sign(relative_coords_table) * torch.log2(</span><br><span class="line">    torch.<span class="built_in">abs</span>(relative_coords_table) + <span class="number">1.0</span>) / np.log2(<span class="number">8</span>)</span><br></pre></td></tr></table></figure>
<p>索引矩阵的构造与V1相同；</p>
<p>在前向传播过程中，添加相对位置编码的代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">relative_position_bias_table = self.cpb_mlp(self.relative_coords_table).view(-<span class="number">1</span>, self.num_heads)</span><br><span class="line">relative_position_bias = relative_position_bias_table[self.relative_position_index.view(-<span class="number">1</span>)].view(</span><br><span class="line">    self.window_size[<span class="number">0</span>] * self.window_size[<span class="number">1</span>], self.window_size[<span class="number">0</span>] * self.window_size[<span class="number">1</span>], -<span class="number">1</span>)  <span class="comment"># Wh*Ww,Wh*Ww,nH</span></span><br><span class="line">relative_position_bias = relative_position_bias.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>).contiguous()  <span class="comment"># nH, Wh*Ww, Wh*Ww</span></span><br><span class="line">relative_position_bias = <span class="number">16</span> * torch.sigmoid(relative_position_bias)</span><br><span class="line">attn = attn + relative_position_bias.unsqueeze(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>这种改进的效果，在下表中体现出来了，原来的方案，如果不在高分辨率的输入上微调，准确率会严重下降，微调后才能恢复（这也启发作者问题可能出现在相对编码处）；使用CPB效果显著；</p>
<figure>
<img src="image-20220903122014122.png" alt="image-20220903122014122" />
<figcaption aria-hidden="true">image-20220903122014122</figcaption>
</figure>
<h3 id="自监督的预训练">自监督的预训练</h3>
<p>作者将具体方法用另一篇论文讲述：<strong>SimMIM: a Simple Framework
for Masked Image Modeling</strong>；这里做如下简单记录：</p>
<p>Masked language modeling与Maskeed image modeling有如下区别：</p>
<ul>
<li>图像像素间存在高度相关性，可能训练出来的模型仅仅是拷贝邻近的像素，而非基于语义的预测；</li>
<li>原始图像信号是low-level的而语言中的词是high-level的；</li>
<li>图像信号是连续的而语言信号是离散的；</li>
</ul>
<p>作者设计的框架结构可以分为四个组件，分别负责mask策略、encoder、预测头、目标函数，其中encoder使用ViT或者Swin
Transformer：</p>
<ul>
<li>使用随机mask，对齐到patch，对于ViT，所有mask都是32x32的，而对于Swin
Transformer，mask的块大小从4x4到32x32不等；由于像素间的相关性很强，掩码率通常远高于语言模型；</li>
<li>预测头，作者尝试了2-layer-MLP，inverse Swin-T, inverse
Swin-B，发现轻量的线性头不仅训练成本更低，而且使用很重的头生成高分辨率的补全，对下游任务的调整没有帮助；</li>
<li>把预测原始像素当成一个输入特征向量，输出mask部分预测的RGB的回归任务，根据下采样率，将特征图的特征向量映射到原始分辨率空间（例如在Swin
Transformer的32x下采样的特征图上的一个token映射到<span
class="math inline">\(3072=32\times 32\times 3\)</span>的输出）；</li>
<li>使用<span class="math inline">\(l_1\)</span>损失；</li>
</ul>
<p>本论文实验的细节详见原文，这里就不记录了；</p>
<h2 id="参考链接">参考链接</h2>
<p><a
href="https://www.bilibili.com/video/BV13L4y1475U?spm_id_from=333.337.search-card.all.click&amp;vd_source=a74e6b349553a485ac5c6643294f25d1">bilibili
- Swin Transformer 论文精读</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/367111046">知乎 - 图解Swin
Transformer</a></p>
<p><a
href="https://github.com/microsoft/Swin-Transformer">github代码</a></p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>学习结构化稀疏：精度与速度的双赢？</title>
    <url>/2023/02/04/Structured-Sparsity/</url>
    <content><![CDATA[<img src="/2023/02/04/Structured-Sparsity/fig2.png" class="">
<p><strong>论文</strong>：<a
href="https://proceedings.neurips.cc/paper/2016/hash/41bfd20a38bb1b0bec75acf0845530a7-Abstract.html"><strong>Learning
Structured Sparsity in Deep Neural Networks</strong></a></p>
<p><strong>作者</strong>：<em>Wei Wen, Chunpeng Wu, Yandan Wang, Yiran
Chen, Hai Li</em></p>
<p><strong>一作单位</strong>：University of Pittsburgh</p>
<p><strong>录用情况</strong>：Neurips'2016</p>
<blockquote>
<p>本文通过添加在滤波器数量、通道、形状维度以及深度上的group
lasso正则化，让模型学习到结构化稀疏，后再微调；实验表明，这种结构化稀疏不需要很大的稀疏率就可以在多种设备上产生可观的加速效果，甚至还能减轻过拟合从而提升性能。</p>
</blockquote>
<span id="more"></span>
<h2 id="方法">方法</h2>
<h3 id="结构化稀疏的优势">结构化稀疏的优势</h3>
<p>非结构化稀疏、非结构化剪枝的方法，能够获得凌乱稀疏的权重矩阵，这样对数据局部性的利用率不高，导致需要达到很高的稀疏率（&gt;95%）才能获得实质性的加速；</p>
<blockquote>
<p>有意思的是，本文的图1位置没有用来解释本文的方法，或者展示本文方法的效果，而是展示了在用L1
norm为AlexNet学习非结构稀疏后，其各层非常有限的加速比；</p>
</blockquote>
<p><img src="fig1.png" /></p>
<p>低秩近似的方法，是结构化的，但是在获得分解结构需要多轮的分解和微调，才能找到精度与速度的权衡；</p>
<p>结构化稀疏的朴素理解，就是把不重要的卷积核、不重要的通道、不重要的卷积核位置全部置0，在推理前移除这些全0的部分，之后仍可以使用off-the-shelf的算子去实现，很好地利用数据局部性；</p>
<h3 id="几种结构化稀疏的模式">几种结构化稀疏的模式</h3>
<p>本文记录一个卷积层4D张量的符号是 <span
class="math inline">\(W^{(l)}\in \mathbb{R}^{N_l\times C_l \times M_l
\times K_l}\)</span>，其中 <span class="math inline">\(N_l, C_l , M_l ,
K_l\)</span> 分别代表第 <span class="math inline">\(l\)</span>
层卷积参数的卷积核数量，通道数，高，宽；</p>
<p>通常的，带有稀疏化正则项的优化目标是：</p>
<p><span class="math display">\[
E(W) = E_D(W) + \lambda R(W) + \lambda_g \sum_{l=1}^{L}R_g(W^{(l)})
\]</span></p>
<p><span class="math inline">\(E_D\)</span> 表示分类损失，<span
class="math inline">\(R\)</span> 是对所有权重的非结构化正则化函数，<span
class="math inline">\(R_g\)</span>
是对某组的结构化正则函数，组与组之间可以有重叠；使用 group lasso
因为其能够将一组的参数全部变为0；</p>
<h4 id="去除不重要的滤波器和通道">去除不重要的滤波器和通道</h4>
<p>将第 <span class="math inline">\(l\)</span> 层的第 <span
class="math inline">\(n_l\)</span> 个卷积核视为一组，将第 <span
class="math inline">\(l\)</span> 层的第 <span
class="math inline">\(c_l\)</span> 个通道视为一组；因为减少第 <span
class="math inline">\(l\)</span> 层的卷积核数量，会响应减少第 <span
class="math inline">\(l+1\)</span>
层的通道数，因此这里将这两种结构化稀疏同时使用；</p>
<p><span class="math display">\[
\lambda_n \sum_{l=1}^L\left(\sum_{n_l=1}^{N_l}\lVert W_{n_l,:,:,:}^{(l)}
\rVert _g\right) +
\lambda_c \sum_{l=1}^L\left(\sum_{c_l=1}^{C_l}\lVert W_{:,c_l,:,:}^{(l)}
\rVert _g\right)
\]</span></p>
<p>进一步地，考虑到3D卷积不过是2D卷积结果之和，我们可以把每一个2D的卷积核作为一组，正则化项写为：
<span class="math display">\[
\lambda_n \sum_{l=1}^L\left(\sum_{n_l=1}^{N_l}\sum_{c_l=1}^{C_l}\lVert
W_{n_l,c_l,:,:}^{(l)} \rVert _g\right)
\]</span></p>
<p>这样每一组更小，更容易被全部zero-out；</p>
<h4 id="去除不重要的位置">去除不重要的位置</h4>
<p>将第 <span class="math inline">\(l\)</span> 层各卷积核的 <span
class="math inline">\(c_l,m_l,k_l\)</span>
位置的权重作为一组；删除不重要的位置会得到非长方体形状的卷积，似乎没有off-the-shelf的算子支持；</p>
<p><span class="math display">\[
\lambda_s \sum_{l=1}^L\left(
\sum_{c_l=1}^{C_l}\sum_{m_l=1}^{M_l}\sum_{k_l=1}^{K_l}\lVert W^{(l)}_{:,
c_l, m_l, k_l} \rVert _g
\right)
\]</span></p>
<p>但是思考卷积运算转化为矩阵乘法的实现，其中卷积核被展开为 <span
class="math inline">\(N_l\times C_lM_lK_l\)</span>
的矩阵，删除不重要的卷积核相当于删除行，删除不重要的位置相当于删除列，因而后文也会将filter-wise稀疏度叫做row-wise稀疏度，将shape-wise稀疏度叫做column-wise稀疏度；</p>
<p>这种转化为矩阵乘法，消除行列的思路更早的出现在论文<a
href="https://openaccess.thecvf.com/content_cvpr_2016/papers/Lebedev_Fast_ConvNets_Using_CVPR_2016_paper.pdf">Fast
ConvNets Using Group-wise Brain Damage</a>中，这篇文章详细地描述了
row-wise 和 column-wise
的稀疏度；在稀疏化+微调的方法上，这篇文章通过为正则化项引入阈值，避免将距离过远的组变成0，实现了一种Gradual
group-wise sparsification，调参更简单；</p>
<h4 id="去除不重要的层">去除不重要的层</h4>
<p>以一层的参数为一组： <span class="math display">\[
\lambda_d \sum_{l=1}^L \lVert W^{(l)} \rVert _g
\]</span></p>
<p>与之前几种模式不同的是，训练过程中将一层的参数全部置位0，会使得前向传播与反向传播在此中断，因此使用该正则项的层必须有shortcut连接；</p>
<h2 id="实验">实验</h2>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>模型压缩</tag>
        <tag>稀疏化</tag>
      </tags>
  </entry>
  <entry>
    <title>把卷积分解成sum-pooling和更小的卷积</title>
    <url>/2023/01/01/Structured-Convolutions/</url>
    <content><![CDATA[<img src="/2023/01/01/Structured-Convolutions/fig1.png" class="">
<p><strong>论文</strong>：<a
href="https://proceedings.neurips.cc/paper/2020/hash/3be0214185d6177a9aa6adea5a720b09-Abstract.html"><strong>Structured
Convolutions for Efficient Neural Network Design</strong></a></p>
<p><strong>作者</strong>：<em>Yash Bhalgat, Yizhe Zhang, Jamie Menjay
Lin, Fatih Porikli</em></p>
<p><strong>一作单位</strong>：Qualcomm AI Research</p>
<p><strong>录用情况</strong>：Neurips'2020</p>
<blockquote>
<p>本文提出了一种Structured Convolution，将传统的卷积运算分解为sum
pooling操作和卷积核更小的卷积操作，从而能够获得更小、更快的推理模型；为了使得分解带来的损失更小，作者还设计了一种Structural
Regularization的正则项添加到训练过程中。该方法不亚于先前的各种张量分解或者通道剪枝方法。笔者认为，本文的分解方法很有意思。</p>
</blockquote>
<span id="more"></span>
<h2 id="方法">方法</h2>
<h3 id="composite-kernels">Composite Kernels</h3>
<p>模型压缩的方法都基于这样一条假设，神经网络都是过参数化的(over-parameterized)。本文主要关注的是卷积核的冗余性。首先，作者定义了卷积核的二元基（命名为Composite
Basis）以及这些基的线性组合得到的卷积核（命名为Composite Kernels）。</p>
<p>原文中的定义如下： <img src="def12.png" /></p>
<p>到目前为止，从原来的 <span class="math inline">\(C\times N^2\)</span>
的一个卷积核，变成了 <span class="math inline">\(M\times C\times
N^2\)</span> 的一组二值基和 <span class="math inline">\(M\)</span>
个权重；</p>
<p>作者带着我们回忆了一下线性代数知识，<span
class="math inline">\(M\)</span>个线性无关的基张成维度为<span
class="math inline">\(M\)</span>的子空间，其中每个张量的长度是<span
class="math inline">\(CN^2\)</span>，因此在<span
class="math inline">\(M&lt;CN^2\)</span>的条件下，这个子空间中的Composite
Kernels相比于任意的Kernels是降维的版本；</p>
<blockquote>
<p>笔者看到这里时有两个后来证明是没什么关联的联想：</p>
<ul>
<li><p>基的binary属性让我一开始想到量化和二值网络，实际上应该只是为了得到类似标准正交基，容易地拥有线性无关属性；</p></li>
<li><p>还会联想到矩阵分解，在<span
class="math inline">\(M\)</span>组分解方向中选出重要的方向，但是本文不是这样；与矩阵分解的核心区别是，矩阵分解得到的基和坐标都是“学得”的（不管是用SVD还是什么方法），而这里的基是固定的；</p></li>
</ul>
</blockquote>
<p>将这种对卷积核的分解带入卷积运算中，带来的收益是乘法运算次数的减少：</p>
<p><img src="comp_kernel_conv.png" /></p>
<p>这里用到了卷积的可加性，并且，利用 <span
class="math inline">\(\beta_m\)</span> 的二值化特征，其与<span
class="math inline">\(X\)</span>的逐点乘法转化成了仅在 <span
class="math inline">\(\beta_m = 1\)</span> 位置的加法（后文结合 <span
class="math inline">\(\beta_m\)</span>
的构造我们才能知道具体如何实现）；这样一来，将原来卷积需要的<span
class="math inline">\(CN^2\)</span>次乘法和<span
class="math inline">\(CN^2-1\)</span>次加法变成了 <span
class="math inline">\(\sum_{m=1}^{M}\mathrm{sum}(\beta_m)-1\)</span>
次加法；</p>
<p>论文的图一（本文文首图）展示了几种不同自由度的 <span
class="math inline">\(\beta_m\)</span>
的构造方式（图中带上了权重），至此，加法的次数相较于原来还是有可能增加的；</p>
<h3 id="structured-convolutions">Structured Convolutions</h3>
<p>作者提出了一种structured kernel的构造基的方式，定义和图示如下：</p>
<p><img src="def3.png" /></p>
<p><img src="fig23.png" /></p>
<p>图1(b)展示了一种2D structured kernel的构造；图2展示了一种3D
structured kernel的构造；图3以2D为例说明了为什么要用这样的构造；</p>
<p>在这种构造方式下，我们不需要再利用卷积的可加性拆成与每个基的卷积后再求和，因为这些基的和与<span
class="math inline">\(X\)</span>的卷积输出很有特点，正如先对<span
class="math inline">\(X\)</span>进行sum-pooling,
再与一个小卷积核做卷积一样，且这个小卷积核正是有所有的权重向量组成；</p>
<p>考虑多个卷积核的情况，容易发现，sum-pooling的结果是可以被各个卷积核复用的，这也是相比传统卷积能够减少加法次数的关键；</p>
<p><img src="fig4.png" /></p>
<p>在4.2节，作者分析了使用sum-pooling + structured
convolution相比于传统卷积在加法次数以及乘法次数上的提升比例，经分析（详见原文），作者使用
<span class="math inline">\(CN^2/cn^2\)</span>
作为卷积层的近似压缩比；</p>
<p>由于线性操作可以视为1*1的卷积，因此，上述压缩方式又可以扩展到线性层上，如图5所示：</p>
<p><img src="fig5.png" /></p>
<h3 id="分解卷积核">分解卷积核</h3>
<p>被分解的卷积核 <span class="math inline">\(W =
\sum_{m}\alpha_m\beta_m\)</span>，定义矩阵 <span
class="math inline">\(A\in \mathbb{R}^{CN^2\times cn^2}\)</span>，其第
<span class="math inline">\(i\)</span> 列都是基 <span
class="math inline">\(\beta_i\)</span> 的展开(vectorized)，定义 <span
class="math inline">\(\boldsymbol{\alpha} = [\alpha_1,\dots,
\alpha_{cn^2}]\)</span>，有： <span class="math display">\[
\mathrm{vectorized}(W) = A \cdot \boldsymbol{\alpha}
\]</span></p>
<p>我们需要表示出在structured kernel下，<span
class="math inline">\(A\)</span> 的形式；</p>
<p><span
class="math inline">\((C-c+1)\times(N-n+1)\times(N-n+1)\)</span>
的sum-pooling可以视为同等大小的全为1的卷积核，因此structured
convolution可以表示为： <span class="math display">\[
X * W = X * \mathbf{1}_{(C-c+1)\times(N-n+1)\times(N-n+1)} *
\boldsymbol{\alpha}_{c\times n\times n}
\]</span></p>
<p>因此有 <span class="math display">\[
W = \mathbf{1}_{(C-c+1)\times(N-n+1)\times(N-n+1)} *
\boldsymbol{\alpha}_{c\times n\times n}
\]</span></p>
<p>使用<a
href="https://zhuanlan.zhihu.com/p/63974249">im2col</a>的方法，在步长为1的条件下可以视为把输入表示成<a
href="https://en.wikipedia.org/wiki/Toeplitz_matrix">Topelitz矩阵</a>，有：
<span class="math display">\[
\mathrm{vectorized}(W) =
\mathrm{Topelitz}(\mathbf{1}_{(C-c+1)\times(N-n+1)\times(N-n+1)}) \cdot
\mathrm{vectorized}(\boldsymbol{\alpha}_{c\times n\times n})
\]</span></p>
<p>因此，我们得到 <span class="math display">\[
A = \mathrm{Topelitz}(\mathbf{1}_{(C-c+1)\times(N-n+1)\times(N-n+1)})
\]</span></p>
<p>在 <span class="math inline">\(A\)</span> 和 <span
class="math inline">\(W\)</span> 已知的情况下，如何求解出 <span
class="math inline">\(\alpha\)</span>？ <span class="math display">\[
\alpha = A^+W
\]</span> 其中，<span class="math inline">\(A^+\)</span> 是 <span
class="math inline">\(A\)</span> 的<a
href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse">伪逆</a>；这样，我们可以把对<span
class="math inline">\(W\)</span>的分解写成 <span class="math inline">\(W
= A\alpha =
AA^+W\)</span>，为了使得这种分解造成的性能损失更小，在训练时引入正则项：
<span class="math display">\[
\lambda\sum_{l=1}^L\frac{\|(I-A_lA_l^+)W_l\|_F}{\|W_l\|_F}
\]</span></p>
<p>相比于直接使用structured
convolution结构训练，这种带正则项训练，完成训练后分解的方式性能更好，因为正则项使得权重是从
<span class="math inline">\(\mathbb{R}^{C\times N\times N}\)</span>
渐进地压缩到 <span class="math inline">\(\mathbb{R}^{c\times n\times
n}\)</span> 中的；</p>
<h2 id="实验">实验</h2>
<p>在正文部分作者报告了ImageNet、CIFAR-10上的分类性能和CityScape上的分割性能，都是将常见的轻量级模型添加正则项后训练再分解得到的，并给出了保持性能版本(A)和尽力压缩版本(B)，详见原文；</p>
<h2 id="总结">总结</h2>
<p>基于固定的基(prefixed
basis)的分解方法先前已经有很多，笔者认为本文出彩之处就在于structured
kernel的构造基的方式，使得运算被简化成sum-pooling和更小的卷积；这种分解对于训练好的模型来说，是数据无关的；这种分解的另一个好处是，分解后算子仍然可以用传统算子完成，这意味着可以集成更多的模型压缩方法。</p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>模型压缩</tag>
        <tag>卷积分解</tag>
      </tags>
  </entry>
  <entry>
    <title>概率论与数理统计要点总结</title>
    <url>/2022/08/13/probability/</url>
    <content><![CDATA[<p>本文是根据陈希儒版《概率论与数理统计》总结的干货。</p>
<blockquote>
<p>相关知识需要大量的例子和证明来补充，笔者发现总结干货并不容易</p>
</blockquote>
<span id="more"></span>
<h2 id="一事件的概率">一、事件的概率</h2>
<p>第一章首先对概率论中使用的名词含义与生活中的相关名词含义进行了区分，小结如下：</p>
<ul>
<li><p><strong>概率</strong>是某一时间发生的数量指标，介于<span
class="math inline">\([0,1]\)</span>之间；生活中我们常常提到发生什么事的概率，通常是我们的主观推断，不是一个科学的概念；</p></li>
<li><p><strong>事件</strong>是对某种情况的描述，而不是已发生的情况，是否发生取决于试验后的结果，这样强调的目的是为了避免在理解一些和事件的概率时，与条件概率产生理解上的混淆；事件一般有一个明确界定的<strong>试验</strong>，即有一个可以罗列出的总体，或者总体的一个范围，并且事件是其中一个可以确定的试验的结果；把单一的试验结果称为<strong>基本事件</strong></p></li>
<li><p><strong>古典概率</strong>，是一种简单的概率模型，它假设全部试验结果是有限个的，且等可能成立的；另外还有<strong>几何概率</strong>，是古典概率到无限个试验结果的引申；</p></li>
<li><p><strong>概率的统计定义</strong>，就是通过大量多次的试验来使用频率去拟合概率，这不是概率的定义，而是一种估计方法，也可以作为假设检验的方法；</p></li>
<li><p><strong>概率的公理化定义</strong>（柯氏公理）：基本事件的集合为<span
class="math inline">\(\Omega\)</span>，考虑一个集类<span
class="math inline">\(\mathcal{F}\)</span>，其元素由<span
class="math inline">\(\Omega\)</span>的子集组成（包括<span
class="math inline">\(\empty\)</span>和<span
class="math inline">\(\Omega\)</span>自己），<span
class="math inline">\(\mathcal{F}\)</span>的每个元素<span
class="math inline">\(A\)</span>即称为事件，事件的概率<span
class="math inline">\(P(A)\)</span>满足三条公理：</p>
<ul>
<li><span class="math inline">\(0 \le P(A) \le 1\)</span></li>
<li><span class="math inline">\(P(\Omega)=1\)</span>，<span
class="math inline">\(P(\empty)=0\)</span></li>
<li>加法公理：若干个互斥事件和的概率，等于各个事件的概率之和</li>
</ul></li>
<li><p>排列组合：</p>
<ul>
<li><p><span class="math inline">\(n\)</span>个不同的物品取出<span
class="math inline">\(r\)</span>个的不同排列总数(<span
class="math inline">\(1\le r \le n\)</span>) <span
class="math display">\[
P_r^n=n(n-1)\dots(n-r+1)
\]</span></p></li>
<li><p><span class="math inline">\(n\)</span>个不同物品取出<span
class="math inline">\(r\)</span>个的不同组合总数(<span
class="math inline">\(1\le r \le n\)</span>) <span
class="math display">\[
C_r^n=\left(\begin{array}{c}
n\\r
\end{array}
\right)
=P_r^n/r!=n!/(r!(n-r)!)
\]</span></p></li>
<li><p>组合数又是二项展开式的系数： <span class="math display">\[
(a+b)^n=\sum_{i=0}^{n}\left(\begin{array}{c}n\\i\end{array}\right)a^ib^{n-i}
\]</span></p></li>
<li><p><span class="math inline">\(n\)</span>个不同物品分成<span
class="math inline">\(k\)</span>堆，各堆物品数分别为<span
class="math inline">\(r_1,\dots,r_k\)</span>的分法（堆和堆是有序的）：
<span class="math display">\[
n!/(r_1!\dots r_k!)
\]</span> 上式是多项展开式<span
class="math inline">\((x_1+\dots+x_k)^n\)</span>的<span
class="math inline">\(x_1^{r_1}\dots x_k^{r_k}\)</span>的系数</p></li>
</ul></li>
<li><p>按照上述事件的公理化定义，事件本身就是集合，一些集合中的关系可以引申到事件中的关系来，包括蕴含、包含、相等、互斥（不交）、对立（补）、和（并）、积（交）、差等，比较简单，按下不表；</p></li>
<li><p><strong>条件概率</strong>，在<span
class="math inline">\(B\)</span>发生下<span
class="math inline">\(A\)</span>发生的概率，<span
class="math inline">\(P(A|B)=P(AB)/P(B)\)</span></p></li>
<li><p>事件<span class="math inline">\(A\)</span>与<span
class="math inline">\(B\)</span>是独立的，当且仅当<span
class="math inline">\(P(AB)=P(A)P(B)\)</span>，这也是<strong>概率的乘法定理</strong>，进而可以推广到多个事件互相独立：设<span
class="math inline">\(A_1,A_2,\dots\)</span>为有限或者无限的时间，如果从其中任意取出有限个事件，都满足事件积的概率等于事件概率的积，则这些时间<strong>相互独立</strong>；相互独立可以推出两两独立，但是两两独立不一定相互独立</p>
<blockquote>
<p>就向判断一堆列向量线性无关一样，两两线性无关也不能保证总体线性无关</p>
</blockquote></li>
<li><p><strong>全概率公式</strong>：设<span
class="math inline">\(B_1,B_2,\dots\)</span>为有限个或无限个事件，两两互斥，每次试验中至少发生一个，则
<span class="math display">\[
\begin{aligned}
P(A)&amp;=P(AB_1)+P(AB_2)+\dots\\
&amp;=P(B_1)P(A|B_1)+P(B_2)P(A|B_2)+\dots
\end{aligned}
\]</span></p></li>
<li><p><strong>贝叶斯公式</strong>：<span
class="math inline">\(P(B_i|A)=P(AB_i)/P(A)=P(B_i)P(A|B_i)/\sum_jP(B_j)P(A|B_j)\)</span></p>
<blockquote>
<p>贝叶斯公式看起来就是移了个项，但是解释起来就牛逼多了：</p>
<p>把<span class="math inline">\(B_1,B_2,\dots\)</span>看成原因，把<span
class="math inline">\(A\)</span>看成结果，全概率公式计算在不同原因下的加权概率和，由因及果，而贝叶斯公式计算在已知结果发生下，是哪个原因导致该结果的概率；</p>
<p>人们原来对事件<span
class="math inline">\(B_i\)</span>的发生有一个经验概率（先验）<span
class="math inline">\(P(B_i)\)</span>，现在事件<span
class="math inline">\(A\)</span>发生了，那么我们使用贝叶斯公式，得到了<span
class="math inline">\(B_i\)</span>发生的新的概率（后验）<span
class="math inline">\(P(B_i|A)\)</span></p>
</blockquote></li>
</ul>
<h2 id="二随机变量及概率分布">二、随机变量及概率分布</h2>
<p>随机变量的概念比随机实践的概念更广，更利于研究，比如之前抛硬币，有两个基本事件，现在可以记为一个变量，有两种取值，每种值有一定的概率；</p>
<p>本章介绍了大量连续型概率分布和离散型概率分布的例子，说实话本人目前只在应用中接触过正态分布，对于大多数第一次在概率论课上接触的分布，想什么指数分布、泊松分布、<span
class="math inline">\(t\)</span>分布等等，在课后就没再见过，掌握的也不深；</p>
<ul>
<li><p>可以根据变量的取值特点分为离散型随机变量和连续型随机变量；</p></li>
<li><p><strong>分布函数</strong>，设<span
class="math inline">\(X\)</span>为一随机变量 <span
class="math display">\[
P(X\le x)=F(x), \ \ -\infty &lt; x &lt; \infty
\]</span> 该函数时单调非减的；并且在<span
class="math inline">\(x\rightarrow -\infty\)</span>时，<span
class="math inline">\(F(x)\rightarrow 0\)</span>，<span
class="math inline">\(x\rightarrow \infty\)</span>时，<span
class="math inline">\(F(x)\rightarrow 1\)</span>；</p></li>
<li><p><strong>二项分布</strong>：事件<span
class="math inline">\(A\)</span>单次发生的概率为<span
class="math inline">\(p\)</span>，经过<span
class="math inline">\(n\)</span>次独立试验之后，<span
class="math inline">\(A\)</span>发生<span
class="math inline">\(i\)</span>次记为事件<span
class="math inline">\(X\)</span>，记为<span class="math inline">\(X
\thicksim B(n, p)\)</span>，概率为： <span class="math display">\[
P(X=i)=b(i;n,p)=\left(\begin{array}{c}n\\i\end{array}\right)p^i(1-p)^{n-i},
i=0,1,\dots, n
\]</span></p></li>
<li><p><strong>泊松分布</strong>：二项分布的极限形式，当<span
class="math inline">\(n\)</span>很大，<span
class="math inline">\(p\)</span>很小而<span
class="math inline">\(np=\lambda\)</span>不太大时，<span
class="math inline">\(X\thicksim P(\lambda)\)</span>，概率为： <span
class="math display">\[
P(X=i) = e^{-\lambda}\lambda^i/i!
\]</span></p></li>
<li><p><strong>超几何分布</strong>：<span
class="math inline">\(X\)</span>为从<span
class="math inline">\(N\)</span>个商品中，随机抽出<span
class="math inline">\(n\)</span>个里面包含的废品数（总废品数<span
class="math inline">\(M\)</span>），与二项分布的差别是，这是不放回抽样：
<span class="math display">\[
  P(X=m)=
  \left(
  \begin{array}{}
  M\\m
  \end{array}
  \right)
  \left(
  \begin{array}{}
  N-M\\n-m
  \end{array}
  \right) /
  \left(
  \begin{array}{}
  N\\n
  \end{array}
  \right)
  \]</span> 当<span class="math inline">\(n\)</span>固定，<span
class="math inline">\(M/N=p\)</span>固定，<span
class="math inline">\(N\rightarrow \infty\)</span>时，<span
class="math inline">\(X\)</span>近似服从二项分布<span
class="math inline">\(B(n,p)\)</span></p></li>
<li><p><strong>负二项分布</strong>：</p></li>
<li><p>对于连续型随机分布，定义概率密度函数<span
class="math inline">\(f(x)=F&#39;(x)\)</span></p>
<ul>
<li><span class="math inline">\(f(x)\ge 0\)</span></li>
<li><span class="math inline">\(\int_{-\infty}^{+\infty}f(x)dx =
1\)</span></li>
<li><span class="math inline">\(P(a \le x \le b) = F(b) - F(a) =
\int_{a}^{b}f(x)dx\)</span></li>
</ul></li>
<li><p><strong>正态分布</strong>：<span class="math inline">\(X \sim
N(\mu, \sigma)\)</span> <span class="math display">\[
f(x) = (\sqrt{2\pi}\sigma)^{-1}e^{-(x-\mu)^2/2\sigma^2}, -\infty &lt; x
&lt; \infty
\]</span> <span
class="math inline">\(N(0,1)\)</span>为标准正态分布</p></li>
<li><p><strong>指数分布</strong>：其中<span
class="math inline">\(\lambda&gt;0\)</span> <span
class="math display">\[
f(x) = \left\{
\begin{array}{rc}
\lambda e^{-\lambda x},&amp;x&gt;0\\
0,&amp;x\le 0
\end{array}
\right.
\]</span></p>
<p><span class="math display">\[
F(x) = \left\{
\begin{array}{rc}
0,&amp;x\le 0\\
1 - e^{-\lambda x},&amp;x&gt;0\\
\end{array}
\right.
\]</span></p>
<blockquote>
<p>原件寿命的分布，<span
class="math inline">\(\lambda\)</span>为失效率，<span
class="math inline">\(\lambda^{-1}\)</span>即平均寿命</p>
</blockquote></li>
<li><p><strong>韦伯分布</strong>：</p></li>
<li><p><strong>均匀分布</strong>：</p></li>
</ul>
<p>以下是多维随机变量分布：</p>
<ul>
<li><p><strong>多项分布</strong>：</p></li>
<li><p><strong>二维正态分布</strong>： <span class="math display">\[
f(x_1,x_2)=(2\pi\sigma_1\sigma_2\sqrt{1-\rho^2}\exp(-\frac{1}{2(1-\rho^2)}\left(\frac{(x_1-a)^2}{\sigma_1^2}-\frac{2\rho(x_1-a)(x_2-b)}{\sigma_1\sigma_2}+\frac{(x_2-b)^2}{\sigma^2}\right))
\]</span></p></li>
<li><p><strong>边缘分布</strong>：<span
class="math inline">\(X=(X_1,\dots, X_n)\)</span>是一个<span
class="math inline">\(n\)</span>维随机变量，其分布<span
class="math inline">\(F\)</span>为一个<span
class="math inline">\(n\)</span>维分布，对于<span
class="math inline">\(X\)</span>的每一个分量<span
class="math inline">\(X_i\)</span>其一维分布<span
class="math inline">\(F_i\)</span>即称为边缘分布；边缘分布也不一定是对于一个分量的一维分布；<span
class="math inline">\(N(a,b,\sigma_1^2, \sigma_2^2,
\rho)\)</span>的两个边缘分布是<span class="math inline">\(N(a,
\sigma_1^2)\)</span>和<span
class="math inline">\(N(b,\sigma_2^2)\)</span></p></li>
<li><p><strong>条件分布</strong>：</p>
<p>离散型随机变量条件分布的概率密度很好写出，设<span
class="math inline">\((X_1,X_2)\)</span>是二维离散型随机变量，<span
class="math inline">\(X_1\)</span>的全部可能值为<span
class="math inline">\(a_1, a_2,\dots\)</span>, <span
class="math inline">\(X_2\)</span>的全部可能值为<span
class="math inline">\(b_1, b_2, \dots\)</span>，即联合概率分布 <span
class="math display">\[
p_{ij}=P(X_1=a_i, X_2=b_j), i,j = 1,2,\dots
\]</span> 则条件分布 <span class="math display">\[
P(X_1=a_i|X_2=b_j)=p_{ij}/P(X_2=b_j)=p_{ij}/\sum_kp_{kj}
\]</span>
连续性随机变量的条件分布也有比较直观的结论（推导过程稍微复杂）： <span
class="math display">\[
f(x_1,x_2)=f_2(x_2)f_1(x_1|x_2)
\]</span> 将上式两边对<span class="math inline">\(x_2\)</span>积分，有
<span class="math display">\[
f_1(x_1)=\int_{-\infty}^{+\infty}f_1(x_1|x_2)f_2(x_2)
\]</span> 解释为<span
class="math inline">\(X_1\)</span>的无条件密度，是<span
class="math inline">\(X_1\)</span>在<span
class="math inline">\(X_2\)</span>条件按<span
class="math inline">\(X_2\)</span>密度的加权平均，这也是全概率公式的概率密度的表现形式；</p>
<p>对弈二维正态分布<span class="math inline">\(N(a,b,\sigma_1^2,
\sigma_2^2, \rho)\)</span>，其<span
class="math inline">\(X=x_1\)</span>条件下，<span
class="math inline">\(X_2\)</span>满足<span
class="math inline">\(N(b+\rho\sigma_2\sigma_1^{-1}(x_1-a),
\sigma^2_2(1-\rho^2))\)</span>分布，可以定性的看出<span
class="math inline">\(\rho&gt;0\)</span>，随着<span
class="math inline">\(X_1\)</span>增加，<span
class="math inline">\(X_2\)</span>数据中心也增加，这称为正相关，反之，<span
class="math inline">\(\rho&lt;0\)</span>有负相关；</p></li>
<li><p><strong>随机变量的独立性</strong>，当且仅当 <span
class="math display">\[
f(x_1, \dots, x_n)=f(x_1)\dots f(x_n)
\]</span></p></li>
<li><p><strong>随机变量函数的分布</strong>，研究这个问题的目的是，为了知道观测变量的统计量(例如均值、方差)的分布；</p>
<p>设<span class="math inline">\(X\)</span>有密度函数<span
class="math inline">\(f(x)\)</span>，设<span
class="math inline">\(Y=g(X)\)</span>，<span
class="math inline">\(g\)</span>严格单调，反函数<span
class="math inline">\(h=g^{-1}\)</span>是可导的，则<span
class="math inline">\(Y\)</span>的概率密度函数为 <span
class="math display">\[
l(y)=f(h(y))|h&#39;(y)|
\]</span> 设<span
class="math inline">\((X_1,X_2)\)</span>的密度函数为<span
class="math inline">\(f(x_1, x_2)\)</span>，<span
class="math inline">\(Y_1=g_1(X_1,X_2)\)</span>，<span
class="math inline">\(Y_2=g_2(X_1, X_2)\)</span>，设<span
class="math inline">\(g_1,g_2\)</span>是一一变换，有<span
class="math inline">\(X_1=h_1(Y_1,Y_2)\)</span>，<span
class="math inline">\(X_2=h_2(Y_1,Y_2)\)</span>，均有一阶连续偏导，雅克比行列式<span
class="math inline">\(J(y_1,y_2)\)</span>不为0，则 <span
class="math display">\[
l(y_1,y_2)=f(h_1(y_1,y_2),h_2(y_1, y_2))|J(y_1,y_2)|
\]</span></p></li>
</ul>
<h2 id="三随机变量的数字特征">三、随机变量的数字特征</h2>
<ul>
<li><p><strong>期望</strong>：</p>
<p><span class="math inline">\(X\)</span>若为离散型随机变量，取<span
class="math inline">\(a_1, a_2,\dots\)</span>，对应的概率为<span
class="math inline">\(p_1,p_2,\dots\)</span>，则期望 <span
class="math display">\[
E(X)=\sum_{i=1}^{\infty}a_ip_i
\]</span> 其中，该级数必须绝对收敛，才存在期望；</p>
<p><span
class="math inline">\(X\)</span>若为连续型随机变量，密度函数为<span
class="math inline">\(f(x)\)</span>，则 <span class="math display">\[
E(x) = \int_{-\infty}^{+\infty}xf(x)dx
\]</span> 同样要求<span
class="math inline">\(\int_{-\infty}^{+\infty}|x|f(x)dx &lt;
\infty\)</span></p></li>
<li><p>期望有如下性质：</p>
<ul>
<li><span
class="math inline">\(E(X_1+\dots+X_n)=E(X_1)+\dots+E(X_n)\)</span></li>
<li>若<span class="math inline">\(X_1,X_n\)</span>独立，则<span
class="math inline">\(E(X_1\dots X_n)=E(X_1)\dots E(X_n)\)</span></li>
</ul>
<p>计算随机变量函数的期望，不需要知道函数的分布： <span
class="math display">\[
E(g(X))=\int_{-\infty}^{+\infty}g(x)f(x)dx
\]</span></p></li>
<li><p><strong>条件期望</strong>：<span
class="math inline">\(E(Y|X=x)=E(Y|x)=\int_{-\infty}^{+\infty}yf(y|x)dy\)</span></p>
<p>全概率公式的期望形式： <span class="math display">\[
E(Y)=\int_{-\infty}^{+\infty}E(Y|x)f_1(x)dx=E[E(Y|x)]
\]</span></p></li>
<li><p><strong>方差</strong>： <span class="math display">\[
\text{Var}(X)=E(X-EX)^2=E(X^2)-(EX)^2
\]</span></p>
<ul>
<li><span
class="math inline">\(\text{Var}(CX)=C^2\text{Var}(X)\)</span></li>
<li><span
class="math inline">\(\text{Var}(C+X)=\text{Var}(X)\)</span></li>
<li><span class="math inline">\(\text{Var}(aX+bY)=a^2\text{Var}(X) +
b^2\text{Var}(Y)+2ab\text{Cov}(X,Y)\)</span></li>
</ul></li>
<li><p><strong>矩</strong>：设<span
class="math inline">\(X\)</span>为随机变量，<span
class="math inline">\(c\)</span>为常数，<span
class="math inline">\(k\)</span>为正整数，则<span
class="math inline">\(E[(X-c)^k]\)</span>称为<span
class="math inline">\(X\)</span>关于<span
class="math inline">\(c\)</span>点的<span
class="math inline">\(k\)</span>阶矩，其中在<span
class="math inline">\(c=0\)</span>时称为原点矩，<span
class="math inline">\(c=EX\)</span>时称为中心矩；</p></li>
<li><p><strong>协方差</strong>：<span
class="math inline">\(\text{Cov}(X,Y)=E[(X-m_1)(Y-m_2)]\)</span>，其中<span
class="math inline">\(m_1,m_2\)</span>表示<span
class="math inline">\(X,Y\)</span>的期望</p>
<ul>
<li><span
class="math inline">\(\text{Cov}(c_1X+c_2,c_3Y+c_4)=c_1c_3\text{Cov}(X,Y)\)</span></li>
<li><span
class="math inline">\(\text{Cov}(X,Y)=E(XY)-m_1m_2\)</span></li>
<li>若<span class="math inline">\(X,Y\)</span>独立，则<span
class="math inline">\(\text{Cov}(X,Y)=0\)</span></li>
<li><span class="math inline">\([\text{Cov}(X,Y)]^2\le
\sigma_1^2\sigma_2^2\)</span></li>
</ul></li>
<li><p><strong>相关系数</strong>：<span
class="math inline">\(\text{Corr}(X,Y)=\text{Cov}(X,Y)/(\sigma_1\sigma_2)\)</span>，可以视为标准尺度下（方差为1）的协方差</p>
<ul>
<li>若<span class="math inline">\(X,Y\)</span>独立，则<span
class="math inline">\(\text{Corr}(X,Y)=0\)</span>，但是<strong>反之不成立</strong></li>
<li><span class="math inline">\(|\text{Corr}(X, Y)|=0\)</span></li>
<li><span class="math inline">\(|\text{Corr}(X,Y)|\le
1\)</span>，线性关系越强，相关系数越大</li>
</ul></li>
<li><p><strong>最小二乘</strong>：选择常数<span
class="math inline">\(a,b\)</span>使得<span
class="math inline">\(E[(Y-a-bX)^2]\)</span>最小：</p>
<p>记<span
class="math inline">\(m_1=EX,m_2=EY,\sigma_1^2=\text{Var}(X),\sigma_2^2=\text{Var}(Y),\rho=\text{Corr}(X,Y),c=a-(m_2-bm_1)\)</span>：
<span class="math display">\[
\begin{aligned}
E[(Y-a-bX)^2]&amp;=E[(Y-m_2)-b(X-m_1)-c]^2\\
&amp;=\sigma_2^2 + b^2\sigma_1^2 -2b\text{Cov}(X,Y) + c^2
\end{aligned}
\]</span> 令<span
class="math inline">\(c=0,b=\text{Cov}(X,Y)/\sigma_1^2=\sigma_1^{-1}\sigma_2\rho\)</span>得到线性逼近：
<span class="math display">\[
L(X)=m_2 - \sigma_1^{-1}\sigma_2\rho m_1 + \sigma_1^{-1}\sigma_2\rho X
\]</span> 这一逼近的剩余是： <span class="math display">\[
E[Y-L(X)]^2=\sigma_2^2(1-\rho^2)
\]</span> 可以看出当<span
class="math inline">\(|\rho|=1\)</span>时，完美的线性关系，而<span
class="math inline">\(\rho=0\)</span>时，剩余为<span
class="math inline">\(\sigma_2\)</span>，毫无线性关系（可能满足其他的非线性关系）；</p>
<ul>
<li>若<span
class="math inline">\((X,Y)\)</span>为二维正态，则任何函数<span
class="math inline">\(M(X)\)</span>，以<span
class="math inline">\(E[Y-M(X)]^2\)</span>最小化为目标都可以得到上述的<span
class="math inline">\(L(X)\)</span></li>
<li>若<span class="math inline">\((X,Y)\)</span>为二维正态，则<span
class="math inline">\(\rho=0\)</span>可以推出<span
class="math inline">\(X,Y\)</span>独立</li>
</ul></li>
<li><p><strong>大数定理</strong>：（独立同分布的）<span
class="math inline">\(\overline{X}\)</span>依概率收敛到<span
class="math inline">\(EX\)</span>；</p></li>
<li><p><strong>马尔科夫不等式</strong>：若<span
class="math inline">\(Y\)</span>为只取非负值的随机变量，则对于任意的常数<span
class="math inline">\(\epsilon &gt; 0\)</span>有<span
class="math inline">\(P(Y&gt;\epsilon)\le EY/\epsilon\)</span></p></li>
<li><p><strong>车比雪夫不等式</strong>：用<span
class="math inline">\((Y-EY)^2\)</span>代替<span
class="math inline">\(Y\)</span>，<span
class="math inline">\(\epsilon^2\)</span>代替<span
class="math inline">\(\epsilon\)</span>，得到：若<span
class="math inline">\(\text{Var}(Y)\)</span>存在，则： <span
class="math display">\[
P(|Y-EY|\ge\epsilon)\le \text{Var}(X)/\epsilon^2
\]</span></p></li>
<li><p><strong>中心极限定理</strong>：（独立同分布的）随机变量的和的分布收敛于正态分布；</p></li>
</ul>
<h2 id="四参数估计">四、参数估计</h2>
<p>在有限总体的情况下，样本的分布取决于总体分布和抽样方式（有放回还是无放回）；对于无限总体或者有放回抽样，总体分布完全决定样本分布；</p>
<p>统计量是指完全由样本决定的量；样本均值、样本矩与随机变量的均值和矩的形式一样；但是要注意样本方差<span
class="math inline">\(S^2=\frac{n}{n-1}m^2\)</span>，<span
class="math inline">\(m^2\)</span>是样本的二阶中心矩；</p>
<p>参数估计可以分为点估计和区间估计；点估计即用一个点（值）去估计参数，区间估计则是用一个区间去估计参数，相当于把误差范围显式地表示出来了；</p>
<h3 id="矩估计">矩估计</h3>
<p>设总体分布为<span
class="math inline">\(f(x,\theta_1,\dots,\theta_k)\)</span>，基本思想是让一系列的样本矩（原点矩或中心距都行）<span
class="math inline">\(a_m\)</span>等于对应的随机变量矩<span
class="math inline">\(\alpha_m\)</span>；以原点矩为例，一边有 <span
class="math display">\[
\alpha_m=\int_{-\infty}^{\infty}x^mf(x,\theta_1,\dots,\theta_k)dx
\]</span> 另一边有： <span class="math display">\[
a_m=\sum_{i=1}^nX_i^m/n
\]</span> 列出<span class="math inline">\(k\)</span>个方程求解参数<span
class="math inline">\(\theta_1,\dots,\theta_k\)</span>；</p>
<h3 id="极大似然估计">极大似然估计</h3>
<p>设总体分布为<span
class="math inline">\(f(X;\theta_1,\dots,\theta_k)\)</span>，<span
class="math inline">\(X_1,\dots,X_n\)</span>为样本，则样本的密度函数为
<span class="math display">\[
L(X_1,\dots,X_n;\theta_1,\dots,\theta_k)=f(X_1;\theta_1,\dots,\theta_k)\dots
f(X_n;\theta_1,\dots,\theta_k)
\]</span> 上式子<span
class="math inline">\(X_1,\dots,X_n\)</span>固定，把<span
class="math inline">\(L\)</span>看做<span
class="math inline">\(\theta_1,\dots,\theta_k\)</span>的参数，即称为似然函数；想法就是使用使得似然函数最大的点作为参数的估计值：
<span class="math display">\[
L(X_1,\dots,X_n;\theta_1^*,\dots,\theta_k^*)=\max_{\theta_1,\dots,\theta_k}L(X_1,\dots,X_n;\theta_1,\dots,\theta_k)
\]</span> 合适时，可以使用一阶导为0或者对数的一阶导为0求得极大值点；</p>
<h3 id="贝叶斯估计">贝叶斯估计</h3>
<p>上面两种方法，在抽样之前，对于参数<span
class="math inline">\(\theta\)</span>没有任何的了解，所有信息来自于样本；贝叶斯学派认为应该对于<span
class="math inline">\(\theta\)</span>有某种先验知识，并且以<span
class="math inline">\(\theta\)</span>的某种概率密度表现出来<span
class="math inline">\(h(\theta)\)</span>；</p>
<p>设总体有概率密度<span
class="math inline">\(f(X,\theta)\)</span>，<span
class="math inline">\(X_1,\dots,X_n\)</span>为样本，则<span
class="math inline">\((\theta, X_1, \dots, X_n)\)</span>的联合密度为：
<span class="math display">\[
h(\theta)f(X_1,\theta)\dots f(X_n,\theta)
\]</span> <span
class="math inline">\((X_1,\dots,X_n)\)</span>的边缘密度为 <span
class="math display">\[
p(X_1,\dots,X_n)=\int h(\theta)f(X_1,\theta)\dots f(X_n,\theta) d\theta
\]</span></p>
<blockquote>
<p>积分的限取决于参数</p>
</blockquote>
<p>后验/<span class="math inline">\(\theta\)</span>的条件密度为： <span
class="math display">\[
h(\theta|X_1,\dots,X_n)=h(\theta)f(X_1,\theta)\dots
f(X_n,\theta)/p(X_1,\dots,X_n)
\]</span>
后验分布如何使用不是一个固定的事情；一种常见的做法是，取后验分布的均值作为<span
class="math inline">\(\theta\)</span>的估计</p>
<blockquote>
<p>广义的先验密度<span
class="math inline">\(h(\theta)\)</span>不一定满足积分为1，甚至可能是<span
class="math inline">\(\infty\)</span></p>
<p>初始化的<span
class="math inline">\(h(x)\)</span>可以基于“同等无知”原则，即均匀分布；基于这个原则，不同的人可能得到不同的初始化结果，例如对<span
class="math inline">\(p\)</span>同等无知，我们令<span
class="math inline">\(h(p)=1, 0\le p\le 1\)</span>；同样的对<span
class="math inline">\(p^2\)</span>也同等无知，我们令<span
class="math inline">\(h(p^2)=1, 0\le p\le1\)</span>又得到<span
class="math inline">\(h(p)=2p,0\le p \le 1\)</span>；</p>
</blockquote>
<h3 id="点估计的优良性准则">点估计的优良性准则</h3>
<p>如何比较上述几种点估计方法？这取决于抽样。不同的样本，一种估计方法可能很好，也可能不太好；我们希望有整体上的评价方式；</p>
<p><strong>无偏性</strong>：设<span
class="math inline">\(\hat{g}(X_1,\dots,
X_n)\)</span>是一个估计量，如果对于任何可能的<span
class="math inline">\(\theta_1,\dots,\theta_k\)</span>都有 <span
class="math display">\[
E_{\theta_1,\dots,\theta_k}[\hat{g}(X_1,\dots,X_n)]=g(\theta_1,\dots,\theta_k)
\]</span> 则称<span class="math inline">\(\hat{g}\)</span>是<span
class="math inline">\(g\)</span>的一个无偏估计量；</p>
<p>无偏性体现了，尽管一部分样本估计偏低，另一部分估计偏高，可是将正负误差在概率上平均起来应该为0，即没有系统误差；</p>
<p>样本均值是总体均值的无偏估计： <span class="math display">\[
E(\overline{X})=E(\sum_{i=1}^nX_i/n)=\sum_{i=1}^nE(X_i)/n=n\theta/n=\theta
\]</span> 样本方差是总体方差的无偏估计，即<span
class="math inline">\(E(X_i)=a\)</span>，由上可知也有<span
class="math inline">\(E(\overline{X})=a\)</span> <span
class="math display">\[
\begin{aligned}
E(\sum_{i=1}^n(X_i-\overline{X})^2)&amp;=E\sum_{i=1}^n[(X_i-a)-(\overline{X}-a)]^2\\
&amp;=E(\sum_{i=1}^n(X_i-a)^2-n(\overline{X}-a)^2)\\
&amp;=\sum_{i=1}^n\text{Var}(X_i)-n\text{Var}(\sum_{i=1}^nX_i/n)\\
&amp;=n\sigma^2-n\sigma^2/n^2\\
&amp;=(n-1)\sigma^2
\end{aligned}
\]</span> 因此样本方差是总体方差的无偏估计： <span
class="math display">\[
E(S^2)=E(\sum_{i=1}^n(X_i-\overline{X})^2/(n-1))=\sigma^2
\]</span> 从自由度的角度解释，<span
class="math inline">\(\sum_{i=0}^n(X_i-\overline{X})^2\)</span>的自由度为<span
class="math inline">\(n-1\)</span>，因为存在一个约束是<span
class="math inline">\(\overline{X}=\sum_{i=1}^nX_i/n\)</span>；当我们知道总体分布的均值为<span
class="math inline">\(a\)</span>而不是使用样本均值时，对方差的估计可以表示为<span
class="math inline">\(\sum_{i=1}^n(X_i-a)^2/n\)</span></p>
<p>下式表明，用<span
class="math inline">\(S\)</span>估计总体分布的标准差不是无偏估计！估计得要小一点；
<span class="math display">\[
\sigma^2=E(S^2)=\text{Var}(S)+(ES)^2
\]</span>
矩估计和极大似然估计对正态分布的方差的估计求出的都是样本二阶中心矩，不是无偏的；</p>
<p><strong>最小方差无偏估计</strong>（MVU）：一个参数可能有多个无偏估计，需要一些指标来评价哪种估计更好；</p>
<ul>
<li><p>均方误差：<span
class="math inline">\(M_{\hat{\theta}}(\theta)=E_{\theta}[\hat{\theta}(X_1,\dotsm,X_n)-\theta]^2\)</span></p>
<p><span
class="math inline">\(M_{\hat{\theta}}(\theta)=\text{Var}(\hat{\theta})+[E_{\theta}(\hat{\theta})-\theta]^2\)</span>，由该式子，得到均方误差由估计的参数的方差和系统偏差组成，如果<span
class="math inline">\(\hat{\theta}\)</span>为<span
class="math inline">\(\theta\)</span>的无偏估计，则第二项为0，即对于无偏估计，方差最小者最优；</p>
<blockquote>
<p>这里是有<span
class="math inline">\(\text{Var}(\hat{\theta}-\theta)=\text{Var}(\hat{\theta})\)</span>？把<span
class="math inline">\(\theta\)</span>当成常量了</p>
</blockquote></li>
</ul>
<p>之前的方法只能判断现有的无偏估计那个更好，却不能求出最小的无偏估计，可以使用克拉美-劳不等式确定下界，证明昝略：</p>
<p>只考虑单个参数的情况，记 <span class="math display">\[
I(\theta)=\int\left[\left(\frac{\part f(x,\theta)}{\part
\theta}\right)^2/f(x,\theta)\right]dx
\]</span></p>
<blockquote>
<p>积分的界取决于<span class="math inline">\(x\)</span>可取的范围；<span
class="math inline">\(I(\theta)\)</span>又称费歇尔信息量</p>
</blockquote>
<p>则有 <span class="math display">\[
\text{Var}_{\theta}(\hat{g})\ge(g&#39;(\theta))^2/(nI(\theta))
\]</span> 其中<span class="math inline">\(n\)</span>是样本大小；</p>
<blockquote>
<p>信息量越大，方差的下界越小，估计越准</p>
</blockquote>
<p><strong>相合性</strong>：类似于大数定理，随着样本量的增加，估计量应该趋于真值：
<span class="math display">\[
\lim_{n\rightarrow
\infty}P_{\theta_1,\dotsm,\theta_k}(|\hat{g}-g|\ge\epsilon) = 0
\]</span> 则称<span class="math inline">\(\hat{g}\)</span>是<span
class="math inline">\(g\)</span>的相合估计；</p>
<p><strong>渐进正态性</strong>：当<span
class="math inline">\(n\)</span>逐渐增大时，样本的和的分布趋近于正态分布；</p>
<h3 id="区间估计">区间估计</h3>
<p>一方面，期望<span class="math inline">\(\theta\)</span>落在区间<span
class="math inline">\([\theta_1,\theta_2]\)</span>的可靠性尽量高，另一方面<span
class="math inline">\([\theta_1,\theta_2]\)</span>的范围尽量小；这两个要求是矛盾的，通常先保证可靠度（给定置信系数求区间）；</p>
<p>给定一个很小的数<span
class="math inline">\(\alpha&gt;0\)</span>，若<span
class="math inline">\(P_\theta(\hat{\theta_1}\le\theta\le\hat{\theta_2})=1-\alpha\)</span>，则称区间估计<span
class="math inline">\([\theta_1,\theta_2]\)</span>的置信系数为<span
class="math inline">\(1-\alpha\)</span>；</p>
<p>这里只讨论如何构造置信区间，但是其优良性比较暂不考虑；</p>
<p>定义<strong>上<span class="math inline">\(\beta\)</span>分位点<span
class="math inline">\(u_\beta\)</span></strong>为满足<span
class="math inline">\(F(u_\beta)=1-\beta\)</span>的位置；</p>
<p><strong>枢轴法</strong>（正态分布区间估计为例）：</p>
<ol type="1">
<li>找到与估计的参数<span
class="math inline">\(g(\theta)\)</span>有关的统计量<span
class="math inline">\(T\)</span>，一般是其良好的点估计（例如<span
class="math inline">\(T=\overline{X}\)</span>；</li>
<li>找到一个<span class="math inline">\(T与\)</span><span
class="math inline">\(g(\theta)\)</span>组成的函数<span
class="math inline">\(S\)</span>，使得<span
class="math inline">\(S\)</span>的分布与带估计参数无关<span
class="math inline">\(\theta\)</span>，例如<span
class="math inline">\(\sqrt{n}(\overline{X}-\mu)/\sigma\)</span>服从标准正态分布<span
class="math inline">\(\Phi\)</span></li>
<li><span class="math inline">\(a\le S(T,g(\theta))\le
b\)</span>能够改写成<span class="math inline">\(A \le g(\theta) \le
B\)</span>，令<span
class="math inline">\(a=u_{1-\alpha/2}\)</span>，<span
class="math inline">\(b=u_{\alpha/2}\)</span>，则<span
class="math inline">\([A,B]\)</span>即为所求的<span
class="math inline">\(1-\alpha\)</span>的置信区间</li>
</ol>
]]></content>
      <categories>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>概率与统计</tag>
      </tags>
  </entry>
  <entry>
    <title>线性代数要点总结</title>
    <url>/2022/08/10/linear-algebra/</url>
    <content><![CDATA[<p>本文是根据《线性代数及其应用》整理的复习笔记。之后又补充了部分《矩阵代数》第0章的内容。</p>
<span id="more"></span>
<blockquote>
<p>一般的，表示向量应该加粗斜体小写，表示矩阵用大写，但是大多数时候我想偷个懒；</p>
<p>教材中涉及到的很多应用我都没来得及细看，只是对“干货”做了一个简单的总结；</p>
</blockquote>
<h2 id="线性方程组">1. 线性方程组</h2>
<h3 id="线性方程组-1">1.1 线性方程组</h3>
<p>为什么要研究线性代数？解线性方程组是一个重要的理解线性代数的角度。<strong>一个</strong>线性方程有如下形式：
<span class="math display">\[
a_1x_1+a_2x_2+\dots+a_nx_n = b
\]</span> 其中，<span class="math inline">\(a_i\)</span>和<span
class="math inline">\(b\)</span>是该线性方程的系数，一般是常量，<span
class="math inline">\(x_i\)</span>是未知数，一般是变量。<strong>线性方程组就是一组含有相同变量的线性方程</strong>，满足所有方程的解的集合称为解集，因此线性方程组可分为无解、有唯一解和有无穷多解三类。其中，有解（唯一解或者无穷解）又称为方程组是<strong>相容</strong>的，无解又称为<strong>不相容</strong>的。</p>
<p>我们可以将<span class="math inline">\(m\)</span>个方程<span
class="math inline">\(n\)</span>个未知数的线性方程组的参数写成一个的参数矩阵：
<span class="math display">\[
A=\left(
\begin{array}{cccc}
a_{11}&amp;\dots&amp;a_{1n}\\
&amp;\vdots&amp;\\
a_{m1}&amp;\dots&amp;a_{mn}\\
\end{array}
\right)
\]</span></p>
<p><span class="math display">\[
\mathbf{x}=(x_1,\dots,x_n)^T
\]</span></p>
<p><span class="math display">\[
\mathbf{b}=({b_1,\dots,b_m})^T
\]</span></p>
<p>方程组写成矩阵形式为 <span class="math display">\[
A\mathbf{x}=\mathbf{b}
\]</span></p>
<blockquote>
<p>对上式还可以从对向量进行线性变换的角度进行解释，见后文</p>
</blockquote>
<p>解线性方程组的一般方法是高斯消去法，基本思想是，通过对方程组的方程之间进行<em>一些操作</em>，保证得到的新方程组与原来的方程组有着相同的解集，并且<em>新的方程组的形式</em>是非常容易求解的。</p>
<p>这种“操作”即在线性方程的增广矩阵<span
class="math inline">\((A|\mathbf{b})\)</span>上进行如下三种行变换（这些行变换是可逆的）：</p>
<ul>
<li><p>倍加：把某一行乘以一个常数加到另一行上；</p></li>
<li><p>倍乘：把某一行乘以一个常数；</p></li>
<li><p>交换：交换某两行；</p></li>
</ul>
<blockquote>
<p>这三种行变换能够保证得到一个解集相同的线性方程组，也就是说它能够保证增广矩阵的一些性质（列向量的线性相关关系），但是也会有一些改变（行向量的线性相关关系），这在后文会讲到。
事实上我在初学线性代数的时候时常纠结“行”还是“列”，在后文中，提到“向量”，均指列向量，这只是一种数学上讨论问题的简化方式，与使用计算机实现时的存储无关，也可以都用行向量来讨论线性代数中的各种问题，把高斯消去改成列变换，等等。</p>
</blockquote>
<p>这种“新的形式”是<strong>阶梯型</strong>或者<strong>简化阶梯型</strong></p>
<p>（行）阶梯型为：</p>
<ul>
<li>每一非零行在每一零行之上；</li>
<li>每一行的第一个非零元素所在的列位于前一行第一个非零元素所在的列的右边；</li>
<li>每一行的第一个非零元素下方都是0；</li>
</ul>
<p>在这基础上，简化（行）阶梯型还要满足下述条件：</p>
<ul>
<li>每一非零行的第一个非零元素是1，并且是该列所在列的唯一非零元素；</li>
</ul>
<p>同一个矩阵可能因为行变换序列的选择不同得到不同的阶梯型，但是最终的简化阶梯型是唯一的；</p>
<p>在阶梯型中，每一非零行的第一个非零元素的位置称为<strong>主元位置</strong>，主元位置所在列为<strong>主元列</strong>；</p>
<blockquote>
<p>在计算机实现高斯消去时，选择某列中绝对值最大的元素作为主元，<a
href="https://www.zhihu.com/question/33862337">避免精度问题带来误差</a></p>
</blockquote>
<p>在经过上述“操作”，得到了想要的“形式”之后，解便可以给出，其中对应于主元列的变量称为<strong>基本变量</strong>，其他变量（阶梯型全0行）称为<strong>自由变量</strong>。对于阶梯型对应的线性方程组</p>
<ul>
<li>方程矛盾/最右列是主元列，比如出现0=4等，则无解；</li>
<li>无自由变量，有唯一解；</li>
<li>有自由变量，将基本变量用自由变量表示，还可以写成以自由变量为权重的一组基向量的加权和；</li>
</ul>
<p>### 1.2 向量方程</p>
<p>在这一节中我们将从另一个角度理解<span
class="math inline">\(A\mathbf{x}=\mathbf{b}\)</span>。在此之前，我们需要补充一些概念：</p>
<p><strong>向量</strong>表示一组数，包含n个数的向量的集合记为<span
class="math inline">\(\mathbb{R}^n\)</span>；</p>
<p>给定<span class="math inline">\(\mathbb{R}^n\)</span>中向量<span
class="math inline">\(v_1, \dots, v_p\)</span>和标量<span
class="math inline">\(c_1, \dots, c_p\)</span>，向量 <span
class="math display">\[
y = c_1v_1+\dots +c_pv_p
\]</span> 称为向量<span class="math inline">\(v_1, \dots,
v_p\)</span>以<span class="math inline">\(c_1, \dots,
c_p\)</span>为权的<strong>线性组合</strong>；</p>
<p>将属于<span class="math inline">\(\mathbb{R}^n\)</span>中的向量<span
class="math inline">\(v_1, \dots,
v_p\)</span>所有的线性组合得到集合记为<span
class="math inline">\(\text{Span}\{v_1,\dots,v_p\}\)</span>，称为<span
class="math inline">\(v_1, \dots, v_p\)</span>所张成的<span
class="math inline">\(\mathbb{R}^n\)</span>子集；</p>
<p>现在再来重新解释<span
class="math inline">\(A\mathbf{x}=\mathbf{b}\)</span>，将<span
class="math inline">\(A\)</span>按列分块，可以将线性方程组表示为如下的向量方程：
<span class="math display">\[
x_1a_1+\dots+x_na_n=b
\]</span> 对于”向量<span
class="math inline">\(b\)</span>能否表示为向量<span
class="math inline">\(a_1,\dots,a_n\)</span>的线性组合“的问题，等价于是判断<span
class="math inline">\(A\mathbf{x}=\mathbf{b}\)</span>是否有解的问题。</p>
<p>对于先前涉及到的定义，<span class="math inline">\(A\)</span>是<span
class="math inline">\(m\times n\)</span>矩阵则下列四个命题等价：</p>
<ul>
<li>对于<span class="math inline">\(\mathbb{R}^m\)</span>中每个<span
class="math inline">\(b\)</span>，<span
class="math inline">\(AX=b\)</span>都有解；</li>
<li><span class="math inline">\(\mathbb{R}^m\)</span>中的每个<span
class="math inline">\(b\)</span>都是<span
class="math inline">\(A\)</span>中列向量的线性组合；</li>
<li><span class="math inline">\(A\)</span>的各列生成<span
class="math inline">\(\mathbb{R}^{m}\)</span>；</li>
<li><span
class="math inline">\(A\)</span>在每一行都有一个主元位置；（这里<span
class="math inline">\(A\)</span>是线性方程组的系数矩阵，而不是增广矩阵）</li>
</ul>
<h3 id="线性方程组的解集">1.3 线性方程组的解集</h3>
<p>我们继续扩充一些与解集相关的定义。</p>
<p>记录一类特殊的线性方程组，<span
class="math inline">\(A\mathbf{x}=\mathbf{0}\)</span>，称为齐次线性方程组，必有一个<span
class="math inline">\(\mathbf{x}=\mathbf{0}\)</span>的解，称为<strong>平凡解</strong>，通常我们更在乎其<strong>非平凡解</strong>，非平凡解不是一个确定的常数解，而是一个线性空间（可以想象成在某个直线或某个平面上的点都是解，后文会定义该空间），表示为参数向量形式；齐次线性方程组的解集总可以表示为<span
class="math inline">\(\text{Span}\{v_1, \dots, v_p\}\)</span>，其中<span
class="math inline">\(v_i\)</span>是解向量。</p>
<p>齐次方程<span
class="math inline">\(A\mathbf{x}=\mathbf{0}\)</span>有非平凡解当且仅当方程至少有一个自由变量；</p>
<p><span class="math inline">\(A\mathbf{x}=\mathbf{0}\)</span>和<span
class="math inline">\(A\mathbf{x}=\mathbf{b}\)</span>的解集有什么关系？若<span
class="math inline">\(A\mathbf{x}=\mathbf{b}\)</span>至少有一个非令解<span
class="math inline">\(p\)</span>，则<span
class="math inline">\(A\mathbf{x}=\mathbf{b}\)</span>的解集是所有形如<span
class="math inline">\(w=p+v_h\)</span>向量的集合，其中<span
class="math inline">\(v_h\)</span>是齐次方程<span
class="math inline">\(A\mathbf{x}=\mathbf{0}\)</span>的任意一个解；</p>
<h3 id="线性无关">1.4 线性无关</h3>
<p>我们需要研究一种向量集合的性质，从人脑可以想象的<span
class="math inline">\(\mathbb{R}^3\)</span>中向量集合来讲，就是判断该集合中向量中任意一个向量是否位于其他向量张成的直线/平面上，均不满足则线性无关，一旦有一个满足则线性相关，下面是严格的定义：</p>
<p><span class="math inline">\(\mathbb{R}^n\)</span>中的一组向量<span
class="math inline">\(v_1, \dots,
v_p\)</span>是<strong>线性无关</strong>的，当且仅当向量方程 <span
class="math display">\[
x_1v_1+\dots+x_pv_p=0
\]</span> 仅有平凡解（即权重<span
class="math inline">\(x_1,\dots,x_p\)</span>均为0），</p>
<p>否则为<strong>线性相关</strong>的，在线性相关的集合中，至少有一个向量一定能够表示为其他向量的线性组合；</p>
<p>特别地，对于包含单个向量的几个，当且仅当该向量是零向量时称该集合是线性相关的；</p>
<p>在<span
class="math inline">\(\mathbb{R}^3\)</span>中某个向量集合中，我们可以想象1个非零向量张成一个直线，2个不互为倍数的向量张成一个平面，3个线性无关的向量就可以张成<span
class="math inline">\(\mathbb{R}^3\)</span>了，此时再加入任何向量都可以表示为之前3个的线性组合。事实上有如下定理：</p>
<p>若一个向量组的向量个数超过向量元素个数（以列向量为例，即对应的矩阵行数小于列数），则向量组线性相关；</p>
<h3 id="初识线性变换">1.5 初识线性变换</h3>
<p>除了线性方程组和向量方程的形式，理解<span
class="math inline">\(A\mathbf{x}=\mathbf{b}\)</span>的第三个角度是线性变换。尽管线性变换本身的定义要广泛得多，我仍然习惯于使用图形学中的入门案例来想象线性变换在干什么：对于3维世界中的一个物体，我们最终希望得到其在2维屏幕上的投影，就需要施加一个对于物体的坐标施加一个透视变换矩阵；</p>
<p>线性变换<span class="math inline">\(T:\mathbb{R}^n\rightarrow
\mathbb{R}^m\)</span>，是指它是一个将<span
class="math inline">\(\mathbb{R}^n\)</span>中的向量<span
class="math inline">\(x\)</span>映射到<span
class="math inline">\(\mathbb{R}^m\)</span>中的向量<span
class="math inline">\(T(x)\)</span>的变换，<span
class="math inline">\(\mathbb{R}^n\)</span>是<span
class="math inline">\(T\)</span>的<strong>定义域</strong>，<span
class="math inline">\(\mathbb{R}^m\)</span>是<span
class="math inline">\(T\)</span>的<strong>余定义域</strong>，<span
class="math inline">\(T(x)\)</span>称为<strong>像</strong>，像的集合称为<strong>值域</strong>；</p>
<p>用矩阵变换表示上述线性变换为<span class="math inline">\(x\mapsto
Ax\)</span>，其中<span class="math inline">\(A\)</span>是<span
class="math inline">\(m\times n\)</span>的矩阵；</p>
<p>写出线性变换<span class="math inline">\(T:\mathbb{R}^n\rightarrow
\mathbb{R}^m\)</span>的<strong>标准矩阵</strong><span
class="math inline">\(A\)</span>：</p>
<p><span class="math inline">\(A\)</span>的每一个列向量作用于<span
class="math inline">\(n\)</span>维空间中的<span
class="math inline">\(n\)</span>个单位向量，即 <span
class="math display">\[
A=[T(e_1),\dots, T(e_n)]
\]</span> 线性变换是一种映射，我们也要讨论一下映射的存在与与唯一性：</p>
<ul>
<li>映射<span class="math inline">\(T:\mathbb{R}^n\mapsto
\mathbb{R}^m\)</span>称为到<span
class="math inline">\(\mathbb{R}^m\)</span>上的映射（即满射），若<span
class="math inline">\(\mathbb{R}^m\)</span>中的任意向量<span
class="math inline">\(b\)</span>都<strong>至少有一个</strong><span
class="math inline">\(\mathbb{R}^n\)</span>中的<span
class="math inline">\(x\)</span>与之对应；</li>
<li>映射<span class="math inline">\(T:\mathbb{R}^n\mapsto
\mathbb{R}^m\)</span>称为一对一映射（即单射），若<span
class="math inline">\(\mathbb{R}^m\)</span>中的每个<span
class="math inline">\(b\)</span>是<span
class="math inline">\(\mathbb{R}^n\)</span>中<strong>至多一个</strong><span
class="math inline">\(x\)</span>的像</li>
</ul>
<p>与之前的定义结合，我们可以得到满射/单射的等价条件，设<span
class="math inline">\(T:\mathbb{R}^n\mapsto
\mathbb{R}^m\)</span>的标准矩阵为<span
class="math inline">\(A\)</span></p>
<ul>
<li><span class="math inline">\(T\)</span>是满射，当且仅当<span
class="math inline">\(A\)</span>的列生成<span
class="math inline">\(\mathbb{R}^m\)</span>；</li>
<li><span class="math inline">\(T\)</span>是单射，当且仅当<span
class="math inline">\(A\)</span>的列线性无关，也可以推出<span
class="math inline">\(A\mathbf{x}={0}\)</span>仅有平凡解；</li>
</ul>
<h2 id="矩阵代数">2. 矩阵代数</h2>
<p>上一章引入矩阵，是为了简明地表示一个线性方程组、一个向量方程、或者一个线性变换；那么矩阵本身作为一个变量，它有那些运算性质？</p>
<h3 id="矩阵乘法">2.1 矩阵乘法</h3>
<p>矩阵加、乘、数乘等运算方法，运算律等按下不表，矩阵乘法的多种计算方法需要了解一下，在不同的应用场景中有不同的解释方式，设<span
class="math inline">\(A\)</span>是<span class="math inline">\(m\times
n\)</span>矩阵，<span class="math inline">\(B\)</span>是<span
class="math inline">\(n\times p\)</span>矩阵，<span
class="math inline">\(C=AB\)</span>是<span class="math inline">\(m\times
p\)</span>矩阵</p>
<ul>
<li><p><span class="math inline">\(AB\)</span>的列是<span
class="math inline">\(A\)</span>与<span
class="math inline">\(B\)</span>的对应列的乘积：<span
class="math inline">\(AB\)</span>的每一列都是<span
class="math inline">\(A\)</span>的各列的线性组合，以<span
class="math inline">\(B\)</span>的对应列的元素为权（基本定义） <span
class="math display">\[
AB=[a_1,\dots,a_n][b_1,\dots,b_p]=[c_1,\dots,c_p]
\]</span> 其中，<span class="math inline">\(C\)</span>的第<span
class="math inline">\(j\)</span>列表示为： <span class="math display">\[
c_j = b_{j1}a_1+\dots+b_{jn}a_n
\]</span></p></li>
<li><p><span class="math inline">\(AB\)</span>的行列计算法则（常见实现）
<span class="math display">\[
C_{ij}=\sum_{k}^{n}A_{ik}B_{kj}
\]</span></p></li>
<li><p><span class="math inline">\(AB\)</span>的行是<span
class="math inline">\(A\)</span>的对应行与<span
class="math inline">\(B\)</span>的乘积：。。。</p></li>
<li><p><span class="math inline">\(A\)</span>的每一列与<span
class="math inline">\(B\)</span>的行的乘积之和（矩阵分块乘法思想） <span
class="math display">\[
AB=[a_1,\dots,a_n][b_1,\dots,b_n]^T=a_1b_1+\dots+a_nb_n
\]</span></p></li>
<li></li>
</ul>
<blockquote>
<p>左乘对角矩阵，<span class="math inline">\(DA\)</span>，相当于用<span
class="math inline">\(D\)</span>的各个对角元素乘以<span
class="math inline">\(A\)</span>的各行</p>
<p>右乘对角矩阵，<span class="math inline">\(AD\)</span>，相当于用<span
class="math inline">\(D\)</span>的各个对角元素乘以<span
class="math inline">\(A\)</span>的各列</p>
</blockquote>
<h3 id="矩阵的逆">2.2 矩阵的逆</h3>
<p>只有方阵才有逆矩阵的定义。方阵<span
class="math inline">\(A\)</span>的逆的定义是存在一个矩阵<span
class="math inline">\(C\)</span>使得<span
class="math inline">\(AC=I\)</span>且<span
class="math inline">\(CA=I\)</span>。不可逆矩阵又称为<strong>奇异矩阵</strong>，可逆矩阵又称为<strong>非奇异矩阵</strong>。</p>
<p>那么对于先前一直讨论的<span
class="math inline">\(A\mathbf{x}=\mathbf{b}\)</span>，若<span
class="math inline">\(A\)</span>可逆，则有<span
class="math inline">\(\mathbf{x}=A^{-1}\mathbf{b}\)</span>。事实上，<strong>可逆矩阵定理</strong>，将包含一大堆使得上市成立的定理，我们会在后文慢慢补充。</p>
<blockquote>
<p>标量代数中的习惯，让我们总想求出一个<span
class="math inline">\(A^{-1}\)</span>使用矩阵乘法求解，事实上在实现时使用在高斯消去法求解，而求逆矩阵的方法也用高斯消去，不仅复杂度更小，精度更好，而且还能在过程中判断是否可逆</p>
</blockquote>
<p>在介绍行变换求逆之前，我们先回顾一下先前的三种行变换，并把行变换表示成左乘<strong>初等矩阵</strong>的形式，以<span
class="math inline">\(3\times3\)</span>矩阵为例：</p>
<ul>
<li><p>倍加，第一行乘4加到第三行 <span class="math display">\[
E_1=\left(
\begin{array}{ccc}
1&amp;0&amp;0\\
0&amp;1&amp;0\\
4&amp;0&amp;1
\end{array}
\right)
\]</span></p></li>
<li><p>交换，第一行与第二行交换 <span class="math display">\[
E_2=\left(
\begin{array}{ccc}
0&amp;1&amp;0\\
1&amp;0&amp;0\\
0&amp;0&amp;1
\end{array}
\right)
\]</span></p></li>
<li><p>倍乘，第三行乘5 <span class="math display">\[
E_3=\left(
\begin{array}{ccc}
1&amp;0&amp;0\\
0&amp;1&amp;0\\
0&amp;0&amp;5
\end{array}
\right)
\]</span></p></li>
</ul>
<blockquote>
<p>初等矩阵是单位矩阵经过一次初等行变换或者初等列变换得到的矩阵</p>
</blockquote>
<p>那么可以从行变换的角度理解矩阵的逆：行变换可以视为左乘一系列初等矩阵，初等矩阵是可逆的，其积<span
class="math inline">\(C\)</span>也是可逆的；矩阵<span
class="math inline">\(A\)</span>可逆，当且仅当<span
class="math inline">\(A\)</span>行等价于<span
class="math inline">\(I\)</span>，这时把<span
class="math inline">\(A\)</span>变为<span
class="math inline">\(I\)</span>的一系列初等变换同时把<span
class="math inline">\(I\)</span>变成<span
class="math inline">\(C=A^{-1}\)</span>；因此可以写一个增广矩阵<span
class="math inline">\([A, I]\)</span>，如果能通过行变换变成<span
class="math inline">\([I,C]\)</span>，则既证明了<span
class="math inline">\(A\)</span>可逆，又得到<span
class="math inline">\(A^{-1}=C\)</span>；也可以把该过程视为解<span
class="math inline">\(n\)</span>个方程组，<span
class="math inline">\(I\)</span>的<span
class="math inline">\(n\)</span>列是增广列，统一都放在了右边，当我们只需要<span
class="math inline">\(A^{-1}\)</span>的部分列时，可以使用这样的方法。</p>
<p><strong>可逆矩阵定理（一）</strong>：</p>
<img src="/2022/08/10/linear-algebra/image-20220811123558336.png" class="" title="可逆矩阵定理（一）">
<h3 id="分块矩阵">2.3 分块矩阵</h3>
<p>分块矩阵的乘法也可以用通常的行列法进行，只要<span
class="math inline">\(A\)</span>的列的分法与<span
class="math inline">\(B\)</span>的行的分法一致；使用如下记号来表示分块矩阵、子矩阵等：</p>
<p>设<span class="math inline">\(A\in M_{m\times
n}(F)\)</span>，对于指标集<span class="math inline">\(\alpha \subseteq
\{1,\dots,m\}\)</span>，<span class="math inline">\(\beta \subseteq
\{1,\dots,n\}\)</span>，把<span
class="math inline">\(A\)</span>中位于<span
class="math inline">\(\alpha\)</span>的各行与位于<span
class="math inline">\(\beta\)</span>的各列组成的子矩阵记为<span
class="math inline">\(A(\alpha, \beta)\)</span>；</p>
<p><span class="math inline">\(m=n\)</span>时，<span
class="math inline">\(A(\alpha,\alpha)\)</span>为主子矩阵，简记为<span
class="math inline">\(A(\alpha)\)</span>；</p>
<p><span
class="math inline">\(A(\alpha&#39;,\beta&#39;)\)</span>是划去对应行列得到的子矩阵；</p>
<p>设<span
class="math inline">\(\alpha_1,\dotsm,\alpha_t\)</span>组成<span
class="math inline">\(\{1,\dotsm,m\}\)</span>的一个划分，<span
class="math inline">\(\beta_1,\dotsm,\beta_s\)</span>组成<span
class="math inline">\(\{1,\dotsm,n\}\)</span>的一个划分，<span
class="math inline">\(A\in M_{m\times n}(F)\)</span>，<span
class="math inline">\(B\in M_{n\times
p}(F)\)</span>，则分块矩阵乘法可以表示为 <span class="math display">\[
[AB](\alpha_i,\gamma_j)=\sum_{k=1}^sA(\alpha_i,\beta_k)B(\beta_k,\gamma_j)
\]</span></p>
<h3 id="lu分解">2.4 LU分解</h3>
<p>为了求解一系列系数相同的线性方程组：<span
class="math inline">\(Ax=b_i\)</span>，在实际过程中通常是在求解第一个方程组时使用行变换，并得到<span
class="math inline">\(A=LU\)</span>，之后的几个方程组使用<span
class="math inline">\(Ly=b\)</span>,<span
class="math inline">\(Ux=y\)</span>求解，由于<span
class="math inline">\(L\)</span>是下三角矩阵，<span
class="math inline">\(U\)</span>是与<span
class="math inline">\(A\)</span>等价的阶梯型矩阵，这样的求解步骤要快得多。那么如何进行分解？注意到倍加行变换是单位下三角矩阵，他们的积仍然是单位下三角矩阵，逆矩阵也是下三角矩阵，则<span
class="math inline">\(U\)</span>是<span
class="math inline">\(A\)</span>的行变换结果，<span
class="math inline">\(L=(E_p\dots
E_1)^{-1}\)</span>（也不会真的求逆，而是找到<span
class="math inline">\(L\)</span>，使得使用相同的行变换把<span
class="math inline">\(L\)</span>变成<span
class="math inline">\(I\)</span>）</p>
<blockquote>
<p>在上述条件下，要求只能使用倍加行变换来得到阶梯矩阵，实际上，行交换常常是不可避免的，<span
class="math inline">\(L\)</span>可以为置换下三角矩阵</p>
</blockquote>
<h3 id="mathbbrn中的子空间">2.5 <span
class="math inline">\(\mathbb{R}^n\)</span>中的子空间</h3>
<p>这一小节提前定义一些概念，在后面正式讨论向量空间时很有用：</p>
<p><strong>子空间</strong>是<span
class="math inline">\(\mathbb{R}^n\)</span>中的向量子集<span
class="math inline">\(H\)</span>，包含零向量，对向量加法和标量乘法是封闭的；</p>
<p>矩阵的<strong>列空间</strong>是A的各列的线性组合的集合，记作 <span
class="math inline">\(\text{Col} A\)</span>；</p>
<p>矩阵的<strong>零空间</strong>是<span
class="math inline">\(Ax=0\)</span>的所有解的集合，记作<span
class="math inline">\(\text{Nul} A\)</span>；</p>
<p>子空间是一个无限大的集合，希望通过研究生成该子空间的有限向量集合，即一组线性无关集，称为<span
class="math inline">\(H\)</span>的一组<strong>基</strong>；</p>
<p>判断一个向量<span class="math inline">\(b\)</span>是否属于<span
class="math inline">\(\text{Col} A\)</span>，求解<span
class="math inline">\(Ax=b\)</span>是否有解，即<span
class="math inline">\(b\)</span>能否使用<span
class="math inline">\(A\)</span>的各列线性组合得到，这是显式的；判断其是否属于<span
class="math inline">\(\text{Nul}A\)</span>，则看<span
class="math inline">\(Ab=0\)</span>是否成立即可，这是隐式的；</p>
<p>如何得到矩阵零空间的基？求解齐次线性方程组<span
class="math inline">\(Ax=0\)</span>的解，并表示为参数向量形式；</p>
<p>如何得到矩阵列空间的基？将矩阵化为阶梯型，其中的主元列对应着基向量。（因为<span
class="math inline">\(Ax=0\)</span>与<span
class="math inline">\(Bx=0\)</span>有相同的解集，则<span
class="math inline">\(A\)</span>与<span
class="math inline">\(B\)</span>的列的线性相关关系时一致的，因此行变换不改变线性相关关系），但是注意，使用变换前的主元列作为基！</p>
<p>非零子空间的<strong>维数</strong> <span
class="math inline">\(\text{dim}H\)</span>,是<span
class="math inline">\(H\)</span>的任意一个基的向量<strong>个数</strong>（注意不是向量的元素个数），零子空间的维数定义为0；</p>
<p>矩阵<span class="math inline">\(A\)</span>的<strong>秩</strong><span
class="math inline">\(\text{rank} A\)</span>是<span
class="math inline">\(A\)</span>的列空间的维数；</p>
<p>rank A = dim Col A = A的列空间的维数 = A的主元列个数；dim Nul A =
Ax=0的自由变量个数；rank A + dim Nul A = n(A的列数)</p>
<blockquote>
<p>手工求秩看起来很简单，但是在计算机中由于浮点数的精度问题（0!=0），使用主元列个数作为秩会导致误差，通常使用后文介绍的奇异值分解求秩</p>
</blockquote>
<p><strong>可逆矩定理（二）</strong>：</p>
<img src="/2022/08/10/linear-algebra/image-20220812125522496.png" class="" title="可逆矩阵定理(二)">
<h3 id="小秩修正矩阵的逆">2.6 小秩修正矩阵的逆</h3>
<p>如果已知一个矩阵的逆，当再加上一个小秩矩阵时，其逆如何变化？下面这种方法，比直接求逆要高效：</p>
<p>已知<span class="math inline">\(X\)</span>是<span
class="math inline">\(n\times r\)</span>矩阵，<span
class="math inline">\(Y\)</span>是<span class="math inline">\(r\times
n\)</span>矩阵，<span class="math inline">\(R\)</span>是<span
class="math inline">\(r\times r\)</span>奇异矩阵，<span
class="math inline">\(B\)</span>是非奇异的，非奇异矩阵<span
class="math inline">\(A\)</span>是<span class="math inline">\(n\times
n\)</span>的且已知其逆<span class="math inline">\(A^{-1}\)</span>，有:
<span class="math display">\[
B=A+XRY
\]</span> 那么B <span class="math display">\[
B^{-1}=A^{-1}-A^{-1}X(R^{-1}+YA^{-1}X)^{-1}YA^{-1}
\]</span> 因为有： <span class="math display">\[
\begin{aligned}
BB^{-1}&amp;=I-X(R^{-1}+YA^{-1}X)^{-1}YA^{-1} +
XRYA^{-1}-XRYA^{-1}X(R^{-1}+YA^{-1}X)^{-1}YA^{-1}\\
&amp;=I - X\left(
-(R^{-1}+YA^{-1}X)^{-1}+R-RYA^{-1}X(R^{-1}+YA^{-1}X)^{-1}
\right)YA^{-1}\\
&amp;=I-XR\left(
-R^{-1}(R^{-1}+YA^{-1}X)^{-1}+I-YA^{-1}X(R^{-1}+YA^{-1}X)^{-1}
\right)YA^{-1}\\
&amp;=I-XR(-I+I)YA^{-1}
&amp;=I
\end{aligned}
\]</span> 和<span class="math inline">\(B^{-1}B=I\)</span> (证明略)</p>
<blockquote>
<p>笔者不知道<span
class="math inline">\(B^{-1}\)</span>是有什么神奇方法构造出来的，只能证明它是对的</p>
</blockquote>
<p>如果<span
class="math inline">\(r&lt;&lt;n\)</span>，那么等式右边的运算比直接对<span
class="math inline">\(B\)</span>求逆要简单得多；</p>
<p>如果<span class="math inline">\(r=1\)</span>，<span
class="math inline">\(R=[1]\)</span>，那么有： <span
class="math display">\[
B^{-1}=A^{-1} - \frac{1}{1+YA^{-1}X}A^{-1}XYA^{-1}
\]</span></p>
<h2 id="行列式">3. 行列式</h2>
<p>在某本紫色的教材上，这是第一章的内容，常常让大一新生摸不着头脑，为什么要研究一堆数堆放成一个矩阵后，算出一个值？在实际问题中，通常不会求一个很大的矩阵的行列式，也不会使用行列式的值来判断矩阵可逆、求逆矩阵等（尽管在理论上是可以的），<span
class="math inline">\(2\times2\)</span>行列式和<span
class="math inline">\(3\times
3\)</span>行列式还有些几何意义；下面对相关内容进行简单介绍。</p>
<h3 id="行列式的计算法则">3.1 行列式的计算法则</h3>
<p>只对方阵讨论行列式，行列式的定义是递归定义，使用<span
class="math inline">\(A_{ij}\)</span>表示矩阵<span
class="math inline">\(A\)</span>去除第<span
class="math inline">\(i\)</span>行和第<span
class="math inline">\(j\)</span>列后的结果，那么 <span
class="math display">\[
\det A = a_{11}\det A_{11} - a_{12}\det A_{12} + \dots +
(-1)^{1+n}a_{1n}\det A_{1n}
\]</span> 上式是按照第一行展开的，又称<span
class="math inline">\(A\)</span>的第一行的余因子展开式，我们还可以按照任意一行或者一列展开，符号遵循一个棋盘形状（行+列为偶数则为1，否则为0，行列从1开始）</p>
<blockquote>
<p>这种算法，对于<span class="math inline">\(n\times
n\)</span>行列式需要约<span class="math inline">\(n!\)</span>次乘法</p>
</blockquote>
<p>行列式的部分性质：</p>
<ul>
<li><span class="math inline">\(\det A^T = \det A\)</span></li>
<li><span class="math inline">\(\det AB = (\det A)(\det B)\)</span></li>
<li>设<span class="math inline">\(A\)</span>是<span
class="math inline">\(n*n\)</span>矩阵，处理一列之外都是固定的向量，则<span
class="math inline">\(\det A\)</span>是那个可变列向量的线性函数</li>
</ul>
<p>利用起之前的行变换，我们有一种快速的计算行列式的方法。在2.2节中，三种变换的行列式值分别为：<span
class="math inline">\(\det E_1 = 1\)</span>， <span
class="math inline">\(\det E_2 = -1\)</span>，<span
class="math inline">\(\det E_3 =
5\)</span>，事实上，倍加行变换不改变行列式的值，交换两行将行列式的值变为其相反数，倍乘一行将使得行列式的值按相同倍数变化，使用行变换将矩阵变为上三角/下三角矩阵后，求出对角线乘积，再乘以所有行变换矩阵的行列式的积即可；</p>
<blockquote>
<p>一个<span class="math inline">\(n\times
n\)</span>矩阵使用行变换展开大约需要<span
class="math inline">\(2n^3/3\)</span>次运算</p>
</blockquote>
<p>另一种直接展开行列式的方法是<strong>交错和</strong>： <span
class="math display">\[
\det A = \sum_{n}\text{sgn}(\sigma)\prod _{i=1}^na_{i\sigma(i)}
\]</span> 其中<span class="math inline">\(\sigma\)</span>是从1到n的<span
class="math inline">\(n!\)</span>种排列，<span
class="math inline">\(\text{sgn}(\sigma)\)</span>是符号函数，对于<span
class="math inline">\(\sigma\)</span>到顺序排列<span
class="math inline">\(\{1,\dots,n\}\)</span>的最小对换，是偶数时，<span
class="math inline">\(\text{sgn}(\sigma)=1\)</span>，是奇数时<span
class="math inline">\(\text{sgn}(\sigma)=-1\)</span>；</p>
<p>对于<span
class="math inline">\(A\)</span>的<strong>积和式</strong><span
class="math inline">\(\text{per}
A\)</span>，是另一种冠以矩阵函数，只要将上式中的<span
class="math inline">\(\text{sgn}(\sigma)\)</span>换成常量<span
class="math inline">\(1\)</span>即可；</p>
<h3 id="克拉默法则">3.2 克拉默法则</h3>
<p>下面一个法则解决的问题是，已知一个含有参数的增广矩阵（无法做高斯消去），我们能不能写出解的一般形式呢（用于研究解随着某个参数的变化如何变化），特殊地，能不能求出逆矩阵的一般形式呢？</p>
<p><strong>克拉默法则</strong>：对于<span
class="math inline">\(n*n\)</span>的矩阵<span
class="math inline">\(A\)</span>和任意的<span
class="math inline">\(\mathbb{R}^n\)</span>中的向量<span
class="math inline">\(b\)</span>，<span
class="math inline">\(A_i(b)\)</span>表示用b替代A的第i列得到的矩阵，设<span
class="math inline">\(A\)</span>是可逆的，则方程<span
class="math inline">\(Ax=b\)</span>的解可以由下式给出： <span
class="math display">\[
x_i = \frac{\det A_i(b)}{\det A}
\]</span> 该法则可以直接由行列式的乘法性质推出，方程组<span
class="math inline">\(Ax=b\)</span>改写成如下形式： <span
class="math display">\[
AI_i(x)=A_i(b)
\]</span> 两边同时取行列式，注意有<span class="math inline">\(\det
I_i(x)=x_i\)</span> <span class="math display">\[
\det A \det I_i(x)=\det A_i(b)
\]</span></p>
<p>利用克拉默法则，由<span
class="math inline">\(AA^{-1}=E\)</span>，可以得到逆矩阵的一般形式，<span
class="math inline">\(A^{-1}=\frac{1}{\det A} \text{adj}
A\)</span>，其中<strong>伴随矩阵</strong>是由一系列余因子<span
class="math inline">\(C_{ij}\)</span>组成的矩阵的转置；</p>
<h3 id="行列式的几何意义">3.3 行列式的几何意义</h3>
<ul>
<li>若<span class="math inline">\(A\)</span>是<span
class="math inline">\(2*2\)</span>矩阵，则其以其列向量确定的<strong>平行四边形面积</strong>是<span
class="math inline">\(\det A\)</span>；若<span
class="math inline">\(A\)</span>是<span
class="math inline">\(3*3\)</span>矩阵，则其以其列向量确定的<strong>平行六面体体积</strong>是<span
class="math inline">\(\det A\)</span></li>
<li><span class="math inline">\(\det A\)</span>还可以描述在<span
class="math inline">\(\mathbb{R}^2\)</span>中的面或在<span
class="math inline">\(\mathbb{R}^3\)</span>中的体经过<span
class="math inline">\(A\)</span>对应的变换后，面积/体积的变化倍数</li>
</ul>
<h2 id="向量空间">4. 向量空间</h2>
<p>一些民科的视频常常以”人类无法想象超过3维的空间“为噱头开始胡编乱造，什么“第4维度是时间”啊，“9维空间是宇宙”啊，令人啼笑皆非；本章研究向量空间，在满足同样的公理上，会发现高维空间，无非就是空间中的每个向量由更多数组成（长了一点），从几何上无法想象罢了，但是不影响用来解决实际问题。</p>
<h3 id="向量空间的公理与定理">4.1 向量空间的公理与定理</h3>
<img src="/2022/08/10/linear-algebra/image-20220812134646106.png" class="" title="向量空间公理">
<blockquote>
<p>向量空间可以是实的，也可以是虚的，可以是连续的，也可以是离散的</p>
</blockquote>
<p>除了<span
class="math inline">\(\mathbb{R}^n\)</span>之外，一些也是向量空间例子：</p>
<ul>
<li>对于<span class="math inline">\(n\ge 0\)</span>，次数最高位<span
class="math inline">\(n\)</span>的多项式集合<span
class="math inline">\(\mathbb{P}_n\)</span></li>
<li>三维空间中所有有向线段的集合</li>
<li>数的双向无穷序列空间<span
class="math inline">\(\{y_k\}=(\dots,y_{-1}, y_0, y_1, \dots
)\)</span></li>
<li>定义在集合<span
class="math inline">\(\mathbb{D}\)</span>（实数集）上的全体实函数的集合</li>
</ul>
<p>判断向量空间<span class="math inline">\(H\)</span>的一个子集<span
class="math inline">\(V\)</span>是其一个子空间，只需要验证：</p>
<ul>
<li><span class="math inline">\(V\)</span>的零向量在<span
class="math inline">\(H\)</span>中</li>
<li><span
class="math inline">\(H\)</span>的向量加法与标量乘法对自己封闭</li>
</ul>
<blockquote>
<p>对于<span
class="math inline">\(\mathbb{R}^3\)</span>，其（真）子空间只有两种情况，过原点的直线或者过原点的平面</p>
</blockquote>
<h3 id="线性变换">4.2 线性变换</h3>
<p>在第一章中初始过线性变换，这里给出一个严格的定义：</p>
<p>由向量空间<span class="math inline">\(V\)</span>映射到向量空间<span
class="math inline">\(W\)</span>的线性变换<span
class="math inline">\(T\)</span>是一个规则，它将<span
class="math inline">\(V\)</span>中的每个向量<span
class="math inline">\(x\)</span>映射成<span
class="math inline">\(W\)</span>中的唯一向量<span
class="math inline">\(T(x)\)</span>，且满足对加法与数乘的封闭性；</p>
<p>线性变换<span class="math inline">\(T\)</span>（从向量空间<span
class="math inline">\(V\)</span>映射到向量空间<span
class="math inline">\(W\)</span>）的<strong>核</strong>（或零空间）是<span
class="math inline">\(V\)</span>中所有满足<span
class="math inline">\(T(u)=0\)</span>的向量的集合，<span
class="math inline">\(T\)</span>的<strong>值域</strong>是<span
class="math inline">\(W\)</span>中所有具有形式<span
class="math inline">\(T(x)\)</span>的向量的集合；</p>
<p>定义在一般向量空间中的线性相关与<span
class="math inline">\(\mathbb{R}^n\)</span>中的线性相关类似，都是判断一个齐次方程有无非平凡解，但是后者的齐次方程还能写成一个齐次方程组（矩阵），但前者的定义比较广泛，不一定能写成矩阵/线性方程组形式；</p>
<blockquote>
<p>例如，<span class="math inline">\(\{\sin t, \cos t\}\)</span>在<span
class="math inline">\(C[0,1]\)</span>上是线性无关的</p>
</blockquote>
<p>对于一个向量空间<span
class="math inline">\(H\)</span>，我们研究其“最小生成集”，即向量空间的<strong>基</strong>，基是生成的子空间是<span
class="math inline">\(H\)</span>并且线性无关的一组向量集合；</p>
<h3 id="坐标与坐标变换">4.3 坐标与坐标变换</h3>
<p>我们可以向量空间中的基想象成一组坐标轴（可能不标准，不正交，但是一定两两不平行/重合），使用基的线性组合表示向量空间中的任意向量时，组合的权重就是该向量在该基下的坐标；确定基后，坐标是唯一的；</p>
<p><span class="math inline">\(B=(b_1,\dots,b_n)\)</span>是<span
class="math inline">\(V\)</span>的一个基，<span
class="math inline">\(x\)</span>相对于基<span
class="math inline">\(B\)</span>的坐标是那组权重<span
class="math inline">\((c_1,
\dots,c_n)=[x]_B\)</span>，称为x的B-坐标向量，从<span
class="math inline">\(x\)</span>到<span
class="math inline">\([x]_B\)</span>是由<span
class="math inline">\(B\)</span>确定的坐标映射，<span
class="math inline">\(B\)</span>都对应了到标准基的<strong>坐标变换矩阵</strong><span
class="math inline">\(P_B=[b_1,\dots,b_n]\)</span>，该矩阵一定是可逆的；
<span class="math display">\[
x=P_B[x]_B
\]</span> 一个向量空间<span class="math inline">\(V\)</span>的基<span
class="math inline">\(B\)</span>若含有<span
class="math inline">\(n\)</span>个向量(维数是n)，则<span
class="math inline">\(V\)</span>与<span
class="math inline">\(\mathbb{R}^n\)</span><strong>同构</strong>，<span
class="math inline">\(n\)</span>是<span
class="math inline">\(V\)</span>的一个内在性质，不依赖于基的选择；</p>
<p>如果是从一个非标准基变换到另一个非标准基，那么坐标变换矩阵如何写出呢？设<span
class="math inline">\(B\)</span>和<span
class="math inline">\(C\)</span>是向量空间<span
class="math inline">\(V\)</span>的基(<span
class="math inline">\(n\)</span>列)，则存在一个<span
class="math inline">\(n\times n\)</span>坐标矩阵<span
class="math inline">\(P_{C\leftarrow B}\)</span>，使得 <span
class="math display">\[
[x]_C = P_{C\leftarrow B}[x]_B
\]</span> 由于在标准坐标变换中，有 <span class="math display">\[
P_B[x]_B=P_C[x]_C
\]</span> 则 <span class="math display">\[
P_{C\leftarrow B} = P_{C}^{-1}P_{B}
\]</span> 有时根据情景我们可以直接写出：<span
class="math inline">\(P_{C\leftarrow
B}\)</span>的列是基B中向量的C-坐标向量，（<span
class="math inline">\(P_{C\leftarrow B}\)</span>的列也是C-坐标向量）</p>
<h3 id="秩">4.4 秩</h3>
<p>在2.5节中，我们定义秩是列空间的维数，也就是线性无关列的最大个数。那与线性无关行的最大个数有关系吗？事实上，他们都是相等的；</p>
<p>首先，若两个矩阵<span class="math inline">\(A\)</span>与<span
class="math inline">\(B\)</span>行等价，则他们的行空间相同，若<span
class="math inline">\(B\)</span>是阶梯型矩阵，则<strong><span
class="math inline">\(B\)</span>的非零行</strong>构成<span
class="math inline">\(A\)</span>的行空间的一个基，同时也是B的行空间的一个基（因为对于阶梯型矩阵，非零行是线性无关的，任何一个非零行不能是它下面的非零行的线性组合），那么同样可以使用行变换来求解<strong>行空间</strong>的基，只不过在求列的基时，使用行变换后主元列的列号到原矩阵中索引原始列；而在求行的基是时，直接使用行变换后的非零行；</p>
<blockquote>
<p>注意，与列不同，行变换不保持行的线性相关关系，但是新行是旧行的线性组合，因此B的非零行对应A的行不一定线性无关</p>
</blockquote>
<p>有如下关系：矩阵的列空间的维数=行空间的维数=主元列的个数=阶梯型非零行的个数=矩阵的秩；</p>
<p><strong>关于秩的不等式</strong></p>
<ul>
<li>对于<span class="math inline">\(A \in M_{m\times
n}(F)\)</span>，<span class="math inline">\(\text{rank}A\le
\min\{m,n\}\)</span></li>
<li>一个矩阵划去若干行或列后，得到的矩阵的秩不大于原来的矩阵的秩；</li>
<li>如果<span class="math inline">\(A\in M_{m\times
k}(F)\)</span>，<span class="math inline">\(B\in M_{k\times
n}(F)\)</span>，则<span class="math inline">\(\text{rank}A +\text{rank}B
- k \le \text{rank}AB \le \min\{\text{rank}A,
\text{rank}B\}\)</span></li>
<li>如果<span class="math inline">\(A,B\in M_{m\times
n}(F)\)</span>，则<span class="math inline">\(\text{rank}(A+B)\le
\text{rank}A + \text{rank} B\)</span></li>
<li>如果<span class="math inline">\(A\in M_{m\times
k}(F)\)</span>，<span class="math inline">\(B\in M_{k\times
p}(F)\)</span>，<span class="math inline">\(C\in M_{p\times
n}(F)\)</span>，则<span class="math inline">\(\text{rank}AB
+\text{rank}BC \le \text{rank}B + \text{rank}ABC\)</span></li>
</ul>
<p><strong>关于秩的等式</strong>：</p>
<ul>
<li><p>左乘或者右乘非奇异矩阵，秩不变；</p></li>
<li><p>如果<span class="math inline">\(A,B\in M_{m\times
n}(F)\)</span>，则<span
class="math inline">\(\text{rank}A=\text{rank}B\)</span>，当且仅当存在非奇异矩阵<span
class="math inline">\(X\in M_{m}(F), Y\in M_n(F)\)</span>，使得<span
class="math inline">\(B=XAY\)</span>；</p></li>
<li><p>如果矩阵<span class="math inline">\(A\in M_{m\times
n}(F)\)</span>有秩<span
class="math inline">\(k\)</span>，那么存在非奇异矩阵<span
class="math inline">\(X\in M_{m\times k}(F), Y\in M_{k\times n}(F), B\in
M_{k}(F)\)</span>，那么 <span class="math display">\[
A=XBY
\]</span> 特殊地，对于任何秩为1的矩阵，都可以写成<span
class="math inline">\(A=xy^T\)</span>，其中<span
class="math inline">\(x\in F^m, y\in F^m\)</span>是两个列向量；</p></li>
</ul>
<h2 id="特征值与特征向量">5. 特征值与特征向量</h2>
<p>特征值/特征向量，对于一个矩阵而言，究竟意味着什么？可能是找到一个向量，使得它在应用了该矩阵后，没有改变方向？可能是对于一个主特征值严格大于其他特征值的<span
class="math inline">\(A^k\)</span>，当<span
class="math inline">\(k\rightarrow
+\infty\)</span>时，都会把任意非零向量”撇“到其主特征向量方向上去？我也需要在更多应用中慢慢理解；</p>
<h3 id="特征值向量方程">5.1 特征值、向量、方程</h3>
<p>定义如下：</p>
<p><span class="math inline">\(A\)</span>为<span
class="math inline">\(n\times n\)</span>矩阵，<span
class="math inline">\(x\)</span>为非零向量，若存在数<span
class="math inline">\(\lambda\)</span>使得<span
class="math inline">\(Ax=\lambda x\)</span>成立，则<span
class="math inline">\(\lambda\)</span>为<span
class="math inline">\(A\)</span>的<strong>特征值</strong>，<span
class="math inline">\(x\)</span>称为对于<span
class="math inline">\(\lambda\)</span>的<strong>特征向量</strong>；</p>
<p><strong>特征方程</strong>： <span
class="math inline">\(\det(A-\lambda I) = 0\)</span>，<span
class="math inline">\(\lambda\)</span>作为方程根的重数，称为<span
class="math inline">\(\lambda\)</span>的（代数）重数</p>
<p>我们又可以补充<strong>可逆矩阵定理（三）：</strong></p>
<p><span class="math inline">\(A\)</span>是<span
class="math inline">\(n\times n\)</span>矩阵，则<span
class="math inline">\(A\)</span>是可逆的当且仅当</p>
<ul>
<li>0不是A的特征值</li>
<li>A的行列式不等于0</li>
</ul>
<p>特征向量之间的线性相关性：</p>
<p><span class="math inline">\(\lambda_1, \dots,
\lambda_r\)</span>是<span class="math inline">\(n\times
n\)</span>矩阵<span class="math inline">\(A\)</span>相异的特征值，<span
class="math inline">\(v_1,\dots, v_r\)</span>是对应的特征向量，则<span
class="math inline">\(\{v_1,\dots, v_r\}\)</span>线性无关；</p>
<p>与一阶差分方程的关系：</p>
<p>一阶差分方程<span
class="math inline">\(x_{k+1}=Ax_{k}\)</span>的解，可以通过找到<span
class="math inline">\(A\)</span>的一个特征值，得到<span
class="math inline">\(x_{k+1}=\lambda^kx_0\)</span>；</p>
<h3 id="对角化">5.2 对角化</h3>
<p>将矩阵<span
class="math inline">\(A\)</span>的所有特征值写入对角矩阵<span
class="math inline">\(D\)</span>，对应的特征向量按列写入矩阵<span
class="math inline">\(P\)</span>，则有<span
class="math inline">\(AP=PD\)</span>，如果<span
class="math inline">\(P\)</span>可逆，那么有<span
class="math inline">\(A=PDP^{-1}\)</span>，这种形式有“好处”，我们可以专门定义一种<strong>相似性</strong>：</p>
<p>A和B是<span
class="math inline">\(n*n\)</span>矩阵，如果存在可逆矩阵<span
class="math inline">\(P\)</span>，使得<span
class="math inline">\(P^{-1}AP =
B\)</span>，则称A相似于B，把A变成B的变换称为相似变换；</p>
<p>相似性的特点/好处是：</p>
<ul>
<li><p>若矩阵A,B是相似的，则他们有相同的特征多项式核相同的特征值（重数也相同）</p>
<blockquote>
<p>行变换通常会改变特征值</p>
</blockquote></li>
<li><p>将A写成<span
class="math inline">\(A=PDP^{-1}\)</span>的形式，其中<span
class="math inline">\(P\)</span>为可逆矩阵，<span
class="math inline">\(D\)</span>为对角矩阵，那么<span
class="math inline">\(A^k = PD^kP^{-1}\)</span></p></li>
</ul>
<p>什么条件下，<span class="math inline">\(P\)</span>可逆呢？</p>
<p><strong>对角化定理</strong>：设<span
class="math inline">\(A\)</span>是<span
class="math inline">\(n*n\)</span>矩阵，其特征向量为<span
class="math inline">\(\lambda_1, \dots, \lambda_p\)</span>，当<span
class="math inline">\(p=n\)</span>或者<span
class="math inline">\(p&lt;n\)</span>但每个<span
class="math inline">\(\lambda_k\)</span>对应的特征空间的维数等于特征方程在<span
class="math inline">\(\lambda_k\)</span>的重数，当且仅当<span
class="math inline">\(A\)</span>可以对角化；</p>
<h3 id="特征向量与线性变换">5.3 特征向量与线性变换</h3>
<p>先回顾一下坐标变换：</p>
<p>线性变换的矩阵：在从<span
class="math inline">\(n\)</span>维向量空间<span
class="math inline">\(V\)</span>变换到<span
class="math inline">\(m\)</span>维向量空间<span
class="math inline">\(W\)</span>时，设<span
class="math inline">\(B\)</span>和<span
class="math inline">\(C\)</span>分别是<span
class="math inline">\(V\)</span>和<span
class="math inline">\(W\)</span>的基，我们知道可以写出标准矩阵<span
class="math inline">\(A\)</span>(<span class="math inline">\(m\times
n\)</span>)直接对向量进行变换, 即<span
class="math inline">\(T(x)=Ax\)</span>，现在介绍一种直接对向量坐标变换的矩阵<span
class="math inline">\(M\)</span>，使得<span
class="math inline">\([T(x)]_C = M[x]_B\)</span>：</p>
<p>其中 <span class="math display">\[
M=[T(b_1)_C \dots T(b_n)_C]
\]</span> <span class="math inline">\(T(b_1)_C\)</span>是类似于<span
class="math inline">\(b_1=t_1c_1 + t_2c_2 +
t_3c_3\)</span>中的参数向量<span class="math inline">\((t_1, t_2,
t_3)^T\)</span></p>
<p>当<span class="math inline">\(B\)</span>与<span
class="math inline">\(C\)</span>是同一线性空间的基时，矩阵<span
class="math inline">\(M\)</span>即为先前介绍的坐标变换矩阵；</p>
<p>当<span class="math inline">\(W=V\)</span>,<span
class="math inline">\(C=B\)</span>时，<span
class="math inline">\(M\)</span>称为为变换<span
class="math inline">\(T\)</span>相对于<span
class="math inline">\(B\)</span>的矩阵，或者<span
class="math inline">\(B-\)</span>矩阵；</p>
<p>下面的证明，旨在从对角化出发，推广到相似矩阵可以充当该矩阵对应线性变换的<span
class="math inline">\(B-\)</span>矩阵：</p>
<p>对于<span
class="math inline">\(\mathbb{R}^n\)</span>上的线性变换<span
class="math inline">\(x\mapsto Ax\)</span>，假设<span
class="math inline">\(A\)</span>是可对角化的，那么存在由<span
class="math inline">\(A\)</span>的特征向量组成的<span
class="math inline">\(\mathbb{R}^n\)</span>的基，<span
class="math inline">\(A=PDP^{-1}\)</span>，则对角矩阵<span
class="math inline">\(D\)</span>是变换<span
class="math inline">\(x\mapsto Ax\)</span>的<span
class="math inline">\(B-\)</span>矩阵；<span
class="math inline">\(P\)</span>也是（到标准坐标的）坐标变换矩阵，有<span
class="math inline">\([x]_B=P^{-1}x\)</span> <span
class="math display">\[
\begin{aligned}\\
[T]_B &amp;= [[T(b_1)]_B\dots[T(b_n)]_B]\\
&amp;= [[Ab_1]_B\dots[Ab_n]_B]\\
&amp;= [P^{-1}Ab_1\dots P^{-1}Ab_n]\\
&amp;= P^{-1}A[b_1\dotsb_n]\\
&amp;= P^{-1}AP\\
&amp;= D
\end{aligned}
\]</span> 推广上述结论，只要矩阵<span
class="math inline">\(A\)</span>与<span
class="math inline">\(C\)</span>相似，有<span
class="math inline">\(A=PCP^{-1}\)</span>，在<span
class="math inline">\(P\)</span>的列向量组成的基<span
class="math inline">\(B\)</span>下，有变换<span
class="math inline">\(x\rightarrow Ax\)</span>的<span
class="math inline">\(B-\)</span>矩阵<span
class="math inline">\(C\)</span>；反之也成立，任意的基<span
class="math inline">\(B\)</span>，变换<span
class="math inline">\(x\rightarrow Ax\)</span>的<span
class="math inline">\(B-\)</span>矩阵相似于<span
class="math inline">\(A\)</span>；</p>
<h3 id="其他">5.4 其他</h3>
<p>方阵<span
class="math inline">\(A\)</span>没有足够的特征向量组成可逆的<span
class="math inline">\(P\)</span>，但是仍可以通过找到一组基构造一个可逆的<span
class="math inline">\(P\)</span>,并得到的<span
class="math inline">\(PAP^{-1}\)</span>是三角矩阵，称之为<span
class="math inline">\(A\)</span>的<strong>约当形</strong>；</p>
<h2 id="正交性与最小二乘法">6. 正交性与最小二乘法</h2>
<p>在实际问题中，<span
class="math inline">\(Ax=b\)</span>通常无解（方程数大于未知数个数），希望找到一个<span
class="math inline">\(\hat{x}\)</span>使得<span
class="math inline">\(A\hat{x}\)</span>尽可能接近<span
class="math inline">\(b\)</span>，这是我们讨论这一章的一个目的。</p>
<h3 id="基本概念">6.1 基本概念</h3>
<p>向量的内积、长度、距离、正交的概念按下不表；</p>
<p>如果向量<span class="math inline">\(z\)</span>与在<span
class="math inline">\(\mathbb{R}^n\)</span>的子空间<span
class="math inline">\(W\)</span>中的任意向量都正交，则<span
class="math inline">\(z\)</span>正交于<span
class="math inline">\(W\)</span>，与<span
class="math inline">\(W\)</span>正交的<span
class="math inline">\(z\)</span>的全体的集合称为<span
class="math inline">\(W\)</span>的<strong>正交补</strong>，即<span
class="math inline">\(W^{\perp}\)</span>；</p>
<p>设<span class="math inline">\(A\)</span>是<span
class="math inline">\(m\times n\)</span>矩阵，则<span
class="math inline">\((\text{Row} A)^{\perp} = \text{Nul}
A\)</span>，且<span class="math inline">\((\text{Col} A)^{\perp} =
\text{Nul} A^T\)</span>；</p>
<blockquote>
<p><span class="math inline">\(x\)</span>是<span
class="math inline">\(\text{Nul} A\)</span>中的向量，则<span
class="math inline">\(Ax=0\)</span>，即<span
class="math inline">\(x\)</span>与<span
class="math inline">\(A\)</span>的每一行都正交</p>
</blockquote>
<h3 id="正交集">6.2 正交集</h3>
<p><strong>正交集</strong>，是一个集合中的向量两两正交；<strong>单位正交集</strong>，要求是单位向量；</p>
<p>如果<span class="math inline">\(S\)</span>是<span
class="math inline">\(\mathbb{R}^n\)</span>中由非零向量组成的正交集，则<span
class="math inline">\(S\)</span>是线性无关集，是一组基；</p>
<p>正交基比其他基优越，因为正交基线性组合的权值比较容易计算，<span
class="math inline">\(c_j = \frac{y\cdot u_j}{u_j\cdot
u_j}\)</span>，对于标准正交基（单位正交基）该权重的分母为1；</p>
<p>一个<span class="math inline">\(m\times n\)</span>矩阵<span
class="math inline">\(U\)</span>具有单位正交列向量的充要条件是<span
class="math inline">\(U^TU=I\)</span>，<span
class="math inline">\(U^TUx=x\)</span>对于所有<span
class="math inline">\(\mathbb{R}^n\)</span>中的<span
class="math inline">\(x\)</span>都成立；</p>
<blockquote>
<p>因而当<span class="math inline">\(U\)</span>是方阵时，有<span
class="math inline">\(U^T=U^{-1}\)</span></p>
</blockquote>
<p>设<span class="math inline">\(U\)</span>是一个具有单位正交列的<span
class="math inline">\(m\times n\)</span>矩阵，且<span
class="math inline">\(x\)</span>和<span
class="math inline">\(y\)</span>是<span
class="math inline">\(\mathbb{R}^n\)</span>的向量，则：</p>
<ul>
<li><span class="math inline">\(\norm{Ux}=\norm{x}\)</span></li>
<li><span class="math inline">\((Ux)\cdot(Uy)=x\cdot y\)</span></li>
<li><span
class="math inline">\((Ux)\cdot(Uy)=0\)</span>的充分必要条件是<span
class="math inline">\(x\cdot y = 0\)</span></li>
</ul>
<h3 id="正交投影">6.3 正交投影</h3>
<p>如果<span class="math inline">\(\{u_1,\dots,u_p\}\)</span>是<span
class="math inline">\(\mathbb{R}^n\)</span>中子空间<span
class="math inline">\(W\)</span>的单位正交基，那么<span
class="math inline">\(y\)</span>在<span
class="math inline">\(W\)</span>上的投影向量为 <span
class="math display">\[
\text{proj}_W y = (y\cdot u_1)u_1 + \dots + (y\cdot u_p)u_p
\]</span> 如果<span
class="math inline">\(U=[u_1,\dots,u_p]\)</span>，则对于所有的<span
class="math inline">\(y\in\mathbb{R}^n\)</span> <span
class="math display">\[
\text{proj}_Wy=UU^Ty
\]</span></p>
<h3 id="格拉姆-施密特方法">6.4 格拉姆-施密特方法</h3>
<p>对于<span
class="math inline">\(\mathbb{R}^n\)</span>中任何非0子空间，构造正交基或者标准正交基的算法，基本思想是，从1维子空间开始，不断纳入新的向量，是当前向量减去其在已知正交基的子空间中的投影得到的；</p>
<p>详细过程暂略；</p>
<h3 id="矩阵qr分解">6.5 矩阵QR分解</h3>
<p>如果<span class="math inline">\(m\times n\)</span>的矩阵<span
class="math inline">\(A\)</span>的列线性无关，则<span
class="math inline">\(A=QR\)</span>,其中<span
class="math inline">\(Q\)</span>的列是<span
class="math inline">\(\text{Col}
A\)</span>的一个标准正交基，施密特方法保证了<span
class="math inline">\(R\)</span>是上三角矩阵，并且是可逆的，对角线元素为正；
<span class="math display">\[
Q^TA=Q^T(QR)=IR=R
\]</span></p>
<h3 id="最小二乘法">6.6 最小二乘法</h3>
<p>最小二乘问题又可以理解为，在<span
class="math inline">\(\text{Col}A\)</span>中寻找与<span
class="math inline">\(b\)</span>最接近的向量，<span
class="math inline">\(\hat{b}=\text{proj}_{\text{Col}A}b\)</span>，求解<span
class="math inline">\(A\hat{x}=\hat{b}\)</span>，由向量与投影向量的差垂直于投影平面的性质，有<span
class="math inline">\(b-A\hat{x}\)</span>正交于<span
class="math inline">\(A\)</span>的每一列，即<span
class="math inline">\(A^T(b-A\hat{x})=0\)</span>，进而得到最小二乘解的法方程
<span class="math display">\[
A^TAx=A^Tb
\]</span> <span
class="math inline">\(A^TA\)</span>可逆的充要条件是，<span
class="math inline">\(A\)</span>的列是线性无关的；</p>
<p>使用法方程求解最小二乘解可能因为<span
class="math inline">\(A^TA\)</span>的病态造成较大误差，<span
class="math inline">\(QR\)</span>分解也可以用来求解最小二乘解：设<span
class="math inline">\(m\times n\)</span>矩阵<span
class="math inline">\(A\)</span>具有线性无关的列，则<span
class="math inline">\(A=QR\)</span>，<span
class="math inline">\(Ax=b\)</span>的最小二乘解是<span
class="math inline">\(x=R^{-1}Q^Tb\)</span>，可以验证，<span
class="math inline">\(Ax=QQ^Tb\)</span>，正是<span
class="math inline">\(b\)</span>在<span
class="math inline">\(\text{Col}A\)</span>上的投影；</p>
<h2 id="对称矩阵和二次型">7. 对称矩阵和二次型</h2>
<h3 id="对称矩阵">7.1 对称矩阵</h3>
<p>对称矩阵比其他类型的矩阵更常出现在应用中，结合前几章的知识，我们会得到许多对称矩阵非常好的性质：</p>
<p>如果<span
class="math inline">\(A\)</span>是对称矩阵，那么不同特征空间的任意两个特征向量是正交的，且<span
class="math inline">\(A\)</span>能满足对角化条件，则<span
class="math inline">\(A=PDP^{T}\)</span> (<span
class="math inline">\(P^T=P^{-1}\)</span>)： <span
class="math inline">\(n\times n\)</span>的矩阵<span
class="math inline">\(A\)</span>可以<strong>正交对角化</strong>的充要条件就是<span
class="math inline">\(A\)</span>是对称矩阵;</p>
<blockquote>
<p>对于一个特征值得到的多个特征向量，却不一定是正交的，需要使用格拉姆施密特方法化为该特征空间的单位正交基</p>
</blockquote>
<p>矩阵<span class="math inline">\(A\)</span>的特征值的集合称为<span
class="math inline">\(A\)</span>的<strong>谱</strong>，有如下<strong>谱定理</strong>：</p>
<img src="/2022/08/10/linear-algebra/image-20220812181851605.png" class="" title="谱定理">
<p>对称矩阵<strong>谱分解</strong>，即把正交对角化的结果展开</p>
<img src="/2022/08/10/linear-algebra/image-20220812182043366.png" class="" title="谱分解">
<h3 id="二次型">7.2 二次型</h3>
<p><span
class="math inline">\(\mathbb{R}^n\)</span>上的二次型是一个定义在<span
class="math inline">\(\mathbb{R}^n\)</span>上的函数，在向量<span
class="math inline">\(x\)</span>处的值为<span
class="math inline">\(Q(x)=x^TAx\)</span>，<span
class="math inline">\(A\)</span>是<span
class="math inline">\(n*n\)</span>对称矩阵，称为关于二次型的矩阵；</p>
<p>我们期望得到一个没有交叉项的二次型，便于求极值等，对应的是对角矩阵，因此我们引入二次型的变量代换：</p>
<p><span class="math inline">\(x\)</span>是<span
class="math inline">\(\mathbb{R}^n\)</span>中的一个向量变量，<span
class="math inline">\(P\)</span>的列是<span
class="math inline">\(\mathbb{R}^n\)</span>的一个基，<span
class="math inline">\(x=Py\)</span>，<span
class="math inline">\(y\)</span>是<span
class="math inline">\(\mathbb{R}^n\)</span>中的一个新变量： <span
class="math display">\[
x^TAx=(Py)^TA(Py)=y^T(P^TAP)y
\]</span> 新的二次型矩阵是<span
class="math inline">\(P^TAP\)</span>，如果<span
class="math inline">\(A\)</span>能够正交对角化，则<span
class="math inline">\(P^TAP=p{-1}AP=D\)</span>，<span
class="math inline">\(D\)</span>是<span
class="math inline">\(A\)</span>的特征值组成的对角矩阵；</p>
<p><span
class="math inline">\(P\)</span>称为二次型的主轴，这个变量代换的过程，在<span
class="math inline">\(\mathbb{R}^2\)</span>空间中想象，就是将以原点为中心的二次曲线（已经是关于原点对称的，没有一次项或者常数项）的主轴旋转到<span
class="math inline">\(x\)</span>轴和<span
class="math inline">\(y\)</span>轴方向；</p>
<p>二次型<span class="math inline">\(Q(x)=x^TAx\)</span>有以下分类：</p>
<ol type="1">
<li>正定的，如果对于所有<span
class="math inline">\(x\ne0\)</span>有<span
class="math inline">\(Q(x)&gt;0\)</span>，当且仅当<span
class="math inline">\(A\)</span>的所有特征值全正；</li>
<li>负定的，如果对于所有<span
class="math inline">\(x\ne0\)</span>有<span
class="math inline">\(Q(x)&lt; 0\)</span>，当且仅当<span
class="math inline">\(A\)</span>的所有特征值全负；</li>
<li>不定的，有正有负</li>
</ol>
<h3 id="条件优化">7.3 条件优化</h3>
<p>设<span class="math inline">\(A\)</span>是对称矩阵，<span
class="math inline">\(m\)</span>是<span
class="math inline">\(A\)</span>的最小特征值，<span
class="math inline">\(M\)</span>是<span
class="math inline">\(A\)</span>的最大特征值，如果<span
class="math inline">\(x\)</span>是对应<span
class="math inline">\(M\)</span>的单位特征向量，那么<span
class="math inline">\(x^TAx=M\)</span>，如果<span
class="math inline">\(x\)</span>是对应<span
class="math inline">\(m\)</span>的单位特征向量，则<span
class="math inline">\(x^TAx=m\)</span>；</p>
<p>在<span class="math inline">\(||x||=1\)</span>的条件下，<span
class="math inline">\(m\)</span>和<span
class="math inline">\(M\)</span>分别是该二次型的最小值和最大值；</p>
<img src="/2022/08/10/linear-algebra/image-20220812190112177.png" class="" title="条件优化">
<h3 id="奇异值分解">7.4 奇异值分解</h3>
<p>不是所有矩阵都有分解式子<span
class="math inline">\(A=PDP^{-1}\)</span>且使得<span
class="math inline">\(D\)</span>是对角的，但是分解<span
class="math inline">\(A=QDP^{-1}\)</span>对任意的<span
class="math inline">\(m\times n\)</span>矩阵<span
class="math inline">\(A\)</span>都有可能；</p>
<p>回顾之前的特征值的定义，如果<span
class="math inline">\(||x||=1\)</span>，则 <span class="math display">\[
||Ax||=\lambda
\]</span> 对于最大特征值对应的特征空间中的向量，均能使得<span
class="math inline">\(||Ax||\)</span>最大化；在<span
class="math inline">\(||Ax||\)</span>最大时，也能保证<span
class="math inline">\(||Ax||^2=x^T(A^TA)x\)</span>取得最大值，这是个二次型，其中<span
class="math inline">\(A^TA\)</span>是对称矩阵，在<span
class="math inline">\(||x||=1\)</span>的条件下，问题转化为了上一小节中讨论的条件优化问题。以下，我们引出奇异值的定义：</p>
<p>令<span class="math inline">\(A\)</span>是<span
class="math inline">\(m\times n\)</span>矩阵，那么<span
class="math inline">\(A^TA\)</span>是对称矩阵且可以正交对角化，让<span
class="math inline">\(\{v_1,\dots,v_n\}\)</span>是<span
class="math inline">\(\mathbb{R}^n\)</span>额单位正交基且构成<span
class="math inline">\(A^TA\)</span>的特征向量，<span
class="math inline">\(\lambda_1,\dots,\lambda_n\)</span>是对应的特征值，那么对于<span
class="math inline">\(1 \le i \le n\)</span>有： <span
class="math display">\[
||Av_i||^2=v_i^TA^TAv_i=v_i^T(\lambda_iv_i)=\lambda_i
\]</span> 所以，<strong><span
class="math inline">\(A^TA\)</span>所有特征值都非负</strong>（半正定的），按照从大到小排序，并开根号，即为<span
class="math inline">\(A\)</span>的<strong>奇异值</strong><span
class="math inline">\(\sigma_1,\dots,\sigma_n\)</span>，奇异值是向量<span
class="math inline">\(Av_1,\dots,Av_n\)</span>的长度；</p>
<p>在对<span
class="math inline">\(A^TA\)</span>的特征值排序后，设有<span
class="math inline">\(r\)</span>个非零特征值，即对应<span
class="math inline">\(r\)</span>个非零奇异值，那么<span
class="math inline">\(Av_1,\dots,Av_r\)</span>是<span
class="math inline">\(\text{Col} A\)</span>的一个正交基，<span
class="math inline">\(\text{rank} A = r\)</span></p>
<blockquote>
<p>这就是计算机计算矩阵的秩的方法——非零奇异值的个数</p>
</blockquote>
<p>矩阵<span class="math inline">\(A\)</span>（<span
class="math inline">\(m\times n\)</span>）的奇异值分解，涉及一个<span
class="math inline">\(m\times n\)</span>的矩阵<span
class="math inline">\(\Sigma\)</span>，左上角是<span
class="math inline">\(r\times r\)</span>的对角矩阵<span
class="math inline">\(D\)</span>，对角线元素是<span
class="math inline">\(A\)</span>的奇异值从大到小排列，另外还有一个<span
class="math inline">\(m\times m\)</span>的正交矩阵<span
class="math inline">\(U\)</span>和<span class="math inline">\(n\times
n\)</span>的正交矩阵<span class="math inline">\(V\)</span>，使得<span
class="math inline">\(A=U\Sigma V^T\)</span></p>
<p><span class="math inline">\(V\)</span>的前<span
class="math inline">\(r\)</span>列正是<span
class="math inline">\(A^TA\)</span>中对应的<span
class="math inline">\(r\)</span>个特征向量标准化的结果；而<span
class="math inline">\(U\)</span>的前<span
class="math inline">\(r\)</span>列构造为<span
class="math inline">\(Av_i\)</span>标准化的结果 <span
class="math display">\[
u_i=\frac{1}{\sigma_i}Av_i
\]</span></p>
<p><span class="math display">\[
U\Sigma = AV
\]</span></p>
<p>我们将<span
class="math inline">\(U\)</span>的列称为左奇异向量，将<span
class="math inline">\(V\)</span>的列称为右奇异向量；当<span
class="math inline">\(r&lt;m\)</span>或者<span class="math inline">\(r
&lt; n\)</span>时，<span class="math inline">\(U\)</span>与<span
class="math inline">\(V\)</span>的剩余列怎么填？</p>
<p>可以先利用正交性算出一组基并且使用施密特方法与先前的基进行正交化，不过，抛弃那几列，让<span
class="math inline">\(U_r\)</span>是<span class="math inline">\(m\times
r\)</span>，<span class="math inline">\(V_r\)</span>是<span
class="math inline">\(n\times r\)</span>，仍有<span
class="math inline">\(U\Sigma V^T=U_rD
V_r^T\)</span>，即简化的<strong>奇异值分解</strong>；</p>
<p>一些结合先前定义的结论:</p>
<ul>
<li><span class="math inline">\(\{u_1,\dots,u_r\}\)</span>是<span
class="math inline">\(\text{Col}A\)</span>的标准正交基</li>
<li><span class="math inline">\(\{u_{r+1},\dots,u_{m}\}\)</span>是<span
class="math inline">\(\text{Nul}A^T\)</span>的标准正交基</li>
<li><span class="math inline">\(\{v_{r+1},\dots,v_{n}\}\)</span>是<span
class="math inline">\(\text{Nul} A\)</span>的标准正交基</li>
<li><span class="math inline">\(\{v_1,\dots,v_r\}\)</span>是<span
class="math inline">\(\text{Row}A\)</span>的标准正交基</li>
</ul>
<p><strong>伪逆</strong>（穆尔-彭罗斯逆）<span
class="math inline">\(A^+=V_rD^{-1}U_r^T\)</span></p>
<p>伪逆的另一种定义是： <span class="math display">\[
A^+ = \lim_{\epsilon\rightarrow 0}(A^TA+\epsilon I)^{-1}A^T =
\lim_{\epsilon\rightarrow 0}A^T(AA^T+\epsilon I)^{-1}
\]</span></p>
<p>设 <span class="math inline">\(A\in \mathbb{R}^{m\times
n}\)</span>，若 <span class="math inline">\(\operatorname{rank} A =
m\)</span>，则 <span class="math inline">\(A^{+} =
A^T(AA^T)^{-1}\)</span>，若 <span
class="math inline">\(\operatorname{rank} A = n\)</span>，则 <span
class="math inline">\(A^{+} = (A^TA)^{-1}A^T\)</span>;</p>
<p>使用伪逆也可以表示最小二乘解 <span class="math display">\[
\hat{x}=A^+b=V_rD^{-1}U_r^T
\]</span> 因为 <span class="math display">\[
Ax=(U_rDV_r^T)(V_rD^{-1}U_r^T)b=U_rU_r^Tb
\]</span> 仍然推出了在<span class="math inline">\(\text{Col}
A\)</span>上的正交投影；</p>
<p><strong>可逆矩阵定理（四）</strong>：</p>
<p>设<span class="math inline">\(A\)</span>为<span
class="math inline">\(n\times n\)</span>矩阵，则下列命题与<span
class="math inline">\(A\)</span>可逆等价：</p>
<ul>
<li><span class="math inline">\((\text{Col} A)^{\perp} =
\{0\}\)</span>；</li>
<li><span class="math inline">\((\text{Nul} A)^{\perp}=
\mathbb{R}^n\)</span>；</li>
<li><span class="math inline">\(\text{Row}A=\mathbb{R}^n\)</span>；</li>
<li><span class="math inline">\(A\)</span>有<span
class="math inline">\(n\)</span>个非零的奇异值；</li>
</ul>
<h3 id="主成分分析">7.5 主成分分析</h3>
<p><strong>观测矩阵</strong>：一个由<span
class="math inline">\(N\)</span>次采样<span
class="math inline">\(p\)</span>维数据得到的<span
class="math inline">\(p\times N\)</span>矩阵；</p>
<p><strong>样本均值</strong>：<span
class="math inline">\(M=\frac{1}{N}(X_1+\dots+X_N)\)</span></p>
<p>使用样本的平均偏差表示<span class="math inline">\(B=X-M\)</span></p>
<p><strong>协方差矩阵</strong>：<span
class="math inline">\(S=\frac{1}{N-1}BB^T\)</span></p>
<p>协方差矩阵是对称的，是半正定的，其中 <span class="math display">\[
S_{ij}=\sum_{k}^{N}(X_{ik}-M_i)(X_{jk}-M_j)
\]</span> 总方差为协方差对角线元素之和，即<span
class="math inline">\(\text{tr}(S)\)</span>；</p>
<p>如果<span class="math inline">\(S_{ij}=0\)</span>，我们认为第<span
class="math inline">\(i\)</span>个变量与第<span
class="math inline">\(j\)</span>个变量是无关的；</p>
<p>之后我们假设<span
class="math inline">\(X_1,\dots,X_N\)</span>已经是平均偏差形式，<strong>主成分分析</strong>的目标就是确定一个变量代换<span
class="math inline">\(X=PY\)</span>，其中<span
class="math inline">\(P=[u_1,\dots,
u_p]\)</span>是正交矩阵，使得代换后的新变量<span
class="math inline">\(y_1,\dots,
y_p\)</span>是两两无关的，并且方差递减；</p>
<p>我们可以求出<span class="math inline">\(Y\)</span>的协方差 <span
class="math display">\[
\begin{aligned}
\frac{1}{N-1}YY^T &amp;= \frac{1}{N-1}P^TXX^TP \\
&amp;= P^TSP
\end{aligned}
\]</span> 期望<span
class="math inline">\(P^TSP\)</span>为对角型，设<span
class="math inline">\(D\)</span>是<span
class="math inline">\(S\)</span>的特征值由大到小排序的对角矩阵，一定有<span
class="math inline">\(S=PDP^T\)</span>其中<span
class="math inline">\(P\)</span>是单位正交矩阵，那么不妨就让我们期望的<span
class="math inline">\(P^TSP=D\)</span>；</p>
<p>协方差矩阵<span class="math inline">\(S\)</span>的单位特征向量<span
class="math inline">\(u_1,\dots, u_p\)</span>就是<span
class="math inline">\(X\)</span>的主成分，第一主成分是最大的特征值对应的特征向量；</p>
<p>单位正交变换不改变矩阵的迹（不改变向量的长度和夹角），即上述变量代换不改变总方差，因此<span
class="math inline">\(\lambda_j/\text{tr}(D)\)</span>反映了成分<span
class="math inline">\(j\)</span>占总体的比例；</p>
<p>在实际应用中，基于奇异值分解的主成分分析比基于特征值分解的主成分分析更常用（算得更快更准）：<span
class="math inline">\(B\)</span>是具有平均偏差形式的<span
class="math inline">\(p\times N\)</span>的观测矩阵，<span
class="math inline">\(A=\frac{1}{\sqrt{N-1}}B^T\)</span>，则<span
class="math inline">\(A^TA\)</span>是协方差矩阵<span
class="math inline">\(S\)</span>，对<span
class="math inline">\(A\)</span>进行奇异值分解，<span
class="math inline">\(A\)</span>的奇异值的平方就是<span
class="math inline">\(S\)</span>的特征值，<span
class="math inline">\(A\)</span>的右奇异向量就是主成分。</p>
<h2 id="更多">8. 更多</h2>
<h3 id="schur-complement">8.1 Schur complement</h3>
<p><a href="https://en.wikipedia.org/wiki/Schur_complement">wiki - Schur
complement</a> 这里比较详细的给出了schur complement的由来已经应用；</p>
<h3 id="matrix-determinant-lemma">8.2 Matrix determinant lemma</h3>
<p><a href="https://en.wikipedia.org/wiki/Matrix_determinant_lemma">wiki
- Matrix determinant lemma</a></p>
<h3 id="关于tr的一些小技巧">8.3 关于tr的一些小技巧</h3>
<ul>
<li>矩阵内积：<span class="math inline">\(\operatorname{tr}(YX) =
\sum_{i,j} Y_{i,j}X_{i, j}\)</span></li>
<li><span class="math inline">\(\operatorname{tr}(vv^T) =
v^Tv\)</span></li>
<li><span class="math inline">\(\nabla_X \operatorname{tr}(YX) =
Y\)</span></li>
<li><span class="math inline">\(\dots\)</span></li>
</ul>
<h3 id="derivative-of-logdet-x">8.4 Derivative of <span
class="math inline">\(\log\det X\)</span></h3>
<p><a
href="https://statisticaloddsandends.wordpress.com/2018/05/24/derivative-of-log-det-x/">blog</a></p>
]]></content>
      <categories>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title>《幻夜》读后感</title>
    <url>/2022/01/20/%E3%80%8A%E5%B9%BB%E5%A4%9C%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/</url>
    <content><![CDATA[<img src="/2022/01/20/%E3%80%8A%E5%B9%BB%E5%A4%9C%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/Wnight.jpg" class="" title="《幻夜》">
<blockquote>
<p>本文实际成文于2020年1月，如今读来还挺有意思，作为博客的第一篇文章上传。</p>
</blockquote>
<p>——如果要说是读了什么的话，可以算上刚看完的《幻夜》</p>
<p>先陈列出自己想写的内容的关键词吧：欲望，快乐，我们所追求着的东西的背后的东西。</p>
<span id="more"></span>
<p>性质上，这是一篇完全的自我剖析，我也不知道自己哪一天会拿给谁看，请不要试图仅根据此来推测出完整的我，不仅是写文章前后，甚至文章本身前中后都可能表达着不同的意思——反正不写高考作文了。</p>
<p>婴儿出生后，脸是是完美对称的，但随着成长中种种外界因素的作用累积，便会产生各种各样的问题，而我想类比的是，人格也是这样。我无论如何也想不起来小时候为什么摔坏那几辆玩具小车，但是我不会忘记长大了一些的我对自己的自责：“这辆车本来该是多么地好玩”，我想，这时我产生了对于“完整”的欲望。当我开始模仿售票员摆弄家里的金属小零件，假装自己在收“银子”时，我总是觉得“银子”不够多，我想我产生了对于“占有”的欲望。当我开始模仿老师，在订好的小本子上任性地打下对勾或八叉时，“支配”的欲望应该已经在我心中成燎原之势了。</p>
<p>没能列举出来的还有很多，似乎在写自己是怎样进化成一个充满欲望的大魔头，事实似乎相反，在别人眼中，我似乎是个还不错的孩子。这些欲望谁没有呢，而它们本身又是相互制约，相互发展的，所以不为人所唾弃。</p>
<p>想占有金钱，支配别人，这些只能去看电影里的恶人，因为实际上这会损害自身的完整性，更多的人、包括我自己，被引导着改变了占有、支配的对象，使其在符合法律与道德的框架中进行。简单地说，我认为那些远大的理想，是穿着时髦衣服的欲望。</p>
<p>之前在L公司听讲座，说话的人曾经在黑暗的地下做着危险的煤矿工作，后来的机缘加他自己的努力，他认为走到了他喜欢的道路上，做了L公司的培训教师。他提出的几个问题我到现在都没能回答上自己：</p>
<ol type="1">
<li>你最擅长的是什么；</li>
<li>你做什么事的时候最专注；</li>
<li>你做什么事的时候最快乐</li>
</ol>
<p>我愿意在这里再理一遍我无法回答的思路：</p>
<p>在我从小长大的这个小地方，别人提到我，就会与“学霸”这个词挂钩。我知道我是运气好，家人没有给我什么压力，到是从小到大碰到了老师总是对我这个不太聪明的孩子寄予厚望，我就会因此觉得，自己可以再好一点，就在学习上对自己的要求不低。像是正反馈调节的机制。然而，我自己一直再反问自己，我真的擅长学习吗？把高考及之前的在校学习比作一场游戏真是非常恰当，我刚好在新手村打得很顺利，拿了很多小红花，像是不愿抛弃一个氪了金的账号一样，我就提着劲玩下去，没有了小红花，自然有更多满足虚荣心的奖励，红色的排名是一朵朵红花，而报考大学的排名更是一朵朵大红花。与其说我擅长学习，不如说我擅长在高考前的这场游戏中满足我的虚荣心。</p>
<p>这份虚荣心曾一度冲昏了我的头脑，让我自以为自己擅长那些被小红花标榜出来的东西。在小学看过一些儿童文学，也在角落里为某个主人公偷偷抹眼泪后，从初中开始，我的选书似乎刻意向语文书后的推荐名著靠拢。常常看不懂书中思想的我在看完一本书后，总是要“为赋新词强说愁”，写一篇文章，还模仿书中的辞藻，有些看起来文绉绉的词甚至都是自己造的。每当这些文章被老师贴在教室后黑板上，空白处被同学们写上赞美之词，我的虚荣心不是一般地被满足，甚至一度以为自己可以当作家。好在我的同学中有清醒者，在文章空白处C同学的一串字格外醒目，以至于我后来只记住这一句话：“抱歉，恕我不是很懂，感觉在无病呻吟”。</p>
<p>好一个“呻吟”，现在想来这是最恰当的词！作文拿了满分，就呻吟着自己相当作家；乐队演出了一次，就呻吟着想好好学音乐；物理上多想了一点，就呻吟着自己想当物理学家；当对于每一个曾经的理想深入了解发现，自己做不到的时候，我发现自己也就是一个只会呻吟的普通人了。</p>
<p>“活了十八年，我活成了普通人。”高考前我有点恐慌的对朋友讲。在每一个领域，我都能感觉到有无数的同龄人做到了很高的水平，而我那会，只会为了一模二模三模发愁——跟一个普通人一样，没什么擅长的。</p>
<p>当然，我后来意识到，我对“普通”这个概念有了误解，但还是无法找到自己擅长的一样实体。</p>
<p>至于我做什么的时候最专注，我可以自豪地说，但凡是我在手头做的事情我都很专注，比如写这篇文章，解一道问题，为女朋友做礼物，而悲哀的是我找不出那个“最”，也不能长期专注，所以没有日记，没有解决问题的一般思路，女朋友也会分手。从不是因为缺乏精力（我倒是总愿意去尝试新东西），而是缺少一个在“长远的动机”和“每天起床第一句（先给自己打口气）”之间的联系。在远处，我希望我能让人们的生活更智能，希望自己能有一个温馨的家，希望自己能用音乐、文字表达自己；在近处，我学习欲望不高，社交能力不强，单词背了又忘，重复的弹音阶也记不住每个位置是什么音，锻炼着怎么都很乏味的身体。以为高考完我的美好人生就要到来，却发现自己做不好的事情越来越多，又该如何专注到一件事上去呢？</p>
<p>至于快乐，那必须要与快感区分开。我也相信自己过去拥有快乐，专注做事，朋友见面，虚荣心得到满足，可每当我回想一件，过去不复存在的悲伤就涌上来一次，想回到眼前，曾经的快乐随着淡忘又不值当了。我信仰快乐是一个过程，尤其在走向未来的过程中。而我需要的是无数个在时间轴上铺开的未来，这个寒假，我会期待着爱情公寓5的更新而感到快乐，下学期，我会期待新的挑战而快乐。那么，要我回答我做什么事最快乐，我会先说追求未来的时候，然后我需要选一个不会成为我的过去的未来，我就只能选择：平静地走向死亡。那我现在还能快乐吗？</p>
<p>再回到欲望的话题上来。理想是穿着衣服的欲望，说出来总归要体面些。本硕博连读背后可能就是“高薪offer”，爱情背后可能就是“我馋她身子”。每个人最终会因为各种过程不同而导致追求的结果不同，但追求的的东西的背后说来还是欲望。原谅我把世界说的这么不堪，我只是发觉，曾经支撑我走的还不错的虚荣心现在正阻止我相信成人世界的事实，我不是世界中心的一朵白莲花，我是一个普通人，如果还能再加一个条件的话，我希望自己过得好些。</p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
  </entry>
  <entry>
    <title>《灵与肉》观后感</title>
    <url>/2022/02/06/%E3%80%8A%E7%81%B5%E4%B8%8E%E8%82%89%E3%80%8B%E8%A7%82%E5%90%8E%E6%84%9F/</url>
    <content><![CDATA[<img src="/2022/02/06/%E3%80%8A%E7%81%B5%E4%B8%8E%E8%82%89%E3%80%8B%E8%A7%82%E5%90%8E%E6%84%9F/stills.jpg" class="" title="《灵与肉》剧照">
<blockquote>
<p>依然是想到哪写到哪的胡言乱语，不过我很享受在写的过程中，拾起一片片回忆的贝壳，透过今天的阳光，品味它的纹理的过程。</p>
</blockquote>
<p>我很少看电视剧，觉得它总是用过多的巧合、反复的套路、拖沓的节奏讲一些很浅显的道理；不过，对于我妈来说，工作了一天后，打开电视是一种惯例了。过年回到家，我正赶上她在看电视剧，我一如既往地看两眼，吐吐槽就走开了。不过就在前几天的一个下午，我被<a
href="https://baike.baidu.com/item/%E7%81%B5%E4%B8%8E%E8%82%89/13012440?fr=aladdin">《灵与肉》</a>给吸引住了。</p>
<span id="more"></span>
<p>故事吸引我的原因很简单，它体现出来了时代现实与个人理想的矛盾，或者就像故事的名字一样，灵魂意向与肉体位置的不一致。带着探求解答的心理，我不停歇地看下去，主人公许灵均一次次有些“软弱”地向命运的不公妥协，老梅用书中的知识一次次为徐灵均守住精神世界，当他们的美好生活还没有完全到来时，我心中那份观众视角对人物角色的爱恨情仇渐渐淡下去了，现实视角对生活的重新审视多了起来。</p>
<p>我最近时常觉得，自己要么是比同龄人多了一份固执，要么是比同龄人少了一份稳重。</p>
<p>15岁那年，我遇到了我的青春期，依稀记得一些课堂上的叛逆、人际关系的迷茫，我急急忙忙地参加了一场考试，转到了一个新的环境中去。在新的学校，认识了更多谈得来的朋友，为难我的事情一下变少了，只有那冬天又冷又困人的早班公交车、青春期男生女生那一点心中的悸动偶尔令我不知所措：迷迷糊糊地闭上了眼，被好心的大妈叫醒，穿着奇怪颜色衣服，手里拿着一本《诗经》，头顶着洗后没干被冻成冰柱的头发，从一个不认识的车站，趁着冬夜还没被打破，奔向不太熟悉的校园。</p>
<p>新的环境还没有落稳脚跟，我又为自己找了无数个跟着朋友们去更远的大城市读书的理由：家里太吵学不进去、好几个朋友都要走没人玩、离开温室能带来更多机遇；提着一些奇怪衣服，奇怪头发包裹着一脑袋自我为中心的奇怪思想，我坐上了4小时行程的绿皮火车，在新学期开学前一周就先在附近的一个宾馆和朋友住下了，一面准备开学考试，一面认识这里的街巷。</p>
<p>几天前，在匿名墙里有一个话题，“大家觉得自己的高中生活怎么样”，我很真诚地添加了回复，“那时可能觉得五味杂陈，现在回想起来却只剩下了美好的回忆”。我也见到剧中的许灵均也说过类似的话！当然，我那时所以为的痛苦现在看来，不过是一个小孩子的胡思乱想罢了，我那时的“勇敢的决定”，大多也都是胡闹，全然也不能与许灵均的苦难相提并论，而我在苦难面前，也不像许灵均那般坚韧。</p>
<p>写出来不怕笑话，那时令我痛苦的大多都是些什么事情啊：不满意”仗势欺人横行霸道“的宿管；不满意”粗暴严格“的校规；不满意收钱”自愿“补课；不会协调白天的时间而只能在深夜偷偷打灯奋笔疾书；静不下心来而被氮元素的计算和排列组合的问题急地死去活来；女朋友和别的男生多说两句就要生气；担心因为早恋被抓现行影响”前途“，反而又闹僵了两人的关系；安排不好早晨扫雪的任务在同学和班主任”两头不是人“；担心好兄弟认为自己是”见色忘义“而急得手足无措；因为乱七八糟的小事而没能把课代表坚持做下去；因为体质不好总是感冒去医院，还担心感染水痘被停课的风险。。。</p>
<p>不过，在那些个周六的夜晚，我独自呆在6人的宿舍中，洗过的衣服因为没用力拧而滴滴答答地往盆子里滴水。起初我还会因为别人都能在假期回亲戚家而伤神，后来我慢慢习惯了这个属于自己的独立的小时空：夏天的时候，我就和朋友一直打篮球打到只剩下去吃饭的力气再回宿舍，有时冬天躁动了，带着扫雪工具拿着篮球也要去球场打一会；不打球的时候，我可能拿着一本夺人眼泪的小说，在有一点氤氲的屋子里看得浑身发抖，手脚冰凉；有时我也喜欢弹会儿吉他，唱几首音域不那么宽广的歌；兴致再好的时候，我还会拿起毛笔，临一临字帖，默一默背诵的古诗词之类的；查手机的风口松的时候，我就在手机上跟人聊天，看一些篮球相关视频；学习压力大的时候，我也绝不愿意用那些写不完的作业破坏了这美好的时刻，最多是拿出一个小本子，一笔一划地写一些计划。</p>
<p>我想，也许就是在那些个周六夜晚，各种的胡思乱想为我那时的喜怒哀乐埋下了昙花种子。也让我在之后逢年过节回到家中，与家中亲戚的那些个“世俗”想法开始有了隔阂，回家这件事本身对我来说成为了一个坐在返程的车上的一个美好想象，和坐在家中忍受着亲戚们“乌七八糟”的价值观（当然是在少不经事的我看来）。</p>
<p>我感觉自己是个“文化人”，我觉得自己平时主动或被动接受的，都是最先进的思想，我坐在家里，就想那个被下放劳动改造的许灵均一样，但是我却不像他那样包容！渐渐地，我不满的对象扩大到了我出生和成长的这个小城市，地方小，节奏慢。在大学的第二个寒假，对家乡的这种羞于启齿但不能扑灭的厌恶之情在我心里彻底燃烧起来了：在中国传统都要回家的春节，我选择独自在大学的宿舍里度过，试图继续找到高中时那些个周六夜晚的惬意。
但是，时光不能倒流，往日的感觉可能就停留在了往日，或者仅仅存在我的回忆中罢了。我没有换来想要的宁静，却收到了爷爷在过年期间，住进了重病房的消息。好在救治及时和家人的悉心照顾，爷爷除了变得更瘦一点，吃得更少了一点之外，都还好。</p>
<p>今年春节回家的路在回家前两天被疫情切断了；值得庆兴的是，在年前我获得了返乡的机会；在中转的城市，因为政策的滞后我没有住处，明明遵守着各项规定的我只能坐上黑车，去又冷又臭的黑旅社凑活一晚上；倘若是几年前的我，可能真的会咽不下这份委屈大闹一场，在与最好的朋友吐槽过后，想了想没有什么文化但是却也曾经经历过这些的黑车司机的劝告，我终于还是忍下了。</p>
<p>家里还是那个样子，有人说你要一直读到博士然后赚大钱，有人不停重复着网上谣传的健康食谱和病态的审美，有人说着你要当上领导然后帮着家里人。嗨，现在看来，这些话也只有自家人才能放下社会上的伪装，在茶余饭后不停地讲，表示各自对自家人的关心吧。</p>
<p>而社会上的种种冲突也好，降临到自己头上时，苦难、不公怕是难以避免的，书上也许写着正义，写着道德与法治，可社会的运作哪里如写书那般轻松？书本会给我们开阔的视野和先进的思想，当这些思想与现实发生冲突，带来痛苦时，不妨再次回到书里，看看从前那些不得意的古人如何遣词造句，纾解心意；看看那些向许灵均这样最终熬过了苦难的人，是如何把坚守自己的理想，在苦难中前行的。我知道，离开校园后，才会有真正的现实社会等着我，新时代下的苦难的形式不会再是挨钢鞭，挨批斗。时代的一粒沙子，落到一个人头上就是一篇荒漠，在这荒漠中承受史无前例的肉体和精神的痛苦的时候，也许灵魂可以向那些看见绿洲的前辈同行。</p>
<blockquote>
<p>P.S.
这部由楼健编导的电视剧改编自作家张贤亮的同名小说，几天前我恰好在书店的书架上看到这本书，是一部中短篇小说集，许灵均的故事仅仅是很小篇幅，我粗糙地读了一遍，发现电视剧的编剧添加了大量的人物和故事情节，在主题上也稍稍有不同，两者都是值得一读/看的。</p>
</blockquote>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
  </entry>
</search>
